\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{adjustbox}

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\tikzset{
block/.style = {draw, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate},
sum/.style = {draw, circle, node distance=1cm},
input/.style = {coordinate},
output/.style = {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}},
every text node part/.style={align=center}
}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\cdf}{\mathrm{F}}
\DeclareMathOperator{\pdf}{\mathrm{f}}

\DeclareMathOperator{\prob}{\mathrm{P}}

\DeclareMathOperator{\expct}{\mathrm{E}}

\DeclareMathOperator{\bin}{\mathrm{Bin}}
\DeclareMathOperator{\poi}{\mathrm{Poi}}
\DeclareMathOperator{\geo}{\mathrm{Geo}}
\DeclareMathOperator{\nb}{\mathrm{NB}}
\DeclareMathOperator{\hg}{\mathrm{HG}}
\DeclareMathOperator{\uniform}{\mathrm{U}}
\DeclareMathOperator{\exponential}{\mathrm{Exp}}
\DeclareMathOperator{\normal}{\mathrm{N}}

\DeclareMathOperator{\var}{\mathrm{V}}
\DeclareMathOperator{\sd}{\mathrm{\sigma}}

\DeclareMathOperator{\cov}{\mathrm{Cov}}

\DeclareMathOperator{\FT}{\mathcal{F}}
\DeclareMathOperator{\IFT}{\mathcal{F}^{-1}}
\DeclareMathOperator{\LT}{\mathcal{L}}

\DeclareMathOperator{\sinc}{\mathrm{sinc}}
\DeclareMathOperator{\rect}{\mathrm{rect}}
\DeclareMathOperator{\tri}{\mathrm{tri}}

\DeclareMathOperator{\QSNR}{\mathrm{QSNR}}

%opening
\title{Communication Systems}
\author{Aakash Jog}
\date{2016-17}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Prof. Arie Yeredor}\\
~\\
Office: Maabadot 121\\
E-mail: \href{mailto:arie@eng.tau.ac.il}{arie@eng.tau.ac.il}\\
Tel: \href{tel:+97236405314}{+972 3 640 5314}\\

\section{Instructor Information}

\textbf{Amir Weiss}\\
~\\
Office: Tochna 309\\
E-mail: \href{mailto:amirwei2@mail.tau.ac.il}{amirwei2@mail.tau.ac.il}\\
Tel: \href{tel:+97236406120}{+972 3 640 6120}\\

\section{Recommended Reading}

\begin{enumerate}
	\item \fullcite{Couch}
	\item \fullcite{Carlson}
	\item \fullcite{Haykin}
	\item \fullcite{HaykinMoher}
	\item \fullcite{Proakis}
	\item \fullcite{Lee}
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definitions and Theorems}

\section{Types of Communication}

\begin{definition}[Digital communication]
	Communication is said to be digital communication if the source can send only one of the predefined messages.
	Usually, the predefined messages are binary, i.e. $0$ or $1$.
	\label{def:digital_communication}
\end{definition}

\begin{definition}[Analogue communication]
	Communication is said to be analogue communication if the source can send one of infinite number of messages.
	\label{def:analogue_communication}
\end{definition}

\section{Periodicity and Finite Energy}

\begin{definition}[Finite energy signal]
	A signal $x(t)$ is said to have finite energy if
	\begin{align*}
		\int\limits_{-\infty}^{\infty} \left| x(t) \right|^2 \dif t &< \infty
	\end{align*}
	\label{def:finite_energy_signal}
\end{definition}

\begin{definition}[Periodic signal]
	A signal $x(t)$ is said to be $T_0$-periodic if for all $t$,
	\begin{align*}
		x(t + T_0) &= x(t)
	\end{align*}
	\label{def:periodic_signal}
\end{definition}

\section{Fourier Series Representation}

\begin{theorem}[Fourier series representation]
	A $T_0$-periodic signal $x(t)$ can be expressed as a Fourier series
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \omega_0 t}
	\end{align*}
	where
	\begin{align*}
		\omega_0 &= \frac{2 \pi}{T_0}
	\end{align*}
	and
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{T_0} x(t) e^{-j k \omega_0 t} \dif t
	\end{align*}
	\label{thm:Fourier_series_representation}
\end{theorem}

\section{Fourier Transform}

\begin{theorem}[Time and frequency domain expressions]
	A finite energy signal $x(t)$ can be expressed in frequency domain such that
	\begin{align*}
		X(\omega) &= \FT\left\{ x(t) \right\}\\
		&= \int\limits_{-\infty}^{\infty} x(t) e^{j \omega t} \dif t\\
		x(t) &= \IFT\left\{ X(\omega) \right\}\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) e^{j \omega t} \dif \omega
	\end{align*}
	\label{thm:time_and_frequency_domain_expressions}
\end{theorem}

\begin{theorem}[Linearity of Fourier transform]
	The Fourier transform is linear, i.e.
	\begin{align*}
		y(t) &= a_1 x_1(t) + a_2 x_2(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= a_1 X_1(\omega) + a_2 X_2(\omega)
	\end{align*}
	\label{thm:linearity_of_fourier_transform}
\end{theorem}

\begin{theorem}[Time shift]
	A time delay corresponds to multiplication by a tone in frequency domain, i.e.
	\begin{align*}
		y(t) &= x(t - \tau)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= e^{-j \omega \tau} X(\omega)
	\end{align*}
	\label{thm:time_shift}
\end{theorem}

\begin{theorem}[Frequency shift]
	A shift in frequency corresponds to a tone in time domain, i.e.
	\begin{align*}
		y(t) &= e^{j \omega_0 t} x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift}
\end{theorem}

\begin{theorem}[Time reversal]
	A reversal in time corresponds to a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(-\omega)
	\end{align*}
	\label{thm:time_reversal}
\end{theorem}

\begin{theorem}[Time conjugation]
	A conjugation in time corresponds to a conjugation and a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(-\omega)
	\end{align*}
	\label{thm:time_conjugation}
\end{theorem}

\begin{theorem}[Time conjugation and reversal]
	A conjugation and a reversal in time corresponds to a conjugation in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(\omega)
	\end{align*}
	\label{thm:time_conjugation}
\end{theorem}

\begin{theorem}[Convolution in time]
	A convolution in time corresponds to multiplication in frequency, i.e.
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\tau) x(t - \tau) \dif \tau
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= H(\omega) X(\omega)
	\end{align*}
	\label{thm:convolution_in_time}
\end{theorem}

\begin{theorem}[Multiplication in time]
	A multiplication in time corresponds to convolution in frequency, i.e.
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= \frac{1}{2 \pi} C(\omega) X(\omega)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} C(\sigma) X(\omega - \sigma) \dif \sigma
	\end{align*}
	\label{them:multiplication_in_time}
\end{theorem}

\begin{definition}[Dirac delta function]
	The Dirac delta function is defined to be $\delta(t)$ such that for $a < b$,
	\begin{align*}
		\int\limits_{a}^{b} \delta(t) \dif t &=
			\begin{cases}
				1 &;\quad a < 0 < b\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence, the value of the function for non-zero values of $t$ may or may not be zero, but the integral of the function over any interval which excludes zero is $1$.
	Hence, for all non-zero values of $t$, the function may have rapid oscillations.\\
	For example, the delta function expressed as a limiting case of a Gaussian is zero at all non-zero point and infinite at zero, while the delta function expressed as a limiting case of a sinc function has rapid oscillations at non-zero points.
	\label{thm:Dirac_delta_function}
\end{definition}

\begin{theorem}[Dirac delta in time]
	A Dirac delta in time corresponds to a constant $1$ in frequency, i.e.
	\begin{align*}
		x(t) &= \delta(t)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 1
	\end{align*}
	\label{thm:Dirac_delta_in_time}
\end{theorem}

\begin{definition}[Rectangular pulse function]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				1 &;\quad |t| \le \frac{1}{2}\\
				0 &;\quad |t| > \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\label{def:rect_function}
\end{definition}

\begin{definition}[Triangular pulse function]
	\begin{align*}
		\tri(t) &=
			\begin{cases}
				1 - |t| &;\quad |t| \le 1\\
				0 &;\quad |t| > 1\\
			\end{cases}
	\end{align*}
	\label{def:tri_function}
\end{definition}

\begin{definition}[Sinc function]
	\begin{align*}
		\sinc(t) &=
			\begin{cases}
				1 &;\quad t = 0\\
				\frac{\sin(t)}{t} &;\quad t \neq 0\\
			\end{cases}
	\end{align*}
	\label{def:sinc_function}
\end{definition}

\begin{theorem}[Dirac delta in frequency]
	A Dirac in frequency corresponds to a constant $1$ in time, i.e.
	\begin{align*}
		x(t) &= 1
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
	\label{thm:Dirac_delta_in_frequency}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		y(t) &= \rect\left( \frac{t}{T_0} \right)
	\end{align*}
	Hence,
	\begin{align*}
		Y(\omega) &= \frac{2}{\omega} \sin\left( \omega \frac{T_0}{2} \right)\\
		&= T_0 \sinc\left( \omega \frac{T_0}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \lim\limits_{T_0 \to \infty} y(t)
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \lim\limits_{T_0 \to \infty} Y(\omega)\\
		&= c \delta(\omega)
	\end{align*}
	where $c$ is a constant.\\
	Also,
	\begin{align*}
		x(0) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) \dif \omega
	\end{align*}
	Therefore,
	\begin{align*}
		1 &= x(0)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} c \delta(\omega) \dif \omega\\
		&= \frac{c}{2 \pi}
	\end{align*}
	Therefore,
	\begin{align*}
		c &= 2 \pi
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Fourier transform in terms of temporal frequency]
	\begin{align*}
		\tilde{X}(f) &= X(\omega) \Big|_{\omega = 2 \pi f}
	\end{align*}
	Hence,
	\begin{align*}
		\tilde{X}(f) &= \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} \dif t\\
		x(t) &= \int\limits_{-\infty}^{\infty} \tilde{X}(f) e^{j 2 \pi f t} \dif f
	\end{align*}
	\label{thm:Fourier_transform_in_terms_of_temporal_frequency}
\end{theorem}

\begin{theorem}[Impulse train in time]
	An impulse train in time corresponds to an impulse train in frequency, i.e.
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:impulse_train_in_time}
\end{theorem}

\begin{proof}
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j \omega_0 k t}
	\end{align*}
	where
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} \delta(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0}
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} e^{j \omega_0 k t}\\
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} 2 \pi \delta(\omega - k \omega_0)\\
		&= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
\end{proof}

\begin{theorem}[Sampling in time]
	A sampling in time corresponds to replication in frequency, i.e.
	\begin{align*}
		y(t) &= x(t) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n T_0) \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega) \ast \sum\limits_{k = -\infty}^{\infty} \frac{2 \pi}{T_0} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)\\
		&= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} X\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:sampling_in_time}
\end{theorem}

\section{Stochastic Signals}

\begin{definition}[Mean of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The mean of $x(t)$ is defined to be
	\begin{align*}
		\tilde{\eta}_X(t) &= \expct\left[ x(t) \right]
	\end{align*}
\end{definition}

\begin{definition}[Autocorrelation function of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The autocorrelation function of $x(t)$ is defined to be
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \expct\left[ x(t_1) x^*(t_2) \right]
	\end{align*}
\end{definition}

\begin{theorem}
	For a real stochastic signal $x(t)$,
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \tilde{R}_{X X}(t_2,t_1)
	\end{align*}
\end{theorem}

\begin{definition}[Wide sense stationary random process]
	A random process $X$ is said to be wide sense stationary if
	\begin{align*}
		\tilde{\eta}_X(t) &= \eta_X\\
		\tilde{R}_{X X}(t_1,t_2) &= R_{X X}(t_1 - t_2)
	\end{align*}
	that is, the mean is independent of time and the autocorrelation function is dependent on the time difference only and is independent of absolute time.
\end{definition}

\begin{theorem}
	For a WSS random process $X$,
	\begin{align*}
		R_{X X}(\tau) &= {R_{X X}}^*(-\tau)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{X X}(-\tau) &= \expct\left[ x(t - \tau) x^*(t) \right]\\
		&= \expct\left[ x^*(t) x(t - \tau) \right]\\
		&= \expct\left[ x(t) x(t - \tau) \right]^*\\
		&= {R_{X X}}^*(\tau)
	\end{align*}
\end{proof}

\begin{definition}[Spectrum of WSS random process]
	\begin{align*}
		S_{X X}(\omega) &= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
\end{definition}


\begin{theorem}
	The spectrum of a random process is the mean of the squared absolute value of the Fourier transform of a long enough segment of a sample function of the process, normalized by the length of the segment, i.e.
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t \right| \right]
	\end{align*}
\end{theorem}

\begin{proof}
	Let $x(t)$ be a sample function of a WSS random process.
	As $R_{X X}(0) = \expct\left[ \left| x(t) \right|^2 \right]$ is constant irrespective of the absolute time, the signal $x(t)$ never converges.
	Therefore, it is not a finite energy signal and hence does not have a Fourier transform.\\
	Hence, let
	\begin{align*}
		X_T(\omega) &= \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \expct\left[ \int\limits_{0}^{T} x(t_1) e^{-j \omega t_1} \dif t_1 \int\limits_{0}^{T} x^*(t_2) e^{j \omega t_2} \dif t_2 \right]\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} \expct\left[ x(t_1) x^*(t_2) \right] e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} {\tilde{R}_{X X}}(t_1,t_2) e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} R_{X X}(t_1 - t_2) e^{-j \omega (t_1 - t_2)} \dif t_1 \dif t_2
	\end{align*}
	Let
	\begin{align*}
		\tau &= t_1 - t_2\\
		\sigma &= t_1 + t_2
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\tau\\
				\sigma\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 1\\
			\end{pmatrix}
			\begin{pmatrix}
				t_1\\
				t_2\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\tau,\sigma)}{(t_1,t_2)} \right| &= 2
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \int\limits_{-T}^{T} \int\limits_{|\tau|}^{2 T - |\tau|} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \dif \sigma \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{T} \int\limits_{|\tau|}^{2 T - |\tau|} \dif \sigma \dif \tau \frac{1}{2}\\
		&= \frac{-T}{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \left( \frac{2 T - 2 |\tau|}{2} \right) \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \left( 1 - \frac{|\tau|}{T} \right) \dif \tau\\
		&= \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
	Therefore, assuming the Ergodicity condition $\int\limits_{-\infty}^{\infty} \left| \tau R_{X X}(\tau) \right| \dif \tau < M < \infty$,
	\begin{align*}
		\lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \lim\limits_{T \to \infty} \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		S_{X X}(\omega) &\ge 0
	\end{align*}
	for all $\omega$.
	\label{thm:non_negativity_of_spectrum}
\end{theorem}

\begin{proof}
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right]\\
		&\ge 0
	\end{align*}
\end{proof}

\begin{theorem}[Time shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then,
	\begin{align*}
		y(t) &= x(t - t_0)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t - t_0 + \tau) x^*(t - t_0) \right]\\
		&= R_{X X}(\tau)
	\end{align*}
	Hence,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Frequency shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(t) e^{j \omega_0 t}
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t + \tau) e^{j \omega_0 (t + \tau)} x^*(t) e^{-j \omega_0 t} \right]\\
		&= e^{j \omega_0 \tau} \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= e^{j \omega_0 \tau} R_{X X}(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
\end{proof}

\begin{theorem}[Time reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(-t - \tau) x^*(-t) \right]\\
		&= R_{X X}(-\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Time conjugation for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x^*(t + \tau) x(t) \right]\\
		&= R_{X X}(-\tau)\\
	\end{align*}
	Equivalently,
	\begin{align*}
		R_{Y Y}(\tau) &= {R_{X X}}^*(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	and
	\begin{align*}
		S_{Y Y}(\omega) &= {S_{X X}}^*(-\omega)
	\end{align*}
	This is consistent with \cref{thm:non_negativity_of_spectrum}.
\end{proof}

\begin{theorem}[Time conjugation and reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{theorem}[Convolution in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\mu) x(t - \mu) \dif \mu
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
	\label{thm:convolution_in_time_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ \int\limits_{-\infty}^{\infty} h(\mu) x(t + \tau - \mu) \dif \mu \int\limits_{-\infty}^{\infty} h^*(\sigma) x^*(t - \sigma) \dif \sigma \right]\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) \expct\left[ x(t + \tau - \mu) x^*(t - \sigma) \right] \dif \mu \dif \sigma\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) R_{X X}(t - \mu + \sigma) \dif \mu \dif \sigma
	\end{align*}
	Let
	\begin{align*}
		\alpha &= \mu - \sigma\\
		\beta &= \mu
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\alpha\\
				\beta\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 0\\
			\end{pmatrix}
			\begin{pmatrix}
				\mu\\
				\sigma\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\alpha,\beta)}{(\mu,\sigma)} \right| &= 1
	\end{align*}
	Therefore,
	\begin{align*}
		R_{Y Y}(\tau) &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) R_{X X}(\tau - \alpha) \dif \alpha \dif \beta
	\end{align*}
	Let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) \dif \beta R_{X X}(\tau - \alpha) \dif \alpha
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= G(\omega) S_{X X}(\omega)
	\end{align*}
	Additionally, let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) \overline{h}(\alpha - \beta) \dif \beta
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) \overline{H}(\omega)
	\end{align*}
	Hence, by the two definitions of $g(\alpha)$,
	\begin{align*}
		\overline{h}(\tau) &= h^*(-\tau)\\
	\end{align*}
	Therefore,
	\begin{align*}
		\overline{H}(\omega) &= H^*(\omega)
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) H^*(\omega)\\
		&= \left| H(\omega) \right|^2
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $Y$ may or may not be a WSS random process.
	\footnote{This is intuitively correct as, for example, if a WSS random process is multiplied by a finite window, the product is zero outside the window, and hence is not WSS.}
	\footnote{If $c(t)$ is periodic, $Y$ is cyclostationary. See \cref{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}}
	\label{thm:multiplication_in_time_for_stochastic_signal_and_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)
	\end{align*}
	Hence, as the autocorrelation function of $Y$ may be dependent on the absolute time, $Y$ may or may not be WSS.
\end{proof}

\section{Cyclostationary Processes}

\begin{definition}[Cyclostationary process]
	A random process is said to be cyclostationary with a cyclic period $T_0$ if the mean $\tilde{\eta}_X(t)$ and $\tilde{R}_{X X}(t,\tau)$ are $T_0$ periodic in $t$, i.e. if for all $t$ and $\tau$,
	\begin{align*}
		\tilde{\eta}_X(t + T_0) &= \tilde{\eta}_X(t)\\
		\tilde{R}_{X X}(t + T_0,\tau) &= \tilde{R}_{X X}(t,\tau)
	\end{align*}
\end{definition}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a $T_0$ periodic deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $y(t)$ is cyclostationary with $T_0$.
	\label{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		\tilde{\eta}_Y(t) &= \expct\left[ c(t) x(t) \right]\\
		&= c(t) \expct\left[ x(t) \right]\\
		&= c(t) \eta_X\\
		&= c(t + T_0) \eta_X\\
		&= \tilde{\eta}_Y(t + T_0)\\
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)\\
		&= c(t + \tau + T_0) c^*(t + T_0) R_{X X}(\tau)\\
		&= \tilde{R}_{Y Y}(t + T_0,\tau)
	\end{align*}
	Hence, $Y$ is cyclostationary with $T_0$.
\end{proof}

\begin{theorem}
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a $T_0$ periodic deterministic function.\\
	Let
	\begin{align*}
		\overline{y}(t) &= c(t - \varphi) x(t)
	\end{align*}
	where $\varphi \sim \uniform(0,T_0)$ is independent of $x(t)$.\\
	Then, $\overline{y}(t)$ is WSS with
	\begin{align*}
		\eta_{\overline{Y}} &= m_c \eta_X\\
		R_{\overline{Y} \overline{Y}}(\tau) &= r_c(\tau) R_{X X}(\tau)
	\end{align*}
	where
	\begin{align*}
		m_c &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t) \dif t\\
		r_c(\tau) &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau) c^*(t) \dif t
	\end{align*}
	and hence,
	\begin{align*}
		S_{\overline{Y} \overline{Y}}(\omega) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} P_c(\sigma) S_{X X}(\omega - \sigma) \dif \sigma
	\end{align*}
	where
	\begin{align*}
		P_c(\omega) &= \FT\left\{ r_c(\tau) \right\}\\
		&= \int\limits_{-\infty}^{\infty} r_c(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
	\label{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal_dependent_on_random_variable}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\overline{y}(t) &= c(t - \varphi) x(t)
	\end{align*}
	where $\varphi$ is a random variable independent of $x(t)$.\\
	Therefore,
	\begin{align*}
		\tilde{\eta}_{\overline{Y}}(t) &= \expct\left[ c(t - \varphi) x(t) \right]\\
		&= \expct\left[ \expct\left[ c(t - \varphi) x(t) \Big| \varphi \right] \right]\\
		&= \expct_{\varphi}\left[ c(t - \varphi) \eta_X \right]\\
		&= \expct\left[ c(t - \varphi) \right] \eta_X\\
		\tilde{R}_{\overline{Y} \overline{Y}}(t,\tau) &= \expct\left[ \overline{y}(t + \tau) \overline{y}^*(t) \right]\\
		&= \expct\left[ \expct\left[ \overline{y}(t + \tau) \overline{y}^+(t) \Big| \varphi \right] \right]\\
		&= \expct\left[ \expct\left[ c(t + \tau - \varphi) x(t + \tau) c^*(t - \varphi) x^*(t) \Big| \varphi \right] \right]\\
		&= \expct_{\varphi}\left[ c(t + \tau - \varphi) c^*(t - \varphi) R_{X X}(\tau) \right]\\
		&= \expct\left[ c(t + \tau - \varphi) c^*(t - \varphi) \right] R_{X X}(\tau)
	\end{align*}
	As $c(t)$ is $T_0$ periodic and $\varphi \sim \uniform(0,T_0)$,
	\begin{align*}
		\expct\left[ c(t - \varphi) \right] &= \int\limits_{-\infty}^{\infty} \pdf_{\varphi}(\varphi) c(t - \varphi) \dif \varphi\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t - \varphi) \dif \varphi
	\end{align*}
	Let
	\begin{align*}
		\varphi' &= t - \varphi\\
		\therefore \dif \varphi' &= \dif \varphi
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ c(t - \varphi) \right] &= \frac{1}{T_0} \int\limits_{t - T_0}^{t} c(\varphi') \dif \varphi'\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t) \dif t\\
		&= m_c
	\end{align*}
	Similarly,
	\begin{align*}
		\expct\left[ c(t + \tau - \varphi) c^*(t - \varphi) \right] &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau - \varphi) c^*(t - \varphi) \dif \varphi\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau) c^*(t) \dif t\\
		&= r_c(\tau)
	\end{align*}
\end{proof}

\begin{question}
	Consider the zero mean WSS process $X(t)$ with autocorrelation function $R_{X X}(\tau)$.
	Let
	\begin{align*}
		Y(t) &= X(t) c(t - \varphi)
	\end{align*}
	where
	\begin{align*}
		c(t) &= \cos(\omega_0 t)\\
		\cdf_{\varphi}(\phi) &=
			\begin{cases}
				\frac{\omega_0}{\pi} \left( 1 - \frac{\omega_0}{\pi} \phi \right) &;\quad |\phi| \le \frac{\pi}{\omega_0}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Show that $Y(t)$ is WSS.
\end{question}

\begin{solution}
	\begin{align*}
		\tilde{\eta}_Y(t) &= \expct\left[ Y(t) \right]\\
		&= \expct\left[ X(t) c(t - \phi) \right]\\
		&= \expct\left[ X(t) \right] \expct\left[ \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= 0\\
		\tilde{R}_{Y Y}(t,\tau) &= \expct\left[ Y(t + \tau) Y(t) \right]\\
		&= \expct\left[ X(t + \tau) \cos\left( \omega_0 (t + \tau - \varphi) \right) x^*(t) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= \expct\left[ x(t + \tau) x^*(t) \right] \expct\left[ \cos\left( \omega_0 (t + \tau - \varphi) \right) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= R_{X X}(\tau) \expct\left[ \cos\left( \omega_0 (t + \tau - \varphi) \right) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= \frac{R_{X X}(\tau)}{2} \expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) + \cos(\omega_0 \tau) \right]\\
		&= \frac{R_{X X}(\tau)}{2} \cos(\omega_0 \tau) + \frac{R_{X X}(\tau)}{2} \expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) \right]
	\end{align*}
	Therefore, solving by definition,
	\begin{align*}
		\expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) \right] &= 0
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{R}_{Y Y}(t,\tau) &= \frac{R_{X X}(\tau) \cos(\omega_0 \tau)}{2}
	\end{align*}
	Therefore, as the mean is independent of time and the autocorrelation function is dependent on the time difference only, $Y(t)$ is WSS.
\end{solution}

\clearpage
\part{Pulse Amplitude Modulation}

\section{Direct Multiplication / Natural Sampling}

\begin{definition}[Direct Multiplication PAM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$.\\
	Let $c(t)$ be given by a train of pulses, such that
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	where $p(t)$ is a general pulse.
	\footnote{$\rect\left( \frac{t}{\Delta} \right)$ is a commonly used pulse.}\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Such a modulation is called direct multiplication pulse amplitude modulation.
	\label{def:direct_multiplication_PAM}
\end{definition}

\begin{theorem}
	For a signal $x(t)$ modulated with direct multiplication PAM,
	\begin{align*}
		Y(\omega) &= \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - \omega_s k)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ scaled according to $a_k$ with adjacent replicas separated $\omega_s$, and in the time domain, the pulses are distorted and have the shape of the original signal.
	\footnote{As these replicas differ by a constant factor only, the signal can be recreated using any replica. In order to avoid noise in the reconstruction, the replica used in reconstruction is that for which the noise is the lowest.}
	Hence, in order to avoid aliasing,
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
	\label{thm:Fourier_transform_of_signal_modulated_by_direct_multiplication_PAM}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\omega_s &= \frac{2 \pi}{T_s}
	\end{align*}
	Therefore,
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	Therefore, the Fourier coefficients of $c(t)$ are
	\begin{align*}
		a_k &= \frac{1}{T_s} \int\limits_{0}^{T_s} c(t) e^{j \omega_s k t} \dif t\\
		&= \frac{1}{T_s} \int\limits_{0}^{T_s} \sum\limits_{n = -\infty}^{\infty} p(t - n T_s) e^{j \omega_s k t} \dif t
	\end{align*}
	Let
	\begin{align*}
		t' &= t - n T_s\\
		\therefore t &= t' + n T_s\\
		\therefore \dif t &= \dif t'
	\end{align*}
	Therefore,
	\begin{align*}
		a_k &= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k (t' + n T_s)} \dif t'\\
		a_k &= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k \left( t' + n \frac{2 \pi}{\omega_s} \right)} \dif t'\\
		&= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k t'} \dif t'\\
		&= \frac{1}{T_s} \int\limits_{-\infty}^{\infty} p(t) e^{-j \omega_s k t} \dif t\\
		&= \frac{1}{T_s} P(\omega_s k)
	\end{align*}
	Therefore,
	\begin{align*}
		X(\omega) &= \sum\limits_{k = -\infty}^{\infty} 2 \pi a_k \delta(\omega - \omega_s k)
	\end{align*}
	Therefore,
	\begin{align*}
		Y(\omega) &= \frac{1}{2 \pi} C(\omega) \ast X(\omega)\\
		&= \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - \omega_s k)\\
		&= \sum\limits_{k = -\infty}^{\infty} \frac{1}{T_s} P(k \omega_s)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ scaled according to $a_k$ with adjacent replicas separated $\omega_s$.
	Hence, in order to avoid aliasing, $X(\omega)$ must be limited to $\pm \omega_s$, i.e.
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
\end{proof}

\begin{theorem}
	A signal modulated with direct multiplication PAM can be reconstructed as in \cref{fig:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}, where the cutoff frequency and the gain of the low pass filter are
	\begin{align*}
		\omega_c &\in [B,\omega_s - B)\\
		A &= \frac{1}{|a_m|}\\
		&= \frac{T_s}{\left| P(\omega_s m) \right|}
	\end{align*}
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,>=latex']
			\node [mixer] (mixer) {};
			\node [block, right = of mixer] (LPF) {LPF};

			\draw [stealth-] (mixer.west) -- ++(-1,0) node [left] {$y(t)$};
			\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$\cos(m \omega_s t + \varphi)$};
			\draw [-stealth] (mixer.east) -- (LPF.west) node [midway, above] {$\tilde{y}(t)$};
			\draw [-stealth] (LPF.east) -- ++(1,0) node [right] {$\hat{x}(t)$};
		\end{tikzpicture}
		\end{adjustbox}
		\caption{Reconstruction of Signal Modulated with Direct Multiplication PAM}
		\label{fig:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}
	\end{figure}
	Hence,
	\begin{align*}
		\hat{X}(\omega) &= \cos(\varphi) X(\omega)
	\end{align*}
	\label{thm:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}
\end{theorem}

\begin{proof}
	\begin{align*}
		\tilde{y} &= \cos(m \omega_s t + \varphi) y(t)\\
		&= \frac{1}{2} \left( e^{j \varphi} e^{j m \omega_s t} + e^{-j \varphi} e^{-j m \omega_s t} \right) y(t)
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{Y}(\omega) &= \frac{1}{2} \left( e^{j \varphi} Y(\omega - m \omega_s) + e^{-j \varphi} Y(\omega - m \omega_s) \right)\\
		&= \frac{1}{2} \left( e^{j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - k \omega_s - m \omega_s) + e^{-j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - k \omega_s + m \omega_s \right)\\
		&= \frac{1}{2} \left( e^{j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X\left( \omega - (k + m) \omega_s \right) + e^{-j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X\left( \omega - (k - m) \omega_s \right) \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\hat{X}(t) &= \frac{1}{2 |a_m|} \left( e^{j \varphi} a_{-m} X(\omega) + e^{j \varphi} a_m X(\omega) \right)\\
		&= \frac{1}{2 |a_m|} \left( e^{j \varphi} {a_m}^* + e^{-j \varphi} a_m \right) X(\omega)\\
		&= \frac{1}{|a_m|} \Re\left\{ e^{-j \varphi} a_m \right\} X(\omega)
	\end{align*}
	Assuming $p(t)$ is symmetric, $a_m$ is real.
	Therefore,
	\begin{align*}
		\hat{X}(\omega) &= \Re\left\{ e^{j \varphi} \right\} X(\omega)\\
		&= \cos(\varphi) X(\omega)
	\end{align*}
\end{proof}

\section{Flat Sampling}

\begin{definition}[Flat Sampling PAM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$.\\
	Let $c(t)$ be given by a train of pulses, such that
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	where $p(t)$ is a general pulse.
	\footnote{$\rect\left( \frac{t}{\Delta} \right)$ is a commonly used pulse.}\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T_s) p(t - n T_s)
	\end{align*}
	that is, the pulses of the modulated signal have the same shape as the original train of pulses.
	Such a modulation is called flat sampling pulse amplitude modulation.
	\label{def:flat_sampling_PAM}
\end{definition}

\begin{theorem}
	For a signal $x(t)$ modulated with flat sampling PAM,
	\begin{align*}
		Y(\omega) &= \frac{1}{T_s} P(\omega) \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ distorted by the Fourier transform of the pulse, with adjacent replicas separated $\omega_s$.
	Hence, in the time domain, the shape of the pulses is preserved, but the shape of frequency spectrum is distorted, and the distortion of each replica is different.
	Hence, in order to avoid aliasing,
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
	\label{thm:Fourier_transform_of_signal_modulated_by_direct_multiplication_PAM}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\tilde{y}(t) &= \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_s) x(t)\\
	\end{align*}
	that is, let $\tilde{y}(t)$ be the signal sampled at all $n T_s$.
	Therefore,
	\begin{align*}
		\tilde{y}(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T_s) \delta(t - n T_s)
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{Y}(\omega) &= \frac{1}{2 \pi} X(\omega) \ast G(\omega)\\
		&= \frac{1}{T_s} \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
	Therefore,
	\begin{align*}
		y(t) &= \tilde{y}(t) \ast p(t)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n T_s) p(t - n T_s)
	\end{align*}
	Therefore,
	\begin{align*}
		Y(\omega) &= \tilde{Y}(\omega) P(\omega)\\
		&= \frac{1}{T_s} P(\omega) \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
\end{proof}

\clearpage
\part{Pulse Position Modulation}

\begin{definition}[PPM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$, bounded by $x_{\text{min}}$ and $x_{\text{max}}$.
	Let $p(t)$ be a general pulse extending from $t = 0$ to $t = \Delta$.\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= \sum\limits_{n = -\infty}^{\infty} p\left( t - n T_s - \left( \alpha x(n T_s) + \beta \right) \right)
	\end{align*}
	where $\alpha$ and $\beta$ are such that
	\begin{align*}
		\alpha x_{\text{min}} + \beta &= 0\\
		\alpha x_{\text{max}} + \beta &= T_s - \Delta
	\end{align*}
	that is, the pulses of the modulated signal have delay corresponding to the amplitude of the original signal at the corresponding multiple of $T_s$.
	\footnote{The delay of the pulse is measured with respect to the left edge of the pulse.}
	Such a modulation is called pulse position modulation.
	\label{def:PPM}
\end{definition}

\clearpage
\part{Quantization}

\begin{definition}[Quantization]
	Let $x(t)$ be a deterministic signal.
	Let
	\begin{align*}
		x[n] &= x(n T_s)
	\end{align*}
	be the corresponding discrete time signal.\\
	The quantization of the signal is defined to be $x_q[n]$ where $x_q$ can have quantized values only.\\
	The quantization error is defined to be
	\begin{align*}
		q[n] &= x[n] - x_q[n]
	\end{align*}
\end{definition}

\begin{definition}[Dense quantization]
	If the quantization error $q$ is distributed uniformly in each quantization cell, the quantization is called dense quantization.
\end{definition}

\begin{definition}[Uniform quantization]
	If the quantum values are such that each quantization cells have equal width, the quantization is called uniform quantization.
\end{definition}

\begin{theorem}
	For uniform and dense quantization,
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
\end{theorem}

\begin{definition}[QSNR]
	The quantization signal to noise ratio is defined to be
	\begin{align*}
		\QSNR &= \frac{\expct\left[ x^2 \right]}{\expct\left[ q^2 \right]}
	\end{align*}
\end{definition}

\begin{theorem}
	If
	\begin{align*}
		x &\sim \uniform\left( -\frac{M}{2} \Delta,\frac{M}{2} \Delta \right)\\
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
	then
	\begin{align*}
		\QSNR &= M^2
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &\sim \uniform\left( -\frac{M}{2} \Delta,\frac{M}{2} \Delta \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ x^2 \right] &= \frac{M^2 \Delta^2}{12}
	\end{align*}
	Similarly,
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \frac{\Delta^2}{12}
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR &= \frac{\expct\left[ x^2 \right]}{\expct\left[ q^2 \right]}\\
		&= M^2
	\end{align*}
\end{proof}

\end{document}
