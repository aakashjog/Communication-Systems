\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\cdf}{\mathrm{F}}
\DeclareMathOperator{\pdf}{\mathrm{f}}

\DeclareMathOperator{\prob}{\mathrm{P}}

\DeclareMathOperator{\expct}{\mathrm{E}}

\DeclareMathOperator{\var}{\mathrm{V}}
\DeclareMathOperator{\sd}{\mathrm{\sigma}}

\DeclareMathOperator{\cov}{\mathrm{Cov}}

\DeclareMathOperator{\FT}{\mathcal{F}}
\DeclareMathOperator{\IFT}{\mathcal{F}^{-1}}
\DeclareMathOperator{\LT}{\mathcal{L}}

\DeclareMathOperator{\sinc}{\mathrm{sinc}}
\DeclareMathOperator{\rect}{\mathrm{rect}}

%opening
\title{Communication Systems}
\author{Aakash Jog}
\date{2016-17}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Prof. Arie Yeredor}\\
~\\
Office: Maabadot 121\\
E-mail: \href{mailto:arie@eng.tau.ac.il}{arie@eng.tau.ac.il}\\
Tel: \href{tel:+97236405314}{+972 3 640 5314}\\

\section{Instructor Information}

\textbf{Amir Weiss}\\
~\\
Office: Tochna 309\\
E-mail: \href{mailto:amirwei2@mail.tau.ac.il}{amirwei2@mail.tau.ac.il}\\
Tel: \href{tel:+97236406120}{+972 3 640 6120}\\

\section{Recommended Reading}

\begin{enumerate}
	\item \fullcite{Couch}
	\item \fullcite{Carlson}
	\item \fullcite{Haykin}
	\item \fullcite{HaykinMoher}
	\item \fullcite{Proakis}
	\item \fullcite{Lee}
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definitions and Theorems}

\section{Types of Communication}

\begin{definition}[Digital communication]
	Communication is said to be digital communication if the source can send only one of the predefined messages.
	Usually, the predefined messages are binary, i.e. $0$ or $1$.
	\label{def:digital_communication}
\end{definition}

\begin{definition}[Analogue communication]
	Communication is said to be analogue communication if the source can send one of infinite number of messages.
	\label{def:analogue_communication}
\end{definition}

\section{Periodicity and Finite Energy}

\begin{definition}[Finite energy signal]
	A signal $x(t)$ is said to have finite energy if
	\begin{align*}
		\int\limits_{-\infty}^{\infty} \left| x(t) \right|^2 \dif t &< \infty
	\end{align*}
	\label{def:finite_energy_signal}
\end{definition}

\begin{definition}[Periodic signal]
	A signal $x(t)$ is said to be $T_0$-periodic if for all $t$,
	\begin{align*}
		x(t + T_0) &= x(t)
	\end{align*}
	\label{def:periodic_signal}
\end{definition}

\section{Fourier Series Representation}

\begin{theorem}[Fourier series representation]
	A $T_0$-periodic signal $x(t)$ can be expressed as a Fourier series
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \omega_0 t}
	\end{align*}
	where
	\begin{align*}
		\omega_0 &= \frac{2 \pi}{T_0}
	\end{align*}
	and
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{T_0} x(t) e^{-j k \omega_0 t} \dif t
	\end{align*}
	\label{thm:Fourier_series_representation}
\end{theorem}

\section{Fourier Transform}

\begin{theorem}[Time and frequency domain expressions]
	A finite energy signal $x(t)$ can be expressed in frequency domain such that
	\begin{align*}
		X(\omega) &= \FT\left\{ x(t) \right\}\\
		&= \int\limits_{-\infty}^{\infty} x(t) e^{j \omega t} \dif t\\
		x(t) &= \IFT\left\{ X(\omega) \right\}\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) e^{j \omega t} \dif \omega
	\end{align*}
	\label{thm:time_and_frequency_domain_expressions}
\end{theorem}

\begin{theorem}[Linearity of Fourier transform]
	The Fourier transform is linear, i.e.
	\begin{align*}
		y(t) &= a_1 x_1(t) + a_2 x_2(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= a_1 X_1(\omega) + a_2 X_2(\omega)
	\end{align*}
	\label{thm:linearity_of_fourier_transform}
\end{theorem}

\begin{theorem}[Time shift]
	A time delay corresponds to multiplication by a tone in frequency domain, i.e.
	\begin{align*}
		y(t) &= x(t - \tau)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= e^{-j \omega \tau} X(\omega)
	\end{align*}
	\label{thm:time_shift}
\end{theorem}

\begin{theorem}[Frequency shift]
	A shift in frequency corresponds to a tone in time domain, i.e.
	\begin{align*}
		y(t) &= e^{j \omega_0 t} x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift}
\end{theorem}

\begin{theorem}[Time reversal]
	A reversal in time corresponds to a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(-\omega)
	\end{align*}
	\label{thm:time_reversal}
\end{theorem}

\begin{theorem}[Time conjugation]
	A conjugation in time corresponds to a conjugation and a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(-\omega)
	\end{align*}
	\label{thm:time_conjugation}
\end{theorem}

\begin{theorem}[Time conjugation and reversal]
	A conjugation and a reversal in time corresponds to a conjugation in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(\omega)
	\end{align*}
	\label{thm:time_conjugation}
\end{theorem}

\begin{theorem}[Convolution in time]
	A convolution in time corresponds to multiplication in frequency, i.e.
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\tau) x(t - \tau) \dif \tau
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= H(\omega) X(\omega)
	\end{align*}
	\label{thm:convolution_in_time}
\end{theorem}

\begin{theorem}[Multiplication in time]
	A multiplication in time corresponds to convolution in frequency, i.e.
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= \frac{1}{2 \pi} C(\omega) X(\omega)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} C(\sigma) X(\omega - \sigma) \dif \sigma
	\end{align*}
	\label{them:multiplication_in_time}
\end{theorem}

\begin{definition}[Dirac delta function]
	The Dirac delta function is defined to be $\delta(t)$ such that for $a < b$,
	\begin{align*}
		\int\limits_{a}^{b} \delta(t) \dif t &=
			\begin{cases}
				1 &;\quad a < 0 < b\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence, the value of the function for non-zero values of $t$ may or may not be zero, but the integral of the function over any interval which excludes zero is $1$.
	Hence, for all non-zero values of $t$, the function may have rapid oscillations.\\
	For example, the delta function expressed as a limiting case of a Gaussian is zero at all non-zero point and infinite at zero, while the delta function expressed as a limiting case of a sinc function has rapid oscillations at non-zero points.
	\label{thm:Dirac_delta_function}
\end{definition}

\begin{theorem}[Dirac delta in time]
	A Dirac delta in time corresponds to a constant $1$ in frequency, i.e.
	\begin{align*}
		x(t) &= \delta(t)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 1
	\end{align*}
	\label{thm:Dirac_delta_in_time}
\end{theorem}

\begin{definition}[Rectangular pulse function]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				1 &;\quad |t| \le \frac{1}{2}\\
				0 &;\quad |t| > \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\label{def:rect_function}
\end{definition}

\begin{definition}[Sinc function]
	\begin{align*}
		\sinc(t) &=
			\begin{cases}
				1 &;\quad t = 0\\
				\frac{\sin(t)}{t} &;\quad t \neq 0\\
			\end{cases}
	\end{align*}
	\label{def:sinc_function}
\end{definition}

\begin{theorem}[Dirac delta in frequency]
	A Dirac in frequency corresponds to a constant $1$ in time, i.e.
	\begin{align*}
		x(t) &= 1
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
	\label{thm:Dirac_delta_in_frequency}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		y(t) &= \rect\left( \frac{t}{T_0} \right)
	\end{align*}
	Hence,
	\begin{align*}
		Y(\omega) &= \frac{2}{\omega} \sin\left( \omega \frac{T_0}{2} \right)\\
		&= T_0 \sinc\left( \omega \frac{T_0}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \lim\limits_{T_0 \to \infty} y(t)
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \lim\limits_{T_0 \to \infty} Y(\omega)\\
		&= c \delta(\omega)
	\end{align*}
	where $c$ is a constant.\\
	Also,
	\begin{align*}
		x(0) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) \dif \omega
	\end{align*}
	Therefore,
	\begin{align*}
		1 &= x(0)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} c \delta(\omega) \dif \omega\\
		&= \frac{c}{2 \pi}
	\end{align*}
	Therefore,
	\begin{align*}
		c &= 2 \pi
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Fourier transform in terms of temporal frequency]
	\begin{align*}
		\tilde{X}(f) &= X(\omega) \Big|_{\omega = 2 \pi f}
	\end{align*}
	Hence,
	\begin{align*}
		\tilde{X}(f) &= \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} \dif t\\
		x(t) &= \int\limits_{-\infty}^{\infty} \tilde{X}(f) e^{j 2 \pi f t} \dif f
	\end{align*}
	\label{thm:Fourier_transform_in_terms_of_temporal_frequency}
\end{theorem}

\begin{theorem}[Impulse train in time]
	An impulse train in time corresponds to an impulse train in frequency, i.e.
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:impulse_train_in_time}
\end{theorem}

\begin{proof}
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j \omega_0 k t}
	\end{align*}
	where
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} \delta(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0}
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} e^{j \omega_0 k t}\\
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} 2 \pi \delta(\omega - k \omega_0)\\
		&= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
\end{proof}

\begin{theorem}[Sampling in time]
	A sampling in time corresponds to replication in frequency, i.e.
	\begin{align*}
		y(t) &= x(t) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n T_0) \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega) \ast \sum\limits_{k = -\infty}^{\infty} \frac{2 \pi}{T_0} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)\\
		&= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} X\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:sampling_in_time}
\end{theorem}

\section{Stochastic Signals}

\begin{definition}[Mean of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The mean of $x(t)$ is defined to be
	\begin{align*}
		\tilde{\eta}_X(t) &= \expct\left[ x(t) \right]
	\end{align*}
\end{definition}

\begin{definition}[Autocorrelation function of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The autocorrelation function of $x(t)$ is defined to be
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \expct\left[ x(t_1) x^*(t_2) \right]
	\end{align*}
\end{definition}

\begin{theorem}
	For a real stochastic signal $x(t)$,
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \tilde{R}_{X X}(t_2,t_1)
	\end{align*}
\end{theorem}

\begin{definition}[Wide sense stationary random process]
	A random process $X$ is said to be wide sense stationary if
	\begin{align*}
		\tilde{\eta}_X(t) &= \eta_X\\
		\tilde{R}_{X X}(t_1,t_2) &= R_{X X}(t_1 - t_2)
	\end{align*}
	that is, the mean is independent of time and the autocorrelation function is dependent on the time difference only and is independent of absolute time.
\end{definition}

\begin{theorem}
	For a WSS random process $X$,
	\begin{align*}
		R_{X X}(\tau) &= {R_{X X}}^*(-\tau)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{X X}(-\tau) &= \expct\left[ x(t - \tau) x^*(t) \right]\\
		&= \expct\left[ x^*(t) x(t - \tau) \right]\\
		&= \expct\left[ x(t) x(t - \tau) \right]^*\\
		&= {R_{X X}}^*(\tau)
	\end{align*}
\end{proof}

\begin{definition}[Spectrum of WSS random process]
	\begin{align*}
		S_{X X}(\omega) &= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
\end{definition}


\begin{theorem}
	The spectrum of a random process is the mean of the squared absolute value of the Fourier transform of a long enough segment of a sample function of the process, normalized by the length of the segment, i.e.
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t \right| \right]
	\end{align*}
\end{theorem}

\begin{proof}
	Let $x(t)$ be a sample function of a WSS random process.
	As $R_{X X}(0) = \expct\left[ \left| x(t) \right|^2 \right]$ is constant irrespective of the absolute time, the signal $x(t)$ never converges.
	Therefore, it is not a finite energy signal and hence does not have a Fourier transform.\\
	Hence, let
	\begin{align*}
		X_T(\omega) &= \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \expct\left[ \int\limits_{0}^{T} x(t_1) e^{-j \omega t_1} \dif t_1 \int\limits_{0}^{T} x^*(t_2) e^{j \omega t_2} \dif t_2 \right]\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} \expct\left[ x(t_1) x^*(t_2) \right] e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} {\tilde{R}_{X X}}(t_1,t_2) e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} R_{X X}(t_1 - t_2) e^{-j \omega (t_1 - t_2)} \dif t_1 \dif t_2
	\end{align*}
	Let
	\begin{align*}
		\tau &= t_1 - t_2\\
		\sigma &= t_1 + t_2
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\tau\\
				\sigma\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 1\\
			\end{pmatrix}
			\begin{pmatrix}
				t_1\\
				t_2\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\tau,\sigma)}{(t_1,t_2)} \right| &= 2
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \int\limits_{-T}^{T} \int\limits_{|\tau|}^{2 T - |\tau|} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \dif \sigma \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{T} \int\limits_{|\tau|}^{2 T - |\tau|} \dif \sigma \dif \tau \frac{1}{2}\\
		&= \frac{-T}{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \left( \frac{2 T - 2 |\tau|}{2} \right) \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \left( 1 - \frac{|\tau|}{T} \right) \dif \tau\\
		&= \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
	Therefore, assuming the Ergodicity condition $\int\limits_{-\infty}^{\infty} \left| \tau R_{X X}(\tau) \right| \dif \tau < M < \infty$,
	\begin{align*}
		\lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \lim\limits_{T \to \infty} \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		S_{X X}(\omega) &\ge 0
	\end{align*}
	for all $\omega$.
	\label{thm:non_negativity_of_spectrum}
\end{theorem}

\begin{proof}
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right]\\
		&\ge 0
	\end{align*}
\end{proof}

\begin{theorem}[Time shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then,
	\begin{align*}
		y(t) &= x(t - t_0)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t - t_0 + \tau) x^*(t - t_0) \right]\\
		&= R_{X X}(\tau)
	\end{align*}
	Hence,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Frequency shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(t) e^{j \omega_0 t}
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t + \tau) e^{j \omega_0 (t + \tau)} x^*(t) e^{-j \omega_0 t} \right]\\
		&= e^{j \omega_0 \tau} \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= e^{j \omega_0 \tau} R_{X X}(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
\end{proof}

\begin{theorem}[Time reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(-t - \tau) x^*(-t) \right]\\
		&= R_{X X}(-\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Time conjugation for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x^*(t + \tau) x(t) \right]\\
		&= R_{X X}(-\tau)\\
	\end{align*}
	Equivalently,
	\begin{align*}
		R_{Y Y}(\tau) &= {R_{X X}}^*(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	and
	\begin{align*}
		S_{Y Y}(\omega) &= {S_{X X}}^*(-\omega)
	\end{align*}
	This is consistent with \cref{thm:non_negativity_of_spectrum}.
\end{proof}

\begin{theorem}[Time conjugation and reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{theorem}[Convolution in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\mu) x(t - \mu) \dif \mu
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
	\label{thm:convolution_in_time_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ \int\limits_{-\infty}^{\infty} h(\mu) x(t + \tau - \mu) \dif \mu \int\limits_{-\infty}^{\infty} h^*(\sigma) x^*(t - \sigma) \dif \sigma \right]\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) \expct\left[ x(t + \tau - \mu) x^*(t - \sigma) \right] \dif \mu \dif \sigma\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) R_{X X}(t - \mu + \sigma) \dif \mu \dif \sigma
	\end{align*}
	Let
	\begin{align*}
		\alpha &= \mu - \sigma\\
		\beta &= \mu
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\alpha\\
				\beta\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 0\\
			\end{pmatrix}
			\begin{pmatrix}
				\mu\\
				\sigma\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\alpha,\beta)}{(\mu,\sigma)} \right| &= 1
	\end{align*}
	Therefore,
	\begin{align*}
		R_{Y Y}(\tau) &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) R_{X X}(\tau - \alpha) \dif \alpha \dif \beta
	\end{align*}
	Let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) \dif \beta R_{X X}(\tau - \alpha) \dif \alpha
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= G(\omega) S_{X X}(\omega)
	\end{align*}
	Additionally, let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) \overline{h}(\alpha - \beta) \dif \beta
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) \overline{H}(\omega)
	\end{align*}
	Hence, by the two definitions of $g(\alpha)$,
	\begin{align*}
		\overline{h}(\tau) &= h^*(-\tau)\\
	\end{align*}
	Therefore,
	\begin{align*}
		\overline{H}(\omega) &= H^*(\omega)
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) H^*(\omega)\\
		&= \left| H(\omega) \right|^2
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $Y$ may or may not be a WSS random process.
	\footnote{This is intuitively correct as, for example, if a WSS random process is multiplied by a finite window, the product is zero outside the window, and hence is not WSS.}
	\footnote{If $c(t)$ is periodic, $Y$ is cyclostationary. See \cref{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}}
	\label{thm:multiplication_in_time_for_stochastic_signal_and_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)
	\end{align*}
	Hence, as the autocorrelation function of $Y$ may be dependent on the absolute time, $Y$ may or may not be WSS.
\end{proof}

\section{Cyclostationary Processes}

\begin{definition}[Cyclostationary process]
	A random process is said to be cyclostationary with a cyclic period $T_0$ if the mean $\tilde{\eta}_X(t)$ and $\tilde{R}_{X X}(t,\tau)$ are $T_0$ periodic in $t$, i.e. if for all $t$ and $\tau$,
	\begin{align*}
		\tilde{\eta}_X(t + T_0) &= \tilde{\eta}_X(t)\\
		\tilde{R}_{X X}(t + T_0,\tau) &= \tilde{R}_{X X}(t,\tau)
	\end{align*}
\end{definition}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a $T_0$ periodic deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $y(t)$ is cyclostationary with $T_0$.
	\label{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		\tilde{\eta}_Y(t) &= \expct\left[ c(t) x(t) \right]\\
		&= c(t) \expct\left[ x(t) \right]\\
		&= c(t) \eta_X\\
		&= c(t + T_0) \eta_X\\
		&= \tilde{\eta}_Y(t + T_0)\\
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)\\
		&= c(t + \tau + T_0) c^*(t + T_0) R_{X X}(\tau)\\
		&= \tilde{R}_{Y Y}(t + T_0,\tau)
	\end{align*}
	Hence, $Y$ is cyclostationary with $T_0$.
\end{proof}

\end{document}
