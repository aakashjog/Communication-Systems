\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{adjustbox}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\tikzset{
block/.style = {draw, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate},
sum/.style = {draw, circle, node distance=1cm},
input/.style = {coordinate},
output/.style = {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}},
every text node part/.style={align=center}
}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}
\renewcommand{\hat}{\widehat}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\cdf}{\mathrm{F}}
\DeclareMathOperator{\pdf}{\mathrm{f}}

\DeclareMathOperator{\prob}{\mathrm{P}}

\DeclareMathOperator{\expct}{\mathrm{E}}

\DeclareMathOperator{\bin}{\mathrm{Bin}}
\DeclareMathOperator{\poi}{\mathrm{Poi}}
\DeclareMathOperator{\geo}{\mathrm{Geo}}
\DeclareMathOperator{\nb}{\mathrm{NB}}
\DeclareMathOperator{\hg}{\mathrm{HG}}
\DeclareMathOperator{\uniform}{\mathrm{U}}
\DeclareMathOperator{\exponential}{\mathrm{Exp}}
\DeclareMathOperator{\normal}{\mathrm{N}}

\DeclareMathOperator{\var}{\mathrm{V}}
\DeclareMathOperator{\sd}{\mathrm{\sigma}}

\DeclareMathOperator{\cov}{\mathrm{Cov}}

\DeclareMathOperator{\FT}{\mathcal{F}}
\DeclareMathOperator{\IFT}{\mathcal{F}^{-1}}
\DeclareMathOperator{\LT}{\mathcal{L}}

\DeclareMathOperator{\sinc}{\mathrm{sinc}}
\DeclareMathOperator{\rect}{\mathrm{rect}}
\DeclareMathOperator{\tri}{\mathrm{tri}}

\DeclareMathOperator{\sign}{\mathrm{sign}}

\DeclareMathOperator{\QSNR}{\mathrm{QSNR}}
\DeclareMathOperator{\Q}{\mathcal{Q}}

\DeclareMathOperator{\SNR}{\mathrm{SNR}}

\DeclareMathOperator*{\argmin}{\mathrm{argmin}}

\DeclareMathOperator{\atan2}{\mathrm{atan2}}

\DeclareMathOperator{\DSB}{\mathrm{DSB}}
\DeclareMathOperator{\AM}{\mathrm{AM}}
\DeclareMathOperator{\USB}{\mathrm{USB}}
\DeclareMathOperator{\LSB}{\mathrm{LSB}}

\DeclareMathOperator{\IF}{\mathrm{IF}}

\DeclareSIUnit{\dBm}{dBm}

%opening
\title{Communication Systems}
\author{Aakash Jog}
\date{2016-17}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Prof. Arie Yeredor}\\
~\\
Office: Maabadot 121\\
E-mail: \href{mailto:arie@eng.tau.ac.il}{arie@eng.tau.ac.il}\\
Tel: \href{tel:+97236405314}{+972 3 640 5314}\\

\section{Instructor Information}

\textbf{Amir Weiss}\\
~\\
Office: Tochna 309\\
E-mail: \href{mailto:amirwei2@mail.tau.ac.il}{amirwei2@mail.tau.ac.il}\\
Tel: \href{tel:+97236406120}{+972 3 640 6120}\\

\section{Recommended Reading}

\begin{enumerate}
	\item \fullcite{Couch}
	\item \fullcite{Carlson}
	\item \fullcite{Haykin}
	\item \fullcite{HaykinMoher}
	\item \fullcite{Proakis}
	\item \fullcite{Lee}
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definitions and Theorems}

\section{Types of Communication}

\begin{definition}[Digital communication]
	Communication is said to be digital communication if the source can send only one of the predefined messages.
	Usually, the predefined messages are binary, i.e. $0$ or $1$.
	\label{def:digital_communication}
\end{definition}

\begin{definition}[Analogue communication]
	Communication is said to be analogue communication if the source can send one of infinite number of messages.
	\label{def:analogue_communication}
\end{definition}

\section{Periodicity and Finite Energy}

\begin{definition}[Finite energy signal]
	A signal $x(t)$ is said to have finite energy if
	\begin{align*}
		\int\limits_{-\infty}^{\infty} \left| x(t) \right|^2 \dif t &< \infty
	\end{align*}
	\label{def:finite_energy_signal}
\end{definition}

\begin{definition}[Periodic signal]
	A signal $x(t)$ is said to be $T_0$-periodic if for all $t$,
	\begin{align*}
		x(t + T_0) &= x(t)
	\end{align*}
	\label{def:periodic_signal}
\end{definition}

\section{Fourier Series Representation}

\begin{theorem}[Fourier series representation]
	A $T_0$-periodic signal $x(t)$ can be expressed as a Fourier series
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j k \omega_0 t}
	\end{align*}
	where
	\begin{align*}
		\omega_0 &= \frac{2 \pi}{T_0}
	\end{align*}
	and
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{T_0} x(t) e^{-j k \omega_0 t} \dif t
	\end{align*}
	\label{thm:Fourier_series_representation}
\end{theorem}

\section{Fourier Transform}

\begin{theorem}[Time and frequency domain expressions]
	A finite energy signal $x(t)$ can be expressed in frequency domain such that
	\begin{align*}
		X(\omega) &= \FT\left\{ x(t) \right\}\\
		&= \int\limits_{-\infty}^{\infty} x(t) e^{j \omega t} \dif t\\
		x(t) &= \IFT\left\{ X(\omega) \right\}\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) e^{j \omega t} \dif \omega
	\end{align*}
	\label{thm:time_and_frequency_domain_expressions}
\end{theorem}

\begin{theorem}[Linearity of Fourier transform]
	The Fourier transform is linear, i.e.
	\begin{align*}
		y(t) &= a_1 x_1(t) + a_2 x_2(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= a_1 X_1(\omega) + a_2 X_2(\omega)
	\end{align*}
	\label{thm:linearity_of_fourier_transform}
\end{theorem}

\begin{theorem}[Time shift]
	A time delay corresponds to multiplication by a tone in frequency domain, i.e.
	\begin{align*}
		y(t) &= x(t - \tau)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= e^{-j \omega \tau} X(\omega)
	\end{align*}
	\label{thm:time_shift}
\end{theorem}

\begin{theorem}[Frequency shift]
	A shift in frequency corresponds to a tone in time domain, i.e.
	\begin{align*}
		y(t) &= e^{j \omega_0 t} x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift}
\end{theorem}

\begin{theorem}[Time reversal]
	A reversal in time corresponds to a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(-\omega)
	\end{align*}
	\label{thm:time_reversal}
\end{theorem}

\begin{theorem}[Time conjugation]
	A conjugation in time corresponds to a conjugation and a reversal in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(-\omega)
	\end{align*}
	\label{thm:time_conjugation}
\end{theorem}

\begin{theorem}[Time conjugation and reversal]
	A conjugation and a reversal in time corresponds to a conjugation in frequency, i.e.
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X^*(\omega)
	\end{align*}
	\label{thm:time_conjugation_and_reversal}
\end{theorem}

\begin{theorem}[Convolution in time]
	A convolution in time corresponds to multiplication in frequency, i.e.
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\tau) x(t - \tau) \dif \tau
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= H(\omega) X(\omega)
	\end{align*}
	\label{thm:convolution_in_time}
\end{theorem}

\begin{theorem}[Multiplication in time]
	A multiplication in time corresponds to convolution in frequency, i.e.
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= \frac{1}{2 \pi} C(\omega) X(\omega)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} C(\sigma) X(\omega - \sigma) \dif \sigma
	\end{align*}
	\label{them:multiplication_in_time}
\end{theorem}

\begin{definition}[Dirac delta function]
	The Dirac delta function is defined to be $\delta(t)$ such that for $a < b$,
	\begin{align*}
		\int\limits_{a}^{b} \delta(t) \dif t &=
			\begin{cases}
				1 &;\quad a < 0 < b\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence, the value of the function for non-zero values of $t$ may or may not be zero, but the integral of the function over any interval which excludes zero is $1$.
	Hence, for all non-zero values of $t$, the function may have rapid oscillations.\\
	For example, the delta function expressed as a limiting case of a Gaussian is zero at all non-zero point and infinite at zero, while the delta function expressed as a limiting case of a sinc function has rapid oscillations at non-zero points.
	\label{thm:Dirac_delta_function}
\end{definition}

\begin{theorem}[Dirac delta in time]
	A Dirac delta in time corresponds to a constant $1$ in frequency, i.e.
	\begin{align*}
		x(t) &= \delta(t)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 1
	\end{align*}
	\label{thm:Dirac_delta_in_time}
\end{theorem}

\begin{definition}[Rectangular pulse function]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				1 &;\quad |t| \le \frac{1}{2}\\
				0 &;\quad |t| > \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\label{def:rect_function}
\end{definition}

\begin{definition}[Triangular pulse function]
	\begin{align*}
		\tri(t) &=
			\begin{cases}
				1 - |t| &;\quad |t| \le 1\\
				0 &;\quad |t| > 1\\
			\end{cases}
	\end{align*}
	\label{def:tri_function}
\end{definition}

\begin{definition}[Sinc function]
	\begin{align*}
		\sinc(t) &=
			\begin{cases}
				1 &;\quad t = 0\\
				\frac{\sin(t)}{t} &;\quad t \neq 0\\
			\end{cases}
	\end{align*}
	\label{def:sinc_function}
\end{definition}

\begin{theorem}[Dirac delta in frequency]
	A Dirac in frequency corresponds to a constant $1$ in time, i.e.
	\begin{align*}
		x(t) &= 1
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
	\label{thm:Dirac_delta_in_frequency}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		y(t) &= \rect\left( \frac{t}{T_0} \right)
	\end{align*}
	Hence,
	\begin{align*}
		Y(\omega) &= \frac{2}{\omega} \sin\left( \omega \frac{T_0}{2} \right)\\
		&= T_0 \sinc\left( \omega \frac{T_0}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \lim\limits_{T_0 \to \infty} y(t)
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \lim\limits_{T_0 \to \infty} Y(\omega)\\
		&= c \delta(\omega)
	\end{align*}
	where $c$ is a constant.\\
	Also,
	\begin{align*}
		x(0) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} X(\omega) \dif \omega
	\end{align*}
	Therefore,
	\begin{align*}
		1 &= x(0)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} c \delta(\omega) \dif \omega\\
		&= \frac{c}{2 \pi}
	\end{align*}
	Therefore,
	\begin{align*}
		c &= 2 \pi
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= 2 \pi \delta(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Fourier transform in terms of temporal frequency]
	\begin{align*}
		\tilde{X}(f) &= X(\omega) \Big|_{\omega = 2 \pi f}
	\end{align*}
	Hence,
	\begin{align*}
		\tilde{X}(f) &= \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} \dif t\\
		x(t) &= \int\limits_{-\infty}^{\infty} \tilde{X}(f) e^{j 2 \pi f t} \dif f
	\end{align*}
	\label{thm:Fourier_transform_in_terms_of_temporal_frequency}
\end{theorem}

\begin{theorem}[Impulse train in time]
	An impulse train in time corresponds to an impulse train in frequency, i.e.
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		X(\omega) &= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:impulse_train_in_time}
\end{theorem}

\begin{proof}
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} a_k e^{j \omega_0 k t}
	\end{align*}
	where
	\begin{align*}
		a_k &= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0} \int\limits_{-\frac{T_0}{2}}^{\frac{T_0}{2}} \delta(t) e^{-j \omega_0 k t} \dif t\\
		&= \frac{1}{T_0}
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} e^{j \omega_0 k t}\\
	\end{align*}
	Hence,
	\begin{align*}
		X(\omega) &= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} 2 \pi \delta(\omega - k \omega_0)\\
		&= \frac{2 \pi}{T_0} \sum\limits_{k = -\infty}^{\infty} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
\end{proof}

\begin{theorem}[Sampling in time]
	A sampling in time corresponds to replication in frequency, i.e.
	\begin{align*}
		y(t) &= x(t) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_0)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n T_0) \delta(t - n T_0)
	\end{align*}
	if and only if
	\begin{align*}
		Y(\omega) &= X(\omega) \ast \sum\limits_{k = -\infty}^{\infty} \frac{2 \pi}{T_0} \delta\left( \omega - \frac{2 \pi}{T_0} k \right)\\
		&= \frac{1}{T_0} \sum\limits_{k = -\infty}^{\infty} X\left( \omega - \frac{2 \pi}{T_0} k \right)
	\end{align*}
	\label{thm:sampling_in_time}
\end{theorem}

\section{Stochastic Signals}

\begin{definition}[Mean of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The mean of $x(t)$ is defined to be
	\begin{align*}
		\tilde{\eta}_X(t) &= \expct\left[ x(t) \right]
	\end{align*}
\end{definition}

\begin{definition}[Autocorrelation function of stochastic signal]
	Let $x(t)$ be a sample function of a random process.
	The autocorrelation function of $x(t)$ is defined to be
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \expct\left[ x(t_1) x^*(t_2) \right]
	\end{align*}
\end{definition}

\begin{theorem}
	For a real stochastic signal $x(t)$,
	\begin{align*}
		\tilde{R}_{X X}(t_1,t_2) &= \tilde{R}_{X X}(t_2,t_1)
	\end{align*}
\end{theorem}

\begin{definition}[Wide sense stationary random process]
	A random process $X$ is said to be wide sense stationary if
	\begin{align*}
		\tilde{\eta}_X(t) &= \eta_X\\
		\tilde{R}_{X X}(t_1,t_2) &= R_{X X}(t_1 - t_2)
	\end{align*}
	that is, the mean is independent of time and the autocorrelation function is dependent on the time difference only and is independent of absolute time.
\end{definition}

\begin{theorem}
	For a WSS random process $X$,
	\begin{align*}
		R_{X X}(\tau) &= {R_{X X}}^*(-\tau)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{X X}(-\tau) &= \expct\left[ x(t - \tau) x^*(t) \right]\\
		&= \expct\left[ x^*(t) x(t - \tau) \right]\\
		&= \expct\left[ x(t) x(t - \tau) \right]^*\\
		&= {R_{X X}}^*(\tau)
	\end{align*}
\end{proof}

\begin{definition}[Spectrum of WSS random process]
	\begin{align*}
		S_{X X}(\omega) &= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
\end{definition}


\begin{theorem}
	The spectrum of a random process is the mean of the squared absolute value of the Fourier transform of a long enough segment of a sample function of the process, normalized by the length of the segment, i.e.
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t \right| \right]
	\end{align*}
\end{theorem}

\begin{proof}
	Let $x(t)$ be a sample function of a WSS random process.
	As $R_{X X}(0) = \expct\left[ \left| x(t) \right|^2 \right]$ is constant irrespective of the absolute time, the signal $x(t)$ never converges.
	Therefore, it is not a finite energy signal and hence does not have a Fourier transform.\\
	Hence, let
	\begin{align*}
		X_T(\omega) &= \int\limits_{0}^{T} x(t) e^{-j \omega t} \dif t
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \expct\left[ \int\limits_{0}^{T} x(t_1) e^{-j \omega t_1} \dif t_1 \int\limits_{0}^{T} x^*(t_2) e^{j \omega t_2} \dif t_2 \right]\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} \expct\left[ x(t_1) x^*(t_2) \right] e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} {\tilde{R}_{X X}}(t_1,t_2) e^{-j \omega t_1} e^{j \omega t_2} \dif t_1 \dif t_2\\
		&= \frac{1}{T} \int\limits_{0}^{T} \int\limits_{0}^{T} R_{X X}(t_1 - t_2) e^{-j \omega (t_1 - t_2)} \dif t_1 \dif t_2
	\end{align*}
	Let
	\begin{align*}
		\tau &= t_1 - t_2\\
		\sigma &= t_1 + t_2
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\tau\\
				\sigma\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 1\\
			\end{pmatrix}
			\begin{pmatrix}
				t_1\\
				t_2\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\tau,\sigma)}{(t_1,t_2)} \right| &= 2
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \frac{1}{T} \int\limits_{-T}^{T} \int\limits_{|\tau|}^{2 T - |\tau|} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \dif \sigma \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{T} \int\limits_{|\tau|}^{2 T - |\tau|} \dif \sigma \dif \tau \frac{1}{2}\\
		&= \frac{-T}{T} R_{X X}(\tau) e^{-j \omega \tau} \frac{1}{2} \left( \frac{2 T - 2 |\tau|}{2} \right) \dif \tau\\
		&= \int\limits_{-T}^{T} R_{X X}(\tau) e^{-j \omega \tau} \left( 1 - \frac{|\tau|}{T} \right) \dif \tau\\
		&= \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
	Therefore, assuming the Ergodicity condition $\int\limits_{-\infty}^{\infty} \left| \tau R_{X X}(\tau) \right| \dif \tau < M < \infty$,
	\begin{align*}
		\lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right] &= \lim\limits_{T \to \infty} \int\limits_{-\tau}^{\tau} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau - \frac{1}{T} \int\limits_{-\tau}^{\tau} |\tau| R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= \int\limits_{-\infty}^{\infty} R_{X X}(\tau) e^{-j \omega \tau} \dif \tau\\
		&= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		S_{X X}(\omega) &\ge 0
	\end{align*}
	for all $\omega$.
	\label{thm:non_negativity_of_spectrum}
\end{theorem}

\begin{proof}
	\begin{align*}
		S_{X X}(\omega) &= \lim\limits_{T \to \infty} \frac{1}{T} \expct\left[ \left| X_T(\omega) \right|^2 \right]\\
		&\ge 0
	\end{align*}
\end{proof}

\begin{theorem}[Time shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then,
	\begin{align*}
		y(t) &= x(t - t_0)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t - t_0 + \tau) x^*(t - t_0) \right]\\
		&= R_{X X}(\tau)
	\end{align*}
	Hence,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Frequency shift for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(t) e^{j \omega_0 t}
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
	\label{thm:frequency_shift_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(t + \tau) e^{j \omega_0 (t + \tau)} x^*(t) e^{-j \omega_0 t} \right]\\
		&= e^{j \omega_0 \tau} \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= e^{j \omega_0 \tau} R_{X X}(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega - \omega_0)
	\end{align*}
\end{proof}

\begin{theorem}[Time reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_reversal_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x(-t - \tau) x^*(-t) \right]\\
		&= R_{X X}(-\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Time conjugation for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	\label{thm:time_conjugation_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ x^*(t + \tau) x(t) \right]\\
		&= R_{X X}(-\tau)\\
	\end{align*}
	Equivalently,
	\begin{align*}
		R_{Y Y}(\tau) &= {R_{X X}}^*(\tau)
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(-\omega)
	\end{align*}
	and
	\begin{align*}
		S_{Y Y}(\omega) &= {S_{X X}}^*(-\omega)
	\end{align*}
	This is consistent with \cref{thm:non_negativity_of_spectrum}.
\end{proof}

\begin{theorem}[Time conjugation and reversal for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Then
	\begin{align*}
		y(t) &= x^*(-t)
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= S_{X X}(\omega)
	\end{align*}
	\label{thm:time_conjugation_and_reversal_for_stochastic_signal}
\end{theorem}

\begin{theorem}[Convolution in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	\begin{align*}
		y(t) &= h(t) \ast x(t)\\
		&= \int\limits_{-\infty}^{\infty} h(\mu) x(t - \mu) \dif \mu
	\end{align*}
	if and only if
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
	\label{thm:convolution_in_time_for_stochastic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ \int\limits_{-\infty}^{\infty} h(\mu) x(t + \tau - \mu) \dif \mu \int\limits_{-\infty}^{\infty} h^*(\sigma) x^*(t - \sigma) \dif \sigma \right]\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) \expct\left[ x(t + \tau - \mu) x^*(t - \sigma) \right] \dif \mu \dif \sigma\\
		&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\mu) h^*(\sigma) R_{X X}(t - \mu + \sigma) \dif \mu \dif \sigma
	\end{align*}
	Let
	\begin{align*}
		\alpha &= \mu - \sigma\\
		\beta &= \mu
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				\alpha\\
				\beta\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				1 & -1\\
				1 & 0\\
			\end{pmatrix}
			\begin{pmatrix}
				\mu\\
				\sigma\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \dpd{(\alpha,\beta)}{(\mu,\sigma)} \right| &= 1
	\end{align*}
	Therefore,
	\begin{align*}
		R_{Y Y}(\tau) &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) R_{X X}(\tau - \alpha) \dif \alpha \dif \beta
	\end{align*}
	Let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) h^*(\beta - \alpha) \dif \beta R_{X X}(\tau - \alpha) \dif \alpha
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= G(\omega) S_{X X}(\omega)
	\end{align*}
	Additionally, let
	\begin{align*}
		g(\alpha) &= \int\limits_{-\infty}^{\infty} h(\beta) \overline{h}(\alpha - \beta) \dif \beta
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) \overline{H}(\omega)
	\end{align*}
	Hence, by the two definitions of $g(\alpha)$,
	\begin{align*}
		\overline{h}(\tau) &= h^*(-\tau)\\
	\end{align*}
	Therefore,
	\begin{align*}
		\overline{H}(\omega) &= H^*(\omega)
	\end{align*}
	Therefore,
	\begin{align*}
		G(\omega) &= H(\omega) H^*(\omega)\\
		&= \left| H(\omega) \right|^2
	\end{align*}
	Therefore,
	\begin{align*}
		S_{Y Y}(\omega) &= \left| H(\omega) \right|^2 S_{X X}(\omega)
	\end{align*}
\end{proof}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $Y$ may or may not be a WSS random process.
	\footnote{This is intuitively correct as, for example, if a WSS random process is multiplied by a finite window, the product is zero outside the window, and hence is not WSS.}
	\footnote{If $c(t)$ is periodic, $Y$ is cyclostationary. See \cref{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}}
	\label{thm:multiplication_in_time_for_stochastic_signal_and_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)
	\end{align*}
	Hence, as the autocorrelation function of $Y$ may be dependent on the absolute time, $Y$ may or may not be WSS.
\end{proof}

\begin{definition}[Circular random process]
	A stochastic process is said to be circular or proper if
	\begin{align*}
		\tilde{R}_{X X^*}(t_1,t_2) &= 0
	\end{align*}
	\label{def:circular_random_process}
\end{definition}

\section{Cyclostationary Processes}

\begin{definition}[Cyclostationary process]
	A random process is said to be cyclostationary with a cyclic period $T_0$ if the mean $\tilde{\eta}_X(t)$ and $\tilde{R}_{X X}(t,\tau)$ are $T_0$ periodic in $t$, i.e. if for all $t$ and $\tau$,
	\begin{align*}
		\tilde{\eta}_X(t + T_0) &= \tilde{\eta}_X(t)\\
		\tilde{R}_{X X}(t + T_0,\tau) &= \tilde{R}_{X X}(t,\tau)
	\end{align*}
\end{definition}

\begin{theorem}[Multiplication in time for stochastic signal]
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a $T_0$ periodic deterministic function.\\
	Let
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Then, $y(t)$ is cyclostationary with $T_0$.
	\label{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal}
\end{theorem}

\begin{proof}
	\begin{align*}
		\tilde{\eta}_Y(t) &= \expct\left[ c(t) x(t) \right]\\
		&= c(t) \expct\left[ x(t) \right]\\
		&= c(t) \eta_X\\
		&= c(t + T_0) \eta_X\\
		&= \tilde{\eta}_Y(t + T_0)\\
		R_{Y Y}(t,\tau) &= \expct\left[ y(t + \tau) y^*(t) \right]\\
		&= \expct\left[ c(t + \tau) x(t + \tau) c^*(t) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) \expct\left[ x(t + \tau) x^*(t) \right]\\
		&= c(t + \tau) c^*(t) R_{X X}(\tau)\\
		&= c(t + \tau + T_0) c^*(t + T_0) R_{X X}(\tau)\\
		&= \tilde{R}_{Y Y}(t + T_0,\tau)
	\end{align*}
	Hence, $Y$ is cyclostationary with $T_0$.
\end{proof}

\begin{theorem}
	Let $x(t)$ be a WSS random process with spectrum $S_{X X}(\omega)$.\\
	Let $c(t)$ be a $T_0$ periodic deterministic function.\\
	Let
	\begin{align*}
		\overline{y}(t) &= c(t - \varphi) x(t)
	\end{align*}
	where $\varphi \sim \uniform(0,T_0)$ is independent of $x(t)$.\\
	Then, $\overline{y}(t)$ is WSS with
	\begin{align*}
		\eta_{\overline{Y}} &= m_c \eta_X\\
		R_{\overline{Y} \overline{Y}}(\tau) &= r_c(\tau) R_{X X}(\tau)
	\end{align*}
	where
	\begin{align*}
		m_c &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t) \dif t\\
		r_c(\tau) &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau) c^*(t) \dif t
	\end{align*}
	and hence,
	\begin{align*}
		S_{\overline{Y} \overline{Y}}(\omega) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} P_c(\sigma) S_{X X}(\omega - \sigma) \dif \sigma
	\end{align*}
	where
	\begin{align*}
		P_c(\omega) &= \FT\left\{ r_c(\tau) \right\}\\
		&= \int\limits_{-\infty}^{\infty} r_c(\tau) e^{-j \omega \tau} \dif \tau
	\end{align*}
	\label{thm:multiplication_in_time_for_stochastic_signal_and_periodic_deterministic_signal_dependent_on_random_variable}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\overline{y}(t) &= c(t - \varphi) x(t)
	\end{align*}
	where $\varphi$ is a random variable independent of $x(t)$.\\
	Therefore,
	\begin{align*}
		\tilde{\eta}_{\overline{Y}}(t) &= \expct\left[ c(t - \varphi) x(t) \right]\\
		&= \expct\left[ \expct\left[ c(t - \varphi) x(t) \Big| \varphi \right] \right]\\
		&= \expct_{\varphi}\left[ c(t - \varphi) \eta_X \right]\\
		&= \expct\left[ c(t - \varphi) \right] \eta_X\\
		\tilde{R}_{\overline{Y} \overline{Y}}(t,\tau) &= \expct\left[ \overline{y}(t + \tau) \overline{y}^*(t) \right]\\
		&= \expct\left[ \expct\left[ \overline{y}(t + \tau) \overline{y}^+(t) \Big| \varphi \right] \right]\\
		&= \expct\left[ \expct\left[ c(t + \tau - \varphi) x(t + \tau) c^*(t - \varphi) x^*(t) \Big| \varphi \right] \right]\\
		&= \expct_{\varphi}\left[ c(t + \tau - \varphi) c^*(t - \varphi) R_{X X}(\tau) \right]\\
		&= \expct\left[ c(t + \tau - \varphi) c^*(t - \varphi) \right] R_{X X}(\tau)
	\end{align*}
	As $c(t)$ is $T_0$ periodic and $\varphi \sim \uniform(0,T_0)$,
	\begin{align*}
		\expct\left[ c(t - \varphi) \right] &= \int\limits_{-\infty}^{\infty} \pdf_{\varphi}(\varphi) c(t - \varphi) \dif \varphi\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t - \varphi) \dif \varphi
	\end{align*}
	Let
	\begin{align*}
		\varphi' &= t - \varphi\\
		\therefore \dif \varphi' &= \dif \varphi
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ c(t - \varphi) \right] &= \frac{1}{T_0} \int\limits_{t - T_0}^{t} c(\varphi') \dif \varphi'\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t) \dif t\\
		&= m_c
	\end{align*}
	Similarly,
	\begin{align*}
		\expct\left[ c(t + \tau - \varphi) c^*(t - \varphi) \right] &= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau - \varphi) c^*(t - \varphi) \dif \varphi\\
		&= \frac{1}{T_0} \int\limits_{0}^{T_0} c(t + \tau) c^*(t) \dif t\\
		&= r_c(\tau)
	\end{align*}
\end{proof}

\begin{question}
	Consider the zero mean WSS process $X(t)$ with autocorrelation function $R_{X X}(\tau)$.
	Let
	\begin{align*}
		Y(t) &= X(t) c(t - \varphi)
	\end{align*}
	where
	\begin{align*}
		c(t) &= \cos(\omega_0 t)\\
		\cdf_{\varphi}(\phi) &=
			\begin{cases}
				\frac{\omega_0}{\pi} \left( 1 - \frac{\omega_0}{\pi} \phi \right) &;\quad |\phi| \le \frac{\pi}{\omega_0}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Show that $Y(t)$ is WSS.
\end{question}

\begin{solution}
	\begin{align*}
		\tilde{\eta}_Y(t) &= \expct\left[ Y(t) \right]\\
		&= \expct\left[ X(t) c(t - \phi) \right]\\
		&= \expct\left[ X(t) \right] \expct\left[ \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= 0\\
		\tilde{R}_{Y Y}(t,\tau) &= \expct\left[ Y(t + \tau) Y(t) \right]\\
		&= \expct\left[ X(t + \tau) \cos\left( \omega_0 (t + \tau - \varphi) \right) x^*(t) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= \expct\left[ x(t + \tau) x^*(t) \right] \expct\left[ \cos\left( \omega_0 (t + \tau - \varphi) \right) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= R_{X X}(\tau) \expct\left[ \cos\left( \omega_0 (t + \tau - \varphi) \right) \cos\left( \omega_0 (t - \varphi) \right) \right]\\
		&= \frac{R_{X X}(\tau)}{2} \expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) + \cos(\omega_0 \tau) \right]\\
		&= \frac{R_{X X}(\tau)}{2} \cos(\omega_0 \tau) + \frac{R_{X X}(\tau)}{2} \expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) \right]
	\end{align*}
	Therefore, solving by definition,
	\begin{align*}
		\expct\left[ \cos\left( \omega_0 (2 t - 2 \varphi + \tau) \right) \right] &= 0
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{R}_{Y Y}(t,\tau) &= \frac{R_{X X}(\tau) \cos(\omega_0 \tau)}{2}
	\end{align*}
	Therefore, as the mean is independent of time and the autocorrelation function is dependent on the time difference only, $Y(t)$ is WSS.
\end{solution}

\clearpage
\part{Pulse Amplitude Modulation}

\section{Direct Multiplication / Natural Sampling}

\begin{definition}[Direct Multiplication PAM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$.\\
	Let $c(t)$ be given by a train of pulses, such that
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	where $p(t)$ is a general pulse.
	\footnote{$\rect\left( \frac{t}{\Delta} \right)$ is a commonly used pulse.}\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= c(t) x(t)
	\end{align*}
	Such a modulation is called direct multiplication pulse amplitude modulation.
	\label{def:direct_multiplication_PAM}
\end{definition}

\begin{theorem}
	For a signal $x(t)$ modulated with direct multiplication PAM,
	\begin{align*}
		Y(\omega) &= \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - \omega_s k)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ scaled according to $a_k$ with adjacent replicas separated $\omega_s$, and in the time domain, the pulses are distorted and have the shape of the original signal.
	\footnote{As these replicas differ by a constant factor only, the signal can be recreated using any replica. In order to avoid noise in the reconstruction, the replica used in reconstruction is that for which the noise is the lowest.}
	Hence, in order to avoid aliasing,
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
	\label{thm:Fourier_transform_of_signal_modulated_by_direct_multiplication_PAM}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\omega_s &= \frac{2 \pi}{T_s}
	\end{align*}
	Therefore,
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	Therefore, the Fourier coefficients of $c(t)$ are
	\begin{align*}
		a_k &= \frac{1}{T_s} \int\limits_{0}^{T_s} c(t) e^{j \omega_s k t} \dif t\\
		&= \frac{1}{T_s} \int\limits_{0}^{T_s} \sum\limits_{n = -\infty}^{\infty} p(t - n T_s) e^{j \omega_s k t} \dif t
	\end{align*}
	Let
	\begin{align*}
		t' &= t - n T_s\\
		\therefore t &= t' + n T_s\\
		\therefore \dif t &= \dif t'
	\end{align*}
	Therefore,
	\begin{align*}
		a_k &= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k (t' + n T_s)} \dif t'\\
		a_k &= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k \left( t' + n \frac{2 \pi}{\omega_s} \right)} \dif t'\\
		&= \frac{1}{T_s} \sum\limits_{n = -\infty}^{\infty} \int\limits_{-n T_s}^{-(n - 1) T_s} p(t') e^{-j \omega_s k t'} \dif t'\\
		&= \frac{1}{T_s} \int\limits_{-\infty}^{\infty} p(t) e^{-j \omega_s k t} \dif t\\
		&= \frac{1}{T_s} P(\omega_s k)
	\end{align*}
	Therefore,
	\begin{align*}
		X(\omega) &= \sum\limits_{k = -\infty}^{\infty} 2 \pi a_k \delta(\omega - \omega_s k)
	\end{align*}
	Therefore,
	\begin{align*}
		Y(\omega) &= \frac{1}{2 \pi} C(\omega) \ast X(\omega)\\
		&= \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - \omega_s k)\\
		&= \sum\limits_{k = -\infty}^{\infty} \frac{1}{T_s} P(k \omega_s)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ scaled according to $a_k$ with adjacent replicas separated $\omega_s$.
	Hence, in order to avoid aliasing, $X(\omega)$ must be limited to $\pm \omega_s$, i.e.
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
\end{proof}

\begin{theorem}
	A signal modulated with direct multiplication PAM can be reconstructed as in \cref{fig:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}, where the cutoff frequency and the gain of the low pass filter are
	\begin{align*}
		\omega_c &\in [B,\omega_s - B)\\
		A &= \frac{1}{|a_m|}\\
		&= \frac{T_s}{\left| P(\omega_s m) \right|}
	\end{align*}
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,>=latex']
			\node [mixer] (mixer) {};
			\node [block, right = of mixer] (LPF) {LPF};

			\draw [stealth-] (mixer.west) -- ++(-1,0) node [left] {$y(t)$};
			\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$\cos(m \omega_s t + \varphi)$};
			\draw [-stealth] (mixer.east) -- (LPF.west) node [midway, above] {$\tilde{y}(t)$};
			\draw [-stealth] (LPF.east) -- ++(1,0) node [right] {$\hat{x}(t)$};
		\end{tikzpicture}
		\end{adjustbox}
		\caption{Reconstruction of Signal Modulated with Direct Multiplication PAM}
		\label{fig:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}
	\end{figure}
	Hence,
	\begin{align*}
		\hat{X}(\omega) &= \cos(\varphi) X(\omega)
	\end{align*}
	\label{thm:reconstruction_of_signal_modulated_with_direct_multiplication_PAM}
\end{theorem}

\begin{proof}
	\begin{align*}
		\tilde{y} &= \cos(m \omega_s t + \varphi) y(t)\\
		&= \frac{1}{2} \left( e^{j \varphi} e^{j m \omega_s t} + e^{-j \varphi} e^{-j m \omega_s t} \right) y(t)
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{Y}(\omega) &= \frac{1}{2} \left( e^{j \varphi} Y(\omega - m \omega_s) + e^{-j \varphi} Y(\omega - m \omega_s) \right)\\
		&= \frac{1}{2} \left( e^{j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - k \omega_s - m \omega_s) + e^{-j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X(\omega - k \omega_s + m \omega_s \right)\\
		&= \frac{1}{2} \left( e^{j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X\left( \omega - (k + m) \omega_s \right) + e^{-j \varphi} \sum\limits_{k = -\infty}^{\infty} a_k X\left( \omega - (k - m) \omega_s \right) \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\hat{X}(t) &= \frac{1}{2 |a_m|} \left( e^{j \varphi} a_{-m} X(\omega) + e^{j \varphi} a_m X(\omega) \right)\\
		&= \frac{1}{2 |a_m|} \left( e^{j \varphi} {a_m}^* + e^{-j \varphi} a_m \right) X(\omega)\\
		&= \frac{1}{|a_m|} \Re\left\{ e^{-j \varphi} a_m \right\} X(\omega)
	\end{align*}
	Assuming $p(t)$ is symmetric, $a_m$ is real.
	Therefore,
	\begin{align*}
		\hat{X}(\omega) &= \Re\left\{ e^{j \varphi} \right\} X(\omega)\\
		&= \cos(\varphi) X(\omega)
	\end{align*}
\end{proof}

\section{Flat Sampling}

\begin{definition}[Flat Sampling PAM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$.\\
	Let $c(t)$ be given by a train of pulses, such that
	\begin{align*}
		c(t) &= \sum\limits_{n = -\infty}^{\infty} p(t - n T_s)
	\end{align*}
	where $p(t)$ is a general pulse.
	\footnote{$\rect\left( \frac{t}{\Delta} \right)$ is a commonly used pulse.}\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T_s) p(t - n T_s)
	\end{align*}
	that is, the pulses of the modulated signal have the same shape as the original train of pulses.
	Such a modulation is called flat sampling pulse amplitude modulation.
	\label{def:flat_sampling_PAM}
\end{definition}

\begin{theorem}
	For a signal $x(t)$ modulated with flat sampling PAM,
	\begin{align*}
		Y(\omega) &= \frac{1}{T_s} P(\omega) \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
	Therefore, $Y(\omega)$ is made up of replicas of $X(\omega)$ distorted by the Fourier transform of the pulse, with adjacent replicas separated $\omega_s$.
	Hence, in the time domain, the shape of the pulses is preserved, but the shape of frequency spectrum is distorted, and the distortion of each replica is different.
	Hence, in order to avoid aliasing,
	\begin{align*}
		B &< \frac{\omega_s}{2}
	\end{align*}
	where $X(\omega)$ is zero outside $(-B,B)$.
	\label{thm:Fourier_transform_of_signal_modulated_by_flat_sampling_PAM}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\tilde{y}(t) &= \sum\limits_{n = -\infty}^{\infty} \delta(t - n T_s) x(t)\\
	\end{align*}
	that is, let $\tilde{y}(t)$ be the signal sampled at all $n T_s$.
	Therefore,
	\begin{align*}
		\tilde{y}(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T_s) \delta(t - n T_s)
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{Y}(\omega) &= \frac{1}{2 \pi} X(\omega) \ast G(\omega)\\
		&= \frac{1}{T_s} \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
	Therefore,
	\begin{align*}
		y(t) &= \tilde{y}(t) \ast p(t)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n T_s) p(t - n T_s)
	\end{align*}
	Therefore,
	\begin{align*}
		Y(\omega) &= \tilde{Y}(\omega) P(\omega)\\
		&= \frac{1}{T_s} P(\omega) \sum\limits_{k = -\infty}^{\infty} X(\omega - k \omega_s)
	\end{align*}
\end{proof}

\clearpage
\part{Pulse Position Modulation}

\begin{definition}[PPM]
	Let $x(t)$ be a deterministic signal with Fourier transform $X(\omega)$, bounded by $x_{\text{min}}$ and $x_{\text{max}}$.
	Let $p(t)$ be a general pulse extending from $t = 0$ to $t = \Delta$.\\
	Let the modulated signal be
	\begin{align*}
		y(t) &= \sum\limits_{n = -\infty}^{\infty} p\left( t - n T_s - \left( \alpha x(n T_s) + \beta \right) \right)
	\end{align*}
	where $\alpha$ and $\beta$ are such that
	\begin{align*}
		\alpha x_{\text{min}} + \beta &= 0\\
		\alpha x_{\text{max}} + \beta &= T_s - \Delta
	\end{align*}
	that is, the pulses of the modulated signal have delay corresponding to the amplitude of the original signal at the corresponding multiple of $T_s$.
	\footnote{The delay of the pulse is measured with respect to the left edge of the pulse.}
	Such a modulation is called pulse position modulation.
	\label{def:PPM}
\end{definition}

\clearpage
\part{Quantization}

\begin{definition}[Quantization]
	Let $x(t)$ be a deterministic signal.
	Let
	\begin{align*}
		x[n] &= x(n T_s)
	\end{align*}
	be the corresponding discrete time signal.\\
	The quantization of the signal is defined to be $x_q[n]$ where $x_q$ can have quantized values only.\\
	The quantization error is defined to be
	\begin{align*}
		q[n] &= x[n] - x_q[n]
	\end{align*}
\end{definition}

\section{Uniform Quantization}

\begin{definition}[Dense quantization]
	If the quantization error $q$ is distributed uniformly in each quantization cell, the quantization is called dense quantization.
\end{definition}

\begin{definition}[Uniform quantization]
	If the quantum values are such that each quantization cells have equal width, the quantization is called uniform quantization.
\end{definition}

\begin{theorem}
	For uniform and dense quantization,
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
\end{theorem}

\begin{definition}[QSNR]
	The quantization signal to noise ratio is defined to be
	\begin{align*}
		\QSNR &= \frac{\expct\left[ x^2 \right]}{\expct\left[ q^2 \right]}
	\end{align*}
\end{definition}

\begin{theorem}
	If $x$ is a uniform random variable quantized by a uniform and dense $M$ level quantizer $Q$, i.e.
	\begin{align*}
		x &\sim \uniform\left( -\frac{M}{2} \Delta,\frac{M}{2} \Delta \right)\\
		Q &=
			\begin{cases}
				x_1 &;\quad x \in I_1\\
				&\vdots\\
				x_m &;\quad x \in I_m\\
			\end{cases}
	\end{align*}
	then
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
	Hence,
	\begin{align*}
		\QSNR &= M^2\\
		\QSNR_{\decibel} &= 20 b \log 2\\
		&\approx 6 b
	\end{align*}
	where $b = \log_2 M$.
\end{theorem}

\begin{proof}
	\begin{align*}
		x &\sim \uniform\left( -\frac{M}{2} \Delta,\frac{M}{2} \Delta \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ x^2 \right] &= \frac{M^2 \Delta^2}{12}
	\end{align*}
	Similarly,
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \frac{\Delta^2}{12}
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR &= \frac{\expct\left[ x^2 \right]}{\expct\left[ q^2 \right]}\\
		&= M^2
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR_{\decibel} &= 10 \log \QSNR\\
		&= 10 \log\left( M^2 \right)\\
		&= 20 \log M\\
		&= 20 \log\left( 2^b \right)\\
		&= 20 b \log 2\\
		&\approx 6 b
	\end{align*}
\end{proof}

\begin{theorem}
	If $x$ is a Gaussian random variable with zero mean and $\sigma_x$, quantized by a uniform $M$ level quantizer $Q$ with highest and lowest thresholds $\pm k \sigma_x$,
	\begin{align*}
		\QSNR &\approx \frac{k^2}{3 M^2} - \frac{2 k}{\sqrt{2 \pi}} e^{-\frac{1}{2} k^2} + 2 Q(k) \left( k^2 + 1 \right)\\
		\QSNR_{\decibel} &\approx -10 \log\left( \frac{k^2}{3 M^2} - \frac{2 k}{\sqrt{2 \pi}} e^{-\frac{1}{2} k^2} + 2 Q(k) \left( k^2 + 1 \right) \right)
	\end{align*}
\end{theorem}

\begin{proof}
	Let $A$ be the event that
	\begin{align*}
		|x| &< k \sigma_x
	\end{align*}
	Therefore,
	\begin{align*}
		\prob(A) &= \int\limits_{-k \sigma_x}^{k \sigma_x} \frac{1}{\sqrt{2 \pi} \sigma_x} e^{-\frac{1}{2} \frac{x^2}{{\sigma_x}^2}} \dif x
	\end{align*}
	Let
	\begin{align*}
		x' &= \frac{x}{\sigma_x}
	\end{align*}
	Therefore,
	\begin{align*}
		x &= \sigma_x x'\\
		\dif x &= \sigma_x \dif x'
	\end{align*}
	Therefore,
	\begin{align*}
		\prob(A) &= \int\limits_{-k}^{k} \frac{1}{\sqrt{2 \pi} \sigma_x} e^{-\frac{1}{2} x'} \sigma_x \dif x'\\
		&= 1 - 2 \Q(k)
	\end{align*}
	where
	\begin{align*}
		\Q(x) &= \int\limits_{k}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2} \dif x
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \prob(A) \expct\left[ q^2 \Big| A \right] + \left( 1 - \prob(A) \right) \expct\left[ q^2 \Big| \overline{A} \right]\\
		&= \left( 1 - 2 \Q(k) \right) \expct\left[ q^2 \Big| A \right] + 2 \Q(k) \expct\left[ q^2 \Big| \overline{A} \right]
	\end{align*}
	If $x$ is in the uniform zone $(-k \sigma_x,k \sigma_x)$, i.e. given $A$,
	\begin{align*}
		q &\sim \uniform\left( -\frac{\Delta}{2},\frac{\Delta}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| A \right] &= \frac{\Delta^2}{12}\\
		&= \frac{\left( \frac{2 k \sigma_x}{M} \right)^2}{12}\\
		&= \frac{k^2 {\sigma_x}^2}{3 M^2}
	\end{align*}
	If $x$ is outside the uniform zone, i.e. given $\overline{A}$,
	\begin{align*}
		q &=
			\begin{cases}
				x - k \sigma_x &;\quad x > \sigma_x\\
				x + k \sigma_x &;\quad x < \sigma_x\\
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| \overline{A} \right] &= \int\limits_{\overline{A}} q^2 \pdf_{x | \overline{A}} \dif x
	\end{align*}
	Also,
	\begin{align*}
		\pdf_{x | \overline{A}} &=
			\begin{cases}
				\frac{\pdf_x(x)}{\prob\left( \overline{A} \right)} &;\quad |x| > k \sigma_x\\
				0 &;\quad |x| > k \sigma_x\\
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| \overline{A} \right] &= \int\limits_{-\infty}^{-k \sigma_x} (x + k \sigma_x)^2 \pdf_{x | \overline{A}}\left( x \Big| \overline{A} \right) \dif x + \int\limits_{-k \sigma_x}^{\infty} (x - k \sigma_x)^2 \pdf_{x | \overline{A}}\left( x \Big| \overline{A} \right) \dif x\\
		&= 2 \int\limits_{k \sigma_x}^{\infty} (x - k \sigma_x)^2 \frac{\pdf_X(x)}{2 \Q(k)} \dif x\\
		&= \frac{1}{\Q(k)} \int\limits_{k \sigma_x}^{\infty} (x - k \sigma_x)^2 \frac{1}{\sqrt{2 \pi} \sigma_x} e^{-\frac{1}{2} \frac{x^2}{{\sigma_x}^2}} \dif x
	\end{align*}
	Let
	\begin{align*}
		x' &= \frac{x}{\sigma_x}
	\end{align*}
	Therefore,
	\begin{align*}
		x &= x' \sigma_x\\
		\therefore \dif x &= \dif x' \sigma_x
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| \overline{A} \right] &= \frac{1}{\Q(k) \sqrt{2 \pi}} \int\limits_{k}^{\infty} (x' \sigma_x - k \sigma_x)^2 \frac{1}{\sigma_x} e^{-\frac{1}{2} {x'}^2} \dif x' \sigma_x\\
		&= \frac{{\sigma_x}^2}{\Q(k) \sqrt{2 \pi}} \int\limits_{k}^{\infty} (x' - k)^2 e^{-\frac{1}{2} {x'}^2} \dif x
	\end{align*}
	Therefore, integrating by parts,
	\begin{align*}
		\expct\left[ q^2 \Big| \overline{A} \right] &= \frac{1}{\Q(k) \sqrt{2 \pi}} \left( k e^{-\frac{1}{2} k^2} + \Q(k) \sqrt{2 \pi} - 2 k e^{-\frac{1}{2} k^2} + k^2 \Q(k) \sqrt{2 \pi} \right)\\
		&= {\sigma_x}^2 \left( k^2 + 1 - \frac{k}{\Q(k) \sqrt{2 \pi}} e^{-\frac{1}{2} k^2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \left( 1 - 2 \Q(k) \right) \expct\left[ q^2 \Big| A \right] + 2 \Q(k) \expct\left[ q^2 \Big| \overline{A} \right]\\
		&= \left( 1 - 2 \Q(k) \right) \frac{k^2 {\sigma_x}^2}{3 M^2} + 2 \Q(k) {\sigma_x}^2 \left( k^2 + 1 - \frac{k}{\Q(k) \sqrt{2 \pi}} e^{-\frac{1}{2} k^2} \right)\\
		&= {\sigma_x}^2 \left( \frac{k^2}{3 M^2} - \frac{2 k}{\sqrt{2 \pi}} e^{-\frac{1}{2} k^2} + 2 \Q(k) \left( k^2 + 1 - \frac{k^2}{3 M^2} \right) \right)
	\end{align*}
	As $M >> k$, the term $\frac{k^2}{3 M^2}$ may be ignored.
	Also,
	\begin{align*}
		\expct\left[ x^2 \right] &= {\sigma_x}^2
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR &\approx \frac{k^2}{3 M^2} - \frac{2 k}{\sqrt{2 \pi}} e^{-\frac{1}{2} k^2} + 2 \Q(k) \left( k^2 + 1 \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR_{\decibel} &\approx -10 \log\left( \frac{k^2}{3 M^2} - \frac{2 k}{\sqrt{2 \pi}} e^{-\frac{1}{2} k^2} + 2 \Q(k) \left( k^2 + 1 \right) \right)
	\end{align*}
\end{proof}

\begin{theorem}
	If $x > 0$,
	\begin{equation*}
		\frac{x}{x^2 + 1} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2} < \Q(x) < \frac{1}{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2}
	\end{equation*}
\end{theorem}

\begin{question}
	Let
	\begin{align*}
		X &\sim \exponential(\lambda)
	\end{align*}
	Consider a $M$ level dense and uniform quantizer with
	\begin{align*}
		\Delta &= \frac{k \lambda^{-1}}{M}
	\end{align*}
	that is, the highest threshold is $k \lambda^{-1}$.
	Compute the QSNR.
\end{question}

\begin{solution}
	\begin{align*}
		q &= x_q - x
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \expct\left[ q^2 \Big| x < k \lambda^{-1} \right] \prob\left( x < k \lambda^{-1} \right) + \expct\left[ q^2 \Big| x > k \lambda^{-1} \right] \prob\left( x > k \lambda^{-1} \right)\\
		&= \frac{\Delta^2}{12} \cdf_x\left( k \lambda^{-1} \right) + \expct\left[ q^2 \Big| x > k \lambda^{-1} \right] \left( 1 - \cdf_x\left( k \lambda^{-1} \right) \right)
	\end{align*}
	Also, if $x > k \lambda^{-1}$,
	\begin{align*}
		q &= x_q - x\\
		&= k \lambda^{-1} \left( 1 - \frac{1}{2 M} \right) - x
	\end{align*}
	Let $A$ be the event that $x > k \lambda^{-1}$.
	Therefore,
	\begin{align*}
		\pdf_{q|A}(q|A) &= \pdf_{x|A}(x_q - q | A)\\
		&= \pdf_{x|A}\left( k \lambda^{-1} \left( 1 - \frac{1}{2 M} \right) - q \Big| A \right)\\
		&= \frac{\pdf_{x|A}\left( x = k \lambda^{-1} \left( 1 - \frac{1}{2 M} \right) - q \cap x > k \lambda^{-1} \right)}{\prob\left( x > k \lambda^{-1} \right)}\\
		&=
			\begin{cases}
				e^k \pdf_x\left( k \lambda^{-1} \left( 1 - \frac{1}{2 M} \right) - q \right) &;\quad q < -\frac{k \lambda^{-1}}{2 M}\\
				0 &;\quad q > -\frac{k \lambda^{-1}}{2 M}\\
			\end{cases}\\
		&=
			\begin{cases}
				e^k \lambda e^{-\lambda \left( k \lambda^{-1} \left( 1 - \frac{1}{2 M} \right) - q \right)} &;\quad q < -\frac{k \lambda^{-1}}{2 M}\\
				0 &;\quad q > -\frac{k \lambda^{-1}}{2 M}\\
			\end{cases}\\
		&=
			\begin{cases}
				\lambda e^{\frac{k}{2 M}} e^{\lambda q} &;\quad q < -\frac{k \lambda^{-1}}{2 M}\\
				0 &;\quad q > -\frac{k \lambda^{-1}}{2 M}\\
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| x > k \lambda^{-1} \right] &= \int\limits_{-\infty}^{-\frac{k \lambda^{-1}}{2 M}} q^2 \pdf_{q|A}(q|A) \dif q\\
		&= \lambda^{-2} \left( \frac{k^2}{4 M^2} + \frac{k}{M} + 2 \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \frac{\Delta^2}{12} \cdf_x\left( k \lambda^{-1} \right) + \expct\left[ q^2 \Big| x > k \lambda^{-1} \right] \left( 1 - \cdf_x\left( k \lambda^{-1} \right) \right)\\
		&= \frac{\Delta^2}{12} \cdf_x\left( k \lambda^{-1} \right) + \lambda^{-2} \left( \frac{k^2}{4 M^2} + \frac{k}{M} + 2 \right) \left( 1 - \cdf_x\left( k \lambda^{-1} \right) \right)\\
	\end{align*}
	Similarly,
	\begin{align*}
		\expct[q] &= \expct\left[ q \Big| x < k \lambda^{-1} \right] \prob\left( x < k \lambda^{-1} \right) + \expct\left[ q \Big| x > k \lambda^{-1} \right] \prob\left( x > k \lambda^{-1} \right)\\
		&= -e^{-k} \lambda^{-1} \left( \frac{k}{2 M} + 1 \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR &= \frac{\var(x)}{\var(q)}\\
		&= \frac{\var(x)}{\expct\left[ x^2 \right] - \expct[x]^2}
	\end{align*}
	Also, for reasonable values of $k$,
	\begin{align*}
		\expct\left[ q^2 \right] &>> \expct[q]^2
	\end{align*}
	Therefore,
	\begin{align*}
		\QSNR &\approx \frac{-\lambda^{-2}}{\expct\left[ q^2 \right]}
	\end{align*}
\end{solution}

\begin{question}
	Let $X$ be a random variable with PDF $\pdf_X(x)$, such that $\forall |x| > 4$,
	\begin{align*}
		\pdf_X(x) &= 0
	\end{align*}
	Consider the quantization
	\begin{align*}
		Q(x) &=
			\begin{cases}
				-4 &;\quad x < -3\\
				-2 &;\quad -3 \le x < -1\\
				0 &;\quad -1 \le x < 1\\
				2 &;\quad 1 \le x < 3\\
				4 &;\quad 3 \le x\\
			\end{cases}
	\end{align*}
	Find $\pdf_q(q)$, the PDF of the quantization error.
\end{question}

\begin{solution}
	\begin{align*}
		\pdf_q(q) &= \sum \prob(X_q = x_q) \pdf_{q|X_q}(q | x_q)
	\end{align*}
	Let $D_{x_q}$ be the domain of $q$ corresponding to $x_q$.
	Therefore, the domains corresponding to all possible values of $x_q$ are
	\begin{table}[H]
		\centering
		\begin{tabular}{l l}
			\toprule
			$x_q$ & $D_{x_q}$\\
			\midrule
			$-4$ & $[-1,0]$\\
			$-2$ & $[-1,1]$\\
			$0$ & $[-1,1]$\\
			$2$ & $[-1,1]$\\
			$4$ & $[0,1]$\\
			\bottomrule
		\end{tabular}
	\end{table}
	Also, $\forall q \in D_{x_q}$,
	\begin{align*}
		\pdf_{q,x_q}(q,x_q) &= \pdf_X(q - x_q)
	\end{align*}
	Therefore,
	\begin{align*}
		\pdf_{q|x_q}(q|x_q) &= \frac{\pdf_{q,x_q}(q,x_q)}{\prob(X_q = x_q)}\\
		&=
			\begin{cases}
				\frac{\pdf_X(q - x_q)}{\prob(X_q = x_q)} &;\quad q \in D_{x_q}\\
				0 &;\quad q \notin D_{x_q}\\
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		\pdf_q(q) &= \sum \prob(X_q = x_q) \pdf_{q|X_q}(q | x_q)\\
		&=
			\begin{cases}
				\pdf_X(-4 - q) + \pdf_X(-2  q) + \pdf_X(-q) + \pdf_X(2 - q) &;\quad q \in [-1,0]\\
				\pdf_X(-2 - q) + \pdf_X(-q) + \pdf_X(2 - q) + \pdf_X(4 - q) &;\quad q \in [0,1]\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence, the quantization error is not uniformly distributed.
\end{solution}

\begin{question}
	Let $Q$ be a quantizer with quantization level $y$, for the cell
	\begin{align*}
		R_i &= [t_{i - 1},t_i]
	\end{align*}
	Find the optimal MMSE $y_i$s.
\end{question}

\begin{solution}
	The MSE is
	\begin{align*}
		\expct\left[ q^2 \right] &= \expct\left[ \left( Q(x) - x \right)^2 \right]\\
		&= \expct_{R_i}\left[ \expct_X\left[ \left( Q(x) - x \right)^2 \Big| x \in R_i \right] \right]\\
		&= \sum\limits_{i = 1}^{M} \prob(x \in R_i) \expct\left[ \left( Q(x) - x \right)^2 \Big| x \in R_i \right]\\
		&= \sum\limits_{i = 1}^{M} \prob(x \in R_i) \expct\left[ (y_i - x)^2 \Big| x \in R_i \right]\\
		&= \sum\limits_{i = 1}^{M} \prob(x \in R_i) \expct\left[ {y_1}^2 - 2 x y_i + x^2 \Big| x \in R_i \right]\\
		&= \sum\limits_{i = 1}^{M} \prob(x \in R_i) \left( {y_i}^2 - 2 y_i \expct[x | x \in R_i] + \expct\left[ x^2 \Big| x \in R_i \right] \right)
	\end{align*}
	Therefore, differentiating with respect to $y_i$, setting to be zero, and solving, the minimal $y_i$ is
	\begin{align*}
		y_i &= \expct[x | x \in R_i]
	\end{align*}
\end{solution}

\section{Companders}

\begin{theorem}
	A random variable $x$ can be transformed into a random variable $y$ distributed uniformly between $0$ and $1$ as
	\begin{align*}
		y &= \cdf_x(x)
	\end{align*}
\end{theorem}

\begin{proof}
	Let $y = g(x)$.
	Therefore, let
	\begin{align*}
		y_1 &= g(x_1)\\
		y_1 + \Delta y_1 &= g(x_1 + \Delta x_1)
	\end{align*}
	Therefore,
	\begin{align*}
		\prob\left( x \in (x_1, x_1 + \Delta x) \right) &= \pdf_x(x_1) \Delta x\\
		&= \prob\left( y \in (y_1, y_1 + \Delta y) \right)\\
		&= \pdf_y(y_1) \Delta y
	\end{align*}
	Therefore,
	\begin{align*}
		\pdf_x(x) \dif x &= \pdf_y(y) \dif y\\
		\therefore \pdf_y(y) &= \frac{1}{\left| \dod{y}{x} \right|} \pdf_x(x)\\
		&= \frac{1}{\left| g'(x) \right|} \pdf_x(x)
	\end{align*}
	In particular, let
	\begin{align*}
		g(x) &= \cdf_x(x)
	\end{align*}
	Therefore,
	\begin{align*}
		g'(x) &= \pdf_x(x)
	\end{align*}
	Therefore,
	\begin{align*}
		\pdf_y(y) &= 1
	\end{align*}
	Hence,
	\begin{align*}
		y &\sim \uniform(0,1)
	\end{align*}
\end{proof}

\subsection{$\mu$ Law}

\begin{definition}[$\mu$ Law]
	Let $x$ be a random variable bounded by $\pm 1$.\\
	Let
	\begin{align*}
		y &= g(x)\\
		&= \sign(x) \frac{\ln\left( 1 + \mu |x| \right)}{\ln(1 + \mu)}
	\end{align*}
	where $\mu \in (0,255]$.\\
	Then, $y$ is said to be obtained by transformation of $x$ with the $\mu$ law.\\
	Hence, the transformation function $g(x)$ is as in \cref{fig:transformation_function_for_mu_law}.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 0.8\textwidth]{./Plots/mu_law.pdf}
		\caption{Transformation Function for $\mu$ Law}
		\label{fig:transformation_function_for_mu_law}
	\end{figure}
	\label{def:mu_law}
\end{definition}

\begin{theorem}
	Let $y$ be a random variable obtained by transformation of $x$ with \cref{def:mu_law}.\\
	Let $y$ be quantized by uniform $M$ level quantizer.
	Then,
	\begin{align*}
		\expct\left[ q^2 \right] &= \frac{\ln^2(1 + \mu)}{3 \mu^2 M^2} \left( 1 + 2 \mu \expct\left[ |x| \right] + \mu^2 \expct\left[ x^2 \right] \right)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		\frac{\Delta y}{\Delta x} &= g'(x_1)
	\end{align*}
	Therefore,
	\begin{align*}
		\Delta x_m &= \frac{\Delta y_m}{g'(x_m)}\\
		&= \frac{\frac{2}{M}}{g'(x_m)}
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \Big| x \in I_m \right] &= \frac{{\Delta x_m}^2}{12}\\
		&= \frac{4}{12 M^2 {g'(x_m)}^2}
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \sum\limits_{m = 1}^{M} \expct\left[ {q_x}^2 \Big| x \in I_m \right] \prob(I_m)\\
		\expct\left[ q^2 \right] &= \sum\limits_{m = 1}^{M} \expct\left[ {q_x}^2 \Big| x \in I_m \right] \pdf_x(x_m) \Delta x_m\\
		&= \sum\limits_{m = 1}^{M} \frac{1}{3 M^2 {g'(x_m)}^2} \pdf_x(x_m) \Delta x_m
	\end{align*}
	Therefore, as $\Delta x_m \to 0$,
	\begin{align*}
		\expct\left[ q^2 \right] &= \int\limits_{-1}^{1} \frac{1}{3 M^2 {g'(x)}^2} \pdf_x(x) \dif x\\
		&= \frac{1}{3 M^2} \expct\left[ \frac{1}{{g'(x)}^2} \right]\\
	\end{align*}
	Also,
	\begin{align*}
		g(x) &=
			\begin{cases}
				\frac{\ln(1 + \mu x)}{\ln(1 + \mu)} &;\quad x > 0\\
				-\frac{\ln(1 + \mu x)}{\ln(1 + \mu)} &;\quad x < 0\\
			\end{cases}\\
		\therefore g'(x) &=
			\begin{cases}
				\frac{\mu}{\ln(1 + \mu)} \frac{1}{1 + \mu x} &;\quad x > 0\\
				\frac{\mu}{\ln(1 + \mu)} \frac{1}{1 - \mu x} &;\quad x < 0\\
			\end{cases}\\
	\end{align*}
	Therefore,
	\begin{align*}
		\expct\left[ q^2 \right] &= \frac{1}{3 M^2} \expct\left[ \frac{1}{{g'(x)}^2} \right]\\
		&= \frac{1}{3 M^2} \expct\left[ \left( \frac{\ln(1 + \mu)}{\mu} \left( 1 + \mu |x| \right)^2 \right) \right]\\
		&= \frac{\ln^2(1 + \mu)}{3 \mu^2 M^2} \expct\left[ \left( 1 + \mu |x| \right)^2 \right]\\
		&= \frac{\ln^2(1 + \mu)}{3 \mu^2 M^2} \left( 1 + 2 \mu \expct\left[ |x| \right] + \mu^2 \expct\left[ x^2 \right] \right)
	\end{align*}
\end{proof}

\subsection{$A$ Law}

\begin{definition}[$A$ Law]
	Let $x$ be a random variable bounded by $\pm 1$.\\
	Let
	\begin{align*}
		y &= g(x)\\
		&=
			\begin{cases}
				\frac{A |x|}{1 + \log A} \sign(x) &;\quad |x| < \frac{1}{A}\\
				\frac{1 + \log\left( A |x| \right)}{1 + \log A} \sign(x) &;\quad |x| > \frac{1}{A}\\
			\end{cases}
	\end{align*}
	where $A > 1$.\\
	Then, $y$ is said to be obtained by transformation of $x$ with the $A$ law.\\
	Hence, the transformation function $g(x)$ is as in \cref{fig:transformation_function_for_A_law}.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 0.8\textwidth]{./Plots/A_law.pdf}
		\caption{Transformation Function for $A$ Law}
		\label{fig:transformation_function_for_A_law}
	\end{figure}
	\label{def:A_law}
\end{definition}

\section{Max-Lloyd Algorithm}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\Require{
			$\pdf(x)$, the PDF of $x$, such that $\pdf(x) = 0$ for $x \notin [\alpha,\beta]$.
		}
		\Ensure{
			Dense quantization cells defined by $x_i$s.
		}
		\State{
			Determine arbitrary adjacent quantization cells, with boundaries at $b_m$, for $0 \le m \le M$, and
			\begin{align*}
				\alpha &= b_0\\
				\beta &= b_M
			\end{align*}
		}
		\State{
			For each cell $[b_{m - 1},b_m]$, compute
			\begin{align*}
				x_m &= \expct\left[ x \Big| x \in [b_{m - 1},b_m] \right]\\
				&= \frac{\int\limits_{b_{m - 1}}^{b_m} x \pdf(x) \dif x}{\int\limits_{b_{m - 1}}^{b_m} \pdf(x) \dif x}
			\end{align*}
			\label{alg_step:Max_Lloyd_Algorithm_find_means}
		}
		\State{
			Redetermine all $b_m$s, such that
			\begin{align*}
				b_m &= \frac{x_m + x_{m + 1}}{2}
			\end{align*}
			\label{alg_step:Max_Lloyd_Algorithm_redefined_boundaries}
		}
		\State{
				Repeat steps \ref{alg_step:Max_Lloyd_Algorithm_find_means} and \ref{alg_step:Max_Lloyd_Algorithm_redefined_boundaries} till the $x_m$s and $b_m$s converge.
			}
	\end{algorithmic}
	\caption{Max-Lloyd Algorithm}
	\label{alg:Max_Lloyd_Algorithm}
\end{algorithm}

\section{Amplitude Shift Keying}

\begin{definition}[Amplitude shift keying]
	Let $b_n$ be the $n$th bit of the data to be transmitted
	\footnote{This data may be possibly obtained by quantization of an analogue signal.}
	Consider the information signal
	\begin{align*}
		s(t) &= \sum\limits_{n = -\infty}^{\infty} b_n p(t - n T)
	\end{align*}
	where $p(t)$ is a pulse chosen according to the requirements.
	\footnote{For example, if a rectangular pulse is used, a limited number of channels can be used as the rectangular pulse has a large bandwidth. Alternatively, if a sinc is used as a pulse, a higher number of channels can be used as the bandwidth is smaller.}
	Then, the information signal $s(t)$ is said to be obtained by amplitude shift keying or ASK.
\end{definition}

\begin{definition}[$M$-ary ASK]
	Consider the information signal
	\begin{align*}
		s(t) &= \sum\limits_{n = -\infty}^{\infty} c_n p(t - n T)
	\end{align*}
	where $0 \le c_n < 2^M$ for each $n$.
	Hence, each `data point' in the information signal is represented by one of $2^M$ levels.
	Then, the information signal $s(t)$ is said to be obtained by $M$-ary ASK.
\end{definition}

\subsection{Gray Coding}

\begin{definition}[Gray coding]
	A coding of $2^M$ levels ordered in a way that the difference between any two adjacent levels is of exactly one bit is called a Gray coding.
\end{definition}

Consider $M$-ary ASK with consecutive levels defined by standard binary coding e.g. $8$ binary levels ordered as $(000,001,010,011,100,101,110,111)$.
If a level is misinterpreted by the receiver as one of the adjacent levels, there may be an error of multiple bits.
For example, if $011$ is misinterpreted as $110$, the error is of three bits.
Hence, small noise can result in a very large error.
This problem can be reduced by using Gray coding, and the error is limited to a single bit.

\section{Inter Symbol Interference}

\begin{definition}[Inter symbol interference]
	The interference of a pulse corresponding to a `data point' at one point in time with that corresponding to another is called inter symbol interference or ISI.
\end{definition}

\begin{definition}[NISI condition]
	The no-ISI condition is
	\begin{align*}
		p(t) &= 0
	\end{align*}
	for all $t = n T$, except at $n = 0$, and
	\begin{align*}
		p(0) &= 1
	\end{align*}
	Equivalently, $\forall \omega$,
	\begin{align*}
		\sum\limits_{k = -\infty}^{\infty} P\left( \omega - \frac{2 \pi}{T} k \right) &= T
	\end{align*}
\end{definition}

\begin{question}
	Consider a signal $s_n$, represented by four levels, i.e. two bits.
	The received signal is
	\begin{align*}
		\hat{s_n} &=
			\begin{cases}
				s_n &;\quad \text{with probability $1 - p$}\\
				(s_n + 1) \mod 4 &;\quad \text{with probability $p$}\\
			\end{cases}
	\end{align*}
	that is, the information is received as sent with probability $1 - p$, and is received with error of one level with probability $p$.
	\begin{enumerate}
		\item
			Find the probability of error if the levels are determined by standard binary coding.
		\item
			Find the probability of error if the levels are determined by Gray coding.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			Let $P_{b_i}$ be the probability of error in the $i$th bit.
			Therefore, let $\prob(b_i|s_j)$ be the probability of error in the $i$th bit, given the value of the $j$th symbol was transmitted.
			Therefore, the total probability of error in the information is
			\begin{align*}
				P_b &= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} P_{b_i}\\
				&= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} \sum\limits_{j = 0}^{3} \prob(b_i|s_j) \prob(s_j)\\
				&= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} \left( \frac{1}{4} \sum\limits_{j = 0}^{3} \prob(b_i|s_j) \right)\\
				&= \frac{1}{2} \left( \frac{1}{4} \left( (p + p + p + p) + (0 + p + 0 + p) \right) \right)\\
				&= \frac{3 p}{4}
			\end{align*}
		\item
			Let $P_{b_i}$ be the probability of error in the $i$th bit.
			Therefore, let $\prob(b_i|s_j)$ be the probability of error in the $i$th bit, given the value of the $j$th symbol was transmitted.
			Therefore, the total probability of error in the information is
			\begin{align*}
				P_b &= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} P_{b_i}\\
				&= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} \sum\limits_{j = 0}^{3} \prob(b_i|s_j) \prob(s_j)\\
				&= \frac{1}{k} \sum\limits_{i = 0}^{k - 1} \left( \frac{1}{4} \sum\limits_{j = 0}^{3} \prob(b_i|s_j) \right)\\
				&= \frac{1}{2} \left( \frac{1}{4} \left( (p + 0 + p + 0) + (0 + p + 0 + p) \right) \right)\\
				&= \frac{p}{2}
			\end{align*}
	\end{enumerate}
\end{solution}

\begin{question}
	Consider a information signal $\sum\limits_{n \in \mathbb{Z}} a_n \delta(t - n T)$ where $a_n \in \{-1,1\}$.
	This signal is passed through a LTI system with transfer function
	\begin{align*}
		\tilde{H}(f) &=
			\begin{cases}
				1 - T |f| &;\quad |f| \le \frac{1}{T}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	to obtain $y(t)$.
	$y(t)$ is then sampled at multiples of $t_n$, and passed through a slicer
	\begin{align*}
		g(x) &= \sign(x)
	\end{align*}
	to obtain $\hat{a_n}$.
	\begin{enumerate}
		\item
			Compute $h(t)$.
			Prove that $\forall n \neq 0, n \in \mathbb{Z}$,
			\begin{align*}
				h(n T) &= 0
			\end{align*}
		\item
			Does the NISI condition hold for $t_n = n T + \frac{T}{4}$?
		\item
			For the above sampling times, is it possible that an estimation error of $\hat{a_n}$ occurs due to ISI?
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			$\tilde{H}(f)$ is a triangular function, and hence is a convolution of two rectangular functions with width $\frac{1}{T}$, and height $\sqrt{T}$.
			Therefore,
			\begin{align*}
				h(t) &= \IFT\left\{ \tilde{H}(f) \right\}\\
				&= \left( \frac{1}{\sqrt{T}} \sinc\left( \frac{\pi t}{T} \right) \right)^2\\
				&= \frac{1}{T} \sinc^2\left( \frac{\pi t}{T} \right)
			\end{align*}
		\item
			\begin{align*}
				h(t) \Big|_{t = t_n} &= h\left( n T + \frac{T}{4} \right)\\
				&= \frac{1}{T} \sinc^2\left( \frac{\pi}{T} \left( n T + \frac{T}{4} \right) \right)\\
				&\neq 0
			\end{align*}
		\item
			Without loss of generality, for $a_0$, and
			\begin{align*}
				t_0 &= \frac{T}{4}
			\end{align*}
			the continuous time signal is
			\begin{align*}
				y\left( \frac{T}{4} \right) &= \sum\limits_{n \in \mathbb{Z}} a_n h\left( t - n T + \frac{T}{4} \right) \Big|_{t = \frac{T}{4}}\\
				&= a_0 h\left( \frac{T}{4} \right) + \sum\limits_{n \in \mathbb{Z} \setminus \{0\}} a_n h\left( t - n T + \frac{T}{4} \right)
			\end{align*}
			Hence, in the worst case
			\footnote{If $y(t)$ is higher than intended, there would be no error as the signal would be corrected to be $1$ due to the slicer. Hence, the worst case is when only $a_0$ is $1$, and all others are $-1$.}
			, i.e. for $a_0 = 1$ and all other $a_i = -1$ for $i \neq 0$,
			\begin{align*}
				h\left( t - n T + \frac{T}{4} \right) &= \frac{1}{T} \sinc^2\left( \frac{\pi}{T} \left( n T + \frac{T}{4} \right) \right)
			\end{align*}
			Therefore, an error will occur if and only if
			\begin{align*}
				y\left( \frac{T}{4} \right) &< 0
			\end{align*}
			Hence, solving,
			\begin{align*}
				h\left( n T + \frac{T}{4} \right) &> 0
			\end{align*}
			Therefore, there be no error due to ISI.
	\end{enumerate}
\end{solution}

\section{The Raised Cosine Family of Pulses}

\begin{definition}[The raised cosine family of pulses]
	The set of all $p(t)$, such that
	\begin{align*}
		P(\omega) &=
			\begin{cases}
				1 &;\quad |\omega| \le \pi - \Delta\\
				g(\omega) &;\quad \pi - \Delta < |\omega| < \pi + \Delta\\
				0 &;\quad |\omega| \ge \pi + \Delta\\
			\end{cases}
	\end{align*}
	where
	\begin{align*}
		g(\omega) &= \frac{1}{2} \left( 1 - \sin\left( \frac{|\omega| - \pi}{\Delta} \frac{\pi}{2} \right) \right)
	\end{align*}
	and
	\begin{align*}
		\Delta &= \rho \pi
	\end{align*}
	is called the raised cosine family of pulses.
	$\rho \in [0,1]$ is called the rolloff factor of the pulse, chosen according to the requirements.
	Hence, the frequency domain representation of the pulse is as in \cref{fig:raised_cosine_pulses_in_frequency_domain}.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 0.8\textwidth]{./Plots/raised_cosine_pulses_in_frequency.pdf}
		\caption{Raised Cosine Pulses in Frequency Domain}
		\label{fig:raised_cosine_pulses_in_frequency_domain}
	\end{figure}
\end{definition}

\begin{theorem}
	The time domain representation of a raised cosine pulse is
	\begin{align*}
		p(t) &= \sinc\left( \pi \frac{t}{T} \right) \frac{\cos\left( \rho \pi \frac{t}{T} \right)}{1 - \left( \frac{2 t \rho}{T} \right)^2}
	\end{align*}
	where $\rho \in [0,1]$ is the rolloff factor of the pulse.
	Hence, the time domain representation of the pulse is as in \cref{fig:raised_cosine_pulse_in_time_domain}.
	\begin{figure}[H]
		\centering
		\includegraphics[width = 0.8\textwidth]{./Plots/raised_cosine_pulses_in_time.pdf}
		\caption{Raised Cosine Pulses in Time Domain}
		\label{fig:raised_cosine_pulse_in_time_domain}
	\end{figure}
\end{theorem}

\begin{proof}
	For simplicity, let $T = 1$.
	Let
	\begin{align*}
		R(\omega) &= \rect\left( \frac{w}{2 \pi} \right)\\
		Q(\omega) &=
			\begin{cases}
				c \cos\left( \frac{\omega}{\Delta} \frac{\pi}{2} \right) &;\quad |\omega| < \Delta\\
				0 &;\quad \text{otherwise}
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		P(\omega) &= \frac{1}{2 \pi} R(\omega) \ast Q(\omega)
	\end{align*}
	Therefore,
	\begin{align*}
		r(t) &= \sinc(\pi t)\\
		q(t) &= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} Q(\omega) e^{j \omega t} \dif \omega\\
		&= \frac{1}{2 \pi} \int\limits_{-\Delta}^{\Delta} \frac{\pi^2}{2 \Delta} \cos\left( \frac{\omega}{\Delta} \frac{\pi}{2} \right) e^{j \omega t} \dif \omega\\
		&= \frac{\pi}{4 \Delta} \int\limits_{-\Delta}^{\Delta} \cos\left( \frac{\omega}{\Delta} \frac{\pi}{2} \right) \left( \cos(\omega t) + j \sin(\omega t) \right) \dif \omega\\
		&= \frac{\pi}{4 \Delta} \int\limits_{-\Delta}^{\Delta} \cos\left( \frac{\omega}{\Delta} \frac{\pi}{2} \right) \cos(\omega t) \dif \omega\\
		&= \frac{\pi}{4 \Delta} \int\limits_{-\Delta}^{\Delta} \frac{1}{2} \left( \cos\left( \omega \left( \frac{\pi}{2 \Delta} + t \right) \right) + \cos\left( \omega \left( \frac{\pi}{2 \Delta} - t \right) \right) \right) \dif \omega\\
		&= \frac{\pi}{4 \Delta} \int\limits_{0}^{\Delta} \left( \cos\left( \omega \left( \frac{\pi}{2 \Delta} + t \right) \right) + \cos\left( \omega \left( \frac{\pi}{2 \Delta} - t \right) \right) \right) \dif \omega\\
		&= \left. \frac{\pi}{4 \Delta} \left( \frac{\sin\left( \omega \left( \frac{\pi}{2 \Delta} + t \right) \right)}{\frac{\pi}{2 \Delta} + t} + \frac{\sin\left( \omega \left( \frac{\pi}{2 \Delta} - t \right) \right)}{\frac{\pi}{2 \Delta} - t} \right) \right|_{0}^{\Delta}\\
		&= \frac{\pi}{4 \Delta} \left( \frac{\sin\left( t \Delta + \frac{\pi}{2} \right)}{\frac{\pi}{2 \Delta} + t} + \frac{\sin\left( -t \Delta + \frac{\pi}{2} \right)}{\frac{\pi}{2 \Delta} - t} \right)\\
		&= \frac{\pi}{4 \Delta} \left( \frac{1}{\frac{\pi}{2 \Delta} + t} + \frac{1}{\frac{\pi}{2 \Delta} - t} \right) \cos(t \Delta)\\
		&= \frac{\frac{\pi^2}{4 \Delta^2}}{\frac{\pi^2}{4 \Delta^2} - t^2} \cos(t \Delta)\\
		&= \frac{\cos(t \Delta)}{1 - \left( \frac{2 t \Delta}{\pi} \right)^2}
	\end{align*}
	Therefore,
	\begin{align*}
		P(\omega) &= \frac{1}{2 \pi} R(\omega) \ast Q(\omega)
	\end{align*}
	Hence,
	\begin{align*}
		p(t) &= r(t) q(t)\\
		&= \sinc(\pi t) \frac{\cos(t \Delta)}{1 - \left( \frac{2 t \Delta}{\pi} \right)^2}
	\end{align*}
	Similarly, for a general $T$,
	\begin{align*}
		p(t) &= \sinc\left( \pi \frac{t}{T} \right) \frac{\cos\left( \rho \pi \frac{t}{T} \right)}{1 - \left( \frac{2 t \rho}{T} \right)^2}
	\end{align*}
\end{proof}

\begin{question}
	Let
	\begin{align*}
		p(t) &= c(t) r(t)
	\end{align*}
	where
	\begin{align*}
		R(\omega) &= \rect\left( \frac{\omega}{2 \pi} \right)\\
		&=
			\begin{cases}
				1 &;\quad |\omega| \le \pi\\
				0 &;\quad \text{otherwise}\\
			\end{cases}\\
		C(\omega) &= A \tri\left( \frac{\omega}{\Delta} \right)\\
		&=
			\begin{cases}
				A \left( 1 - \frac{|\omega|}{\Delta} \right) &;\quad |\omega| \le \Delta\\
				0 &;\quad \text{otherwise}\\
			\end{cases}\\
	\end{align*}
	\begin{enumerate}
		\item
			Find $P(\omega)$.
		\item
			Given
			\begin{align*}
				\int\limits_{-\infty}^{\infty} c(t) \dif t &= \frac{2 \pi}{\Delta}
			\end{align*}
			find $A$.
		\item
			Does the NISI condition hold for the above?
		\item
			Find $p(t)$ for $\Delta \to 0$.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			\begin{align*}
				P(\omega) &= \FT\left\{ p(t) \right\}\\
				&= \FT\left\{ c(t) r(t) \right\}\\
				&= \frac{1}{2 \pi} \FT\left\{ c(t) \right\} \ast \FT\left\{ r(t) \right\}\\
				&= \frac{1}{2 \pi} C(\omega) \ast R(\omega)\\
				&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} R(\sigma) C(\omega - \sigma) \dif \sigma\\
				&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} R(\sigma) C(\sigma - \omega) \dif \sigma\\
				&=
					\begin{cases}
						0 &;\quad \omega < -\pi - \Delta\\
						\frac{A \Delta}{4 \pi} \left( 1 + \frac{\omega + \pi}{\Delta} \right)^2 &;\quad -\pi - \Delta < \omega \le -\pi\\
						\frac{A \Delta}{4 \pi} \left( 2 - \left( 1 - \frac{\omega + \pi}{\Delta} \right)^2 \right) &;\quad -\pi < \omega < -\pi + \Delta\\
						\frac{A \Delta}{4 \pi} 2 &;\quad -\pi + \Delta < \omega < \pi - \Delta\\
						\frac{A \Delta}{4 \pi} \left( 2 - \left( 1 + \frac{\omega + \pi}{\Delta} \right)^2 \right) &;\quad \pi - \Delta < \omega < \pi\\
						\frac{A \Delta}{4 \pi} \left( 1 - \frac{\omega + \pi}{\Delta} \right)^2 &;\quad \pi < \Delta < \omega < \pi + \Delta\\
						0 &;\quad \pi + \Delta < \omega\\
					\end{cases}
			\end{align*}
		\item
			\begin{align*}
				C(\omega) &= \int\limits_{-\infty}^{\infty} c(t) e^{-j \omega t} \dif t\\
				\therefore C(0) &= \int\limits_{-\infty}^{\infty} c(t) \dif t\\
				\therefore A &= \frac{2 \pi}{\Delta}
			\end{align*}
		\item
			\begin{align*}
				p(t) &= c(t) r(t)\\
				&= \sinc(\pi t) \sinc^2\left( \frac{\Delta t}{2} \right)
			\end{align*}
			Therefore, $\forall n \neq 0$,
			\begin{align*}
				p(n T) &= \sinc(n \pi T) \sinc^2\left( \frac{n \Delta T}{2} \right)\\
				&= 0
			\end{align*}
		\item
			\begin{align*}
				\lim\limits_{\Delta \to 0} C(\omega) &= 2 \pi \delta(\omega)
			\end{align*}
			Therefore,
			\begin{align*}
				P(\omega) &= \frac{1}{2 \pi} R(\omega) \ast C(\omega)\\
				&= \frac{1}{2 \pi} R(\omega) \ast \left( 2 \pi \delta(\omega) \right)\\
				&= R(\omega)
			\end{align*}
			This is also obvious as a delta in frequency corresponds to a constant $1$ in time.
			Similarly, as the triangle in frequency converges to a delta, $P(\omega)$ converges a rectangle.
			Therefore,
			\begin{align*}
				p(t) &= r(t)\\
				&= \sinc(\pi t)
			\end{align*}
	\end{enumerate}
\end{solution}

\clearpage
\part{Matched Filters}

\section{Single Pulse with White Noise}

Consider an information signal
\begin{align*}
	s(t) &= \sum\limits_{n = -\infty}^{\infty} b_n p(t - n T)
\end{align*}
Let the received signal be
\begin{align*}
	x(t) &= s(t) + v(t)
\end{align*}
where $v(t)$ is zero mean Gaussian white noise.\\
For simplicity, let $n = 1$.
Therefore,
\begin{align*}
	x(t) &= b_0 p(t) + v(t)
\end{align*}
Hence, based on $x(t)$, the receiver must decide if $b_0$ is $0$ or $1$.\\
Consider the hypotheses
\begin{align*}
	H_0 &: b_0 = 0\\
	H_1 &: b_0 = 1
\end{align*}
Equivalently,
\begin{align*}
	H_0 &: x(t) = v(t)\\
	H_1 &: x(t) = p(t) + v(t)
\end{align*}
As $v(t)$ is zero mean Gaussian white noise,
\begin{align*}
	\expct\left[ v(t) \right] &= 0\\
	\expct\left[ v(t + \tau) v(t) \right] &= \frac{N_0}{2} \delta(t)\\
	S_{v v}(\omega) &= \frac{N_0}{2}
\end{align*}
Therefore, let $y(t)$ be the signal obtained by passing $x(t)$ through a LTI system with $h(\tau)$, and let $z$ be a sample of this $y(t)$ taken at $t_0$.
Let the critical value be $\lambda$, such that $H_0$ is accepted if and only
\begin{align*}
	z &< \lambda
\end{align*}
and $H_1$ is accepted if and only if
\begin{align*}
	z &> \lambda
\end{align*}
Hence, the filter is as in
\begin{figure}[H]
	\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,>=latex']
			\node [block] (optimal_filter) {$\tilde{h}(\tau) = \tilde{p}^*\left( \tilde{t_0} - \tau \right)$};
			\draw [stealth-] (optimal_filter.west) -- ++(-1,0) node [left] {$x(t), p(t), v(t)$};
			\draw (optimal_filter.east) to [closing switch, label = $t_0$] ++(2,0) [-stealth] to ++(1,0) node [right] {$y(t_0) \gtrless \lambda$};
		\end{tikzpicture}
		\end{adjustbox}
	\caption{Matched Filter for a Single Pulse with White Noise}
	\label{fig:matched_filter_for_a_single_pulse_with_white_noise}
\end{figure}
Under either of the hypotheses, the variance of $y(t)$ is
\begin{align*}
	\sigma^2 &= \frac{N_0}{2} \int\limits_{-\infty}^{\infty} \left| h(\tau) \right|^2 \dif t
\end{align*}
Under $H_0$, the expectation of $y(t)$ is
\begin{align*}
	\expct\left[ y(t) \right] &= 0
\end{align*}
Under $H_1$, the expectation of $y(t)$ is
\begin{align*}
	\mu &= \expct\left[ y(t) \right]\\
	&= \int\limits_{-\infty}^{\infty} h(\tau) p(t_0 0 \tau) \dif \tau
\end{align*}
Hence, under $H_0$,
\begin{align*}
	z &= \normal\left( 0,\sigma^2 \right)
\end{align*}
and under $H_1$,
\begin{align*}
	z &= \normal\left( \mu,\sigma^2 \right)
\end{align*}
Hence, the critical value should be
\begin{align*}
	\lambda &= \frac{\mu}{2}
\end{align*}
Therefore, the probability of a type 1 error
\footnote{False positive}
is
\begin{align*}
	\prob\left( z > \lambda \Big| H_0 \right) &= \int\limits_{\lambda}^{\infty} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \frac{z^2}{\sigma^2}} \dif z\\
	&= \Q\left( \frac{\lambda}{\sigma} \right)\\
	&= \Q\left( \frac{\mu}{2 \sigma} \right)
\end{align*}
Hence, in order to minimize the probability of a type 1 error, $\frac{|\mu|^2}{\sigma^2}$ should be maximized.
Hence, by the Cauchy-Schwartz inequality for integrals over complex valued functions
\footnote{
	\begin{align*}
		\left| \int\limits_{a}^{b} w_1(t) {w_2}^*(t) \dif t \right|^2 &\le \int\limits_{a}^{b} \left| w_1(t) \right|^2 \dif t \int\limits_{a}^{b} \left| w_2(t) \right|^2 \dif t
	\end{align*}
	and the equality holds if and only if $\exists \rho, \theta$, such that $w_2 = \rho e^{j \theta} w_1$.
}
,
\begin{align*}
	\left| \int\limits_{-\infty}^{\infty} h(\tau) p(t_0 - \tau) \dif \tau \right|^2 &\le \int\limits_{-\infty}^{\infty} \left| h(\tau) \right|^2 \dif t \int\limits_{-\infty}^{\infty} \left| p^*(t_0 - \tau) \right|^2 \dif \tau
\end{align*}
Hence, for the equality to be satisfied, i.e. for minimal error, the LTI system must be chosen such that
\begin{align*}
	h(\tau) &= p^*(t_0 - \tau)
\end{align*}
Furthermore, $t_0$ must be chosen such $h(\tau)$ is causal.
Let $t_f$ be the time at which the pulse $p(t)$ ends.
Hence, for $h(\tau)$ to be causal,
\begin{align*}
	t_0 > t_f
\end{align*}
Hence, the optimal filter, matched to the pulse, is given by
\begin{align*}
	h_{\text{optimal}}(\tau) &= p^*(t_f - \tau)\\
	H_{\text{optimal}}(\omega) &= e^{-j \omega \tilde{t_0}} \tilde{P}^*(\omega)
\end{align*}

\section{Single Pulse with Coloured Noise}

\begin{theorem}[Paley-Wiener Theorem]
	If $S_{v v}(\omega)$ is positive and bounded for all $\omega$, then there exists a stable and causal system $G(\omega)$ with a stable and causal inverse, such that
	\begin{align*}
		S_{v v}(\omega) &= G(\omega) G^*(\omega)\\
		&= \left| G(\omega) \right|^2
	\end{align*}
	\label{thm:Paley_Wiener_theorem}
\end{theorem}

A matched filter for a single pulse, affected by coloured noise, can be implemented using a matched filter for white noise, as in \cref{fig:matched_filter_for_coloured_noise_implemented_using_a_matched_filter_for_white_noise}.
\begin{figure}[H]
	\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,>=latex']
			\node [block] (whitening_filter) {$\frac{1}{G(\omega)}$\\whitening filter};

			\draw [stealth-] (whitening_filter.west) -- ++(-1,0) node [left] {$x(t), p(t), v(t)$};
			\node [block, right = of whitening_filter] (optimal_filter) {$\tilde{h}(\tau) = \tilde{p}^*\left( \tilde{t_0} - \tau \right)$};
			\draw (whitening_filter.east) -- (optimal_filter.west) node [midway, above] {$\tilde{x}(t)$\\$\tilde{p}(t)$\\$\tilde{v}(t)$};
			\draw (optimal_filter.east) to [closing switch, label = $\tilde{t_0}$] ++(2,0) [-stealth] to ++(1,0) node [right] {$\tilde{y}\left( \tilde{t_0} \right) \gtrless \tilde{\lambda}$};
		\end{tikzpicture}
		\end{adjustbox}
	\caption{Matched Filter for Coloured Noise Implemented Using a Matched Filter for White Noise}
	\label{fig:matched_filter_for_coloured_noise_implemented_using_a_matched_filter_for_white_noise}
\end{figure}

\section{Multiple Pluses with White Noise}

\begin{theorem}
	For a information signal
	\begin{align*}
		s(t) &= \sum\limits_{n = -\infty}^{\infty} b_n p(t - n T)
	\end{align*}
	\begin{align*}
		\overline{H}(\omega) &= H_T(\omega) H_R(\omega)\\
		&= P(\omega) e^{-j \omega t_0} p^*(\omega)\\
		&= e^{-j \omega t_0} \left| P(\omega) \right|^2
	\end{align*}
	Hence, let
	\begin{align*}
		R(\omega) &= \left| P(\omega) \right|^2
	\end{align*}
	Therefore,
	\begin{align*}
		\overline{s}(t) &= \sum\limits_{n = -\infty}^{\infty} b_n r(t - n T)
	\end{align*}
	Hence, in order to have no ISI, $\left| P(\omega) \right|^2$ must satisfy the NISI condition.
	Hence, a raised cosine signals cannot be used, and root raised cosine pulses must be used.
\end{theorem}

\section{Single Pulse with Non-binary Levels}

Let $H_i$ where $0 \le i \le m - 1$ be the hypotheses
\begin{align*}
	b_i &= l_i
\end{align*}
Therefore,
\begin{align*}
	\expct\left[ y(t_0) \Big| H_m \right] &= \int\limits_{-\infty}^{\infty} h(\tau) l_m p(t_0 - \tau) \dif \tau\\
	&= l_m \int\limits_{-\infty}^{\infty} h(\tau) p(t_0 - \tau) \dif \tau
\end{align*}
Let
\begin{align*}
	\mu &= \int\limits_{-\infty}^{\infty} h(\tau) p(t_0 - \tau) \dif \tau
\end{align*}
Therefore,
\begin{align*}
	\expct\left[ y(t_0) \Big| H_m \right] &= l_m \mu
\end{align*}
Similarly,
\begin{align*}
	\var\left( y(t_0) \Big| H_m \right) &= \sigma^2\\
	&= \frac{N_0}{2} \int\limits_{-\infty}^{\infty} \left| h(\tau) \right|^2 \dif \tau
\end{align*}
Therefore, as for the case with binary levels, in order to minimize the probability of a type 1 error, $\frac{|\mu|^2}{\sigma^2}$ should should be maximized.
Hence, the matched filter is the same as for the case with binary levels, with the comparison with the threshold $\lambda$ replaced by approximation to the nearest $l_i \mu$.

\section{Quadrature Phase Shift Keying}

Consider a complex modulated signal
\begin{align*}
	s_{\text{RF}}(t) &= s(t) \cos(\omega_c t)\\
	&= \Re\left\{ s(t) e^{j \omega_c t} \right\}
\end{align*}
where $s(t)$ is the information signal, and $\omega_c$ is the carrier frequency.
Hence, assuming zero noise, the received signal is
\begin{align*}
	x_{\text{RF}}(t) &= s_{\text{RF}}(t)\\
	&= \Re\left\{ s(t) e^{j \omega_c t} \right\}\\
	&= \frac{1}{2} s(t) e^{j \omega_c t} + \frac{1}{2} s^*(t) e^{-j \omega_c t}
\end{align*}
Let
\begin{align*}
	\tilde{y}(t) &= 2 e^{-j \omega_c t} x_{\text{RF}}(t)\\
	&= s(t) + s(t) e^{-2 j \omega_c t}
\end{align*}
Hence, $s(t)$ can be reconstructed from $y(t)$ by applying a LPF with cutoff frequency less than $2 \omega_c$.\\
This complex multiplication of $x(t)$ with $2 e^{-j \omega_c t}$ can be implemented by splitting $2 e^{-j \omega_c t}$ as
\begin{align*}
	2 e^{-j \omega_c t} &= 2 \cos(\omega_c t) - j \left( 2 \sin(\omega_c t) \right)
\end{align*}
and then multiplying $x(t)$ separately by the two terms, and treating the first product as the real part, and the second as the imaginary part.
Hence, $s(t)$ can be reconstructed as in \cref{fig:reconstruction_of_complex_valued_signal_implemented_by_separately_treating_real_and_imaginary_parts}.
\begin{figure}[H]
	\centering
	\begin{adjustbox}{max width=\columnwidth}
	\begin{tikzpicture}[auto, node distance=2cm,>=latex']
		\draw (0,0) -- (-1,0) node [left] {$x_{\text{RF}}(t)$};

		\node [mixer] (mixer_real) at (2,2) {};
		\draw [-stealth] (0,0) |- (mixer_real.west);
		\draw [stealth-] (mixer_real.north) -- ++(0,1) node [above] {$2 \cos(\omega_c t)$};
		\node [block] (LPF_real) at (4,2) {LPF};
		\draw [-stealth] (mixer_real.east) -- (LPF_real.west);
		\draw [-stealth] (LPF_real.east) -- ++(1,0) node [right] {$\Re\left\{ s(t) \right\}$};

		\node [mixer] (mixer_imaginary) at (2,-2) {};
		\draw [-stealth] (0,0) |- (mixer_imaginary.west);
		\draw [stealth-] (mixer_imaginary.south) -- ++(0,-1) node [below] {$-2 \sin(\omega_c t)$};
		\node [block] (LPF_imaginary) at (4,-2) {LPF};
		\draw [-stealth] (mixer_imaginary.east) -- (LPF_imaginary.west);
		\draw [-stealth] (LPF_imaginary.east) -- ++(1,0) node [right] {$\Im\left\{ s(t) \right\}$};
	\end{tikzpicture}
	\end{adjustbox}
	\caption{Reconstruction of Complex Valued Signal Implemented by Separately Treating Real and Imaginary Parts}
	\label{fig:reconstruction_of_complex_valued_signal_implemented_by_separately_treating_real_and_imaginary_parts}
\end{figure}
Hence, as it is possible to use a complex valued information signal, the coding levels can occupy complex values, i.e. for example, the levels $0$ to $3$ can be represented by $\pm \sqrt{0.5} \pm \sqrt{0.5}$.
Hence, the levels are more loosely packed in QPSK, in comparison with ASK, as in \cref{fig:comparison_of_QPSK_and_ASK_levels}.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[scale = 3]
		\def\xMIN{-2};
		\def\xMAX{2};
		\def\yMIN{-2};
		\def\yMAX{2};

		\begin{scope}[stealth-stealth]
			\draw (\xMIN,0) -- (\xMAX,0) node [right] {$\Re$};
			\draw (0,\yMIN) -- (0,\yMAX) node [above] {$\Im$};
		\end{scope}

		\begin{scope}[blue]
			\filldraw (45:1) circle (0.5pt) node [above right] {${l_0}_{\text{QPSK}}$};
			\filldraw (135:1) circle (0.5pt) node [above left] {${l_1}_{\text{QPSK}}$};
			\filldraw (225:1) circle (0.5pt) node [below left] {${l_2}_{\text{QPSK}}$};
			\filldraw (315:1) circle (0.5pt) node [below right] {${l_3}_{\text{QPSK}}$};
		\end{scope}

		\begin{scope}[red]
			\filldraw (-1,0) circle (0.5pt) node [below] {${l_0}_{\text{ASK}}$};
			\filldraw (-1/3,0) circle (0.5pt) node [below] {${l_1}_{\text{ASK}}$};
			\filldraw (1/3,0) circle (0.5pt) node [below] {${l_2}_{\text{ASK}}$};
			\filldraw (1,0) circle (0.5pt) node [below] {${l_3}_{\text{ASK}}$};
		\end{scope}
	\end{tikzpicture}
	\caption{Comparison of QPSK and ASK Levels}
	\label{fig:comparison_of_QPSK_and_ASK_levels}
\end{figure}
Although QPSK is more immune to noise than ASK, it is sensitive to the phase shift of the transmitter.
That is, if the transmitter works with $e^{j (\omega_c t + \varphi)}$ instead of $e^{j \omega_c t}$, the reconstructed signal will be shifted by $\varphi$ with respect to the original signal.\\
This can be resolved by using a predetermined training sequence, known to both the transmitter and the receiver, to synchronize the phase shift.

\begin{question}
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width = \textwidth}
		\begin{tikzpicture}
			\node [block] (quantizer) at (1,0) {Quantizer};
			\draw [stealth-] (quantizer.west) -- ++(-1,0) node [left] {$m(t)$};
			\node [block] (pulse) at (6,0) {$p(t)$};
			\draw [-stealth] (quantizer.east) -- (pulse.west) node [midway, above] {$\sum\limits_{n = -\infty}^{\infty} b_n \delta(t - n T)$};
			\node [adder] (noise_adder) at (9,0) {};
			\draw [-stealth] (pulse.east) -- (noise_adder.west) node [midway, above] {$s(t)$};
			\draw [stealth-] (noise_adder.south) -- ++(0,-1) node [below] {$v(t)$};
			\node [block] (matched_filter) at (13,0) {$h(t)$};
			\draw [-stealth] (noise_adder.east) -- (matched_filter.west) node [midway, above] {$x(t)$};
			\node [block] (decision_rule) at (19,0) {Decision Rule};
			\draw (matched_filter) -- ++(2,0) node [midway, above] {$y(t)$} to [closing switch = $t_n$] ($ (decision_rule.west) + (-2,0) $) -- (decision_rule.west) node [midway, above] {$y(t_n)$};
			\draw [-stealth] (decision_rule.east) -- ++(1,0) node [right] {$\hat{b_n}$};
		\end{tikzpicture}
		\end{adjustbox}
	\end{figure}
	Consider a communication scheme as shown, where
	\begin{itemize}
		\item
			$m(t)$ is uniformly distributed between $-1$ and $1$
		\item
			The quantizer is a four level MMSE quantizer with Gray coding
		\item
			$v(t)$ is white Gaussian noise with zero mean and variance $\frac{N_0}{2}$
		\item
			\begin{align*}
				T &= \frac{3}{2}
			\end{align*}
		\item
			\begin{align*}
				p(t) &= u(t) - u(t - 2)
			\end{align*}
	\end{itemize}
	\begin{enumerate}
		\item
			Find
			\begin{align*}
				P(\omega) &= \FT\left\{ p(t) \right\}
			\end{align*}
		\item
			Find $h(t)$ such that the error probability is minimal.
		\item
			Given
			\begin{align*}
				m(0) &= 0.11\\
				m(T) &= 0.53\\
				m(2 T) &= -0.41\\
				m(3 T) &= -0.9\\
				\tilde{v}(t_0 + T) &= 0.4
			\end{align*}
			find the estimated symbol at the receiver for $m(T)$.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			\begin{align*}
				p(t) &= \rect\left( \frac{t - 1}{2} \right)
			\end{align*}
			Therefore,
			\begin{align*}
				P(\omega) &= \FT\left\{ \rect\left( \frac{t - 1}{2} \right) \right\}\\
				&= e^{-j \omega} \FT\left\{ \rect\left( \frac{t}{2} \right) \right\}\\
				&= 2 e^{-j \omega} \sinc(\omega)
			\end{align*}
		\item
			\begin{align*}
				H(\omega) &= e^{-j t_0 \omega} P^*(\omega)\\
				&= e^{-2 j \omega} 2 e^{-j \omega} \sinc(\omega)\\
				&= 2 e^{-j \omega} \sinc(\omega)\\
				&= P(\omega)
			\end{align*}
		\item
			Let the quantization values be between $0$ and $1$.
			Therefore,
			\begin{align*}
				l_0 &= 0\\
				l_1 &= \frac{1}{3}\\
				l_2 &= \frac{2}{3}\\
				l_3 &= 1
			\end{align*}
			Therefore, according to the given values of $m(n T)$,
			\begin{align*}
				b_0 &= \frac{2}{3}\\
				b_1 &= 1\\
				b_2 &= \frac{1}{3}\\
				b_3 &= 0
			\end{align*}
			As $m(T)$ is obtained from
			\begin{align*}
				y(t_0 + T) &= \tilde{s}(t_0 + T) + \tilde{v}(t_0 + T)\\
				&= \left. \sum\limits_{n = -\infty}^{\infty} b_n \tilde{h}(t - n T) \right|_{t_0 - n T} + 0.4\\
				&= \left( \frac{1}{2} b_0 + 2 b_1 + \frac{1}{2} b_2 \right) + 0.4\\
				&= \frac{1}{3} + 2 + \frac{1}{6} + 0.4\\
				&= 2.9
			\end{align*}
			Therefore,
			\begin{align*}
				\tilde{l_1} &= \argmin\limits_{l_n \in \left\{ 0, \frac{1}{3}, \frac{2}{3}, 1 \right\}} \left| \mu l_n - \tilde{y}(t_0 + T) \right|\\
				\tilde{l_1} &= \argmin\limits_{l_n \in \left\{ 0, \frac{1}{3}, \frac{2}{3}, 1 \right\}} \left| 2 l_n - 2.9 \right|\\
				&= l_3\\
				&= 1
			\end{align*}
			Therefore,
			\begin{align*}
				\tilde{s_1} &= 0.75
			\end{align*}
	\end{enumerate}
\end{solution}

\section{Delta Pulse Code Modulation}

\begin{definition}[Delta PCM]
	A modulation in which the difference between the current data and the previous data is transmitted, rather than transmitting the data itself, is called delta pulse code modulation.
\end{definition}

A nave implementation of delta PCM is as in \cref{fig:conceptual_implementation_of_PCM_transmitter} and \cref{fig:implementation_of_PCM_receiver}.
However, in such an implementation, the transmitter predictor and the receiver predictor operated based on $u_{n - 1}$ and $\tilde{u_{n - 1}}$ respectively, their outputs may differ.
As there is no way to fix the discrepancy between these errors, this error between the prediction errors accumulates.\\
This can be corrected by implementing the transmitter as in \cref{fig:practical_implementation_of_PCM_transmitter}.
In such an implementation, the discrepancy can be fixed by anticipating the error between errors, and using the same inputs for the transmitter predictor as the receiver predictor.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{max width = \textwidth}
	\begin{tikzpicture}
		\node [block] (sampler) at (1,0) {sampler};
		\node [block] (delay) at (3,-2) {$z^{-1}$};
		\node [block] (predictor) at (5,-4) {predictor};
		\node [adder] (adder) at (7,0) {};
		\node [block] (quantizer) at (9,0) {$Q_m$};
		\node [block] (PCM_mod) at (14,0) {PCM modulator};

		\draw [stealth-] (sampler.west) -- ++(-1,0) node [left] {$u(t)$};
		\draw [-stealth] (sampler.east) -- (adder.west) node [midway, above] {$u_n$};
		\node [above left] at (adder.west) {$+$};
		\draw [-stealth] (sampler.east) -| (delay.north);
		\draw [-stealth] (delay.south) |- (predictor.west) node [midway, below left] {$u_{n - 1}$};
		\draw [-stealth] (predictor.east) -| (adder.south) node [midway, below right] {$\hat{u_n}$};
		\node [below right] at (adder.south) {$-$};
		\draw [-stealth] (adder.east) -- (quantizer.west) node [midway, above] {$e_n$};
		\draw [-stealth] (quantizer.east) -- (PCM_mod.west) node [midway, above] {$\tilde{e_n} = e_n + q_n$};
		\draw [-stealth] (PCM_mod.east) -- ++(1,0) node [right] {$x_{\text{PCM}}(t)$};
	\end{tikzpicture}
	\end{adjustbox}
	\caption{Conceptual Implementation of PCM Transmitter}
	\label{fig:conceptual_implementation_of_PCM_transmitter}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{max width = \textwidth}
	\begin{tikzpicture}
		\node [block] (PCM_demod) at (0,0) {PCM demodulator};
		\node [adder] (adder) at (5,0) {};
		\node [block] (predictor) at (7,-4) {predictor};
		\node [block] (delay) at (9,-2) {$z^{-1}$};
		\node [block] (DAC) at (11,0) {D/A, D/C};

		\draw [stealth-] (PCM_demod.west) -- ++(-1,0) node [left] {$x_{\text{PCM}}(t)$};
		\draw [-stealth] (PCM_demod.east) -- (adder.west) node [midway, above] {$\tilde{e_n} = e_n + q_n$};
		\draw [stealth-] (adder.south) |- (predictor.west) node [midway, below left] {$\tilde{\hat{u_n}}$};
		\draw [stealth-] (predictor.east) -| (delay.south) node [midway, below right] {$\tilde{u_{n - 1}}$};
		\draw [stealth-] (delay.north) |- (adder.east);
		\draw [-stealth] (adder.east) -- (DAC.west) node [midway, above] {$\tilde{u_n}$};
		\draw [-stealth] (DAC.east) -- ++(1,0) node [right] {$\tilde{u}(t)$};
	\end{tikzpicture}
	\end{adjustbox}
	\caption{Implementation of PCM Receiver}
	\label{fig:implementation_of_PCM_receiver}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{max width = \textwidth}
	\begin{tikzpicture}
		\node [block] (sampler) at (1,0) {sampler};
		\node [adder] (adder_1) at (3,0) {};
		\node [block] (quantizer) at (5,0) {$Q_m$};
		\node [block] (delay_1) at (8,-2) {$z^{-1}$};
		\node [block] (predictor) at (5,-4) {predictor};
		\node [block] (delay_2) at (5,-6) {$z^{-1}$};
		\node [adder] (adder_2) at (8,-4) {};

		\node [block] (PCM_mod) at (12,0) {PCM modulator};

		\draw [stealth-] (sampler.west) -- ++(-1,0) node [left] {$u(t)$};
		\draw [-stealth] (sampler.east) -- (adder_1.west) node [midway, above] {$u_n$};
		\draw [-stealth] (adder_1.east) -- (quantizer.west) node [midway, above] {$e_n$};
		\draw [-stealth] (quantizer.east) -- (PCM_mod.west) node [midway, above] {$\tilde{e_n} = e_n + q_n$};
		\draw [-stealth] (quantizer.east) -| (delay_1.north);
		\draw [-stealth] (delay_1.south) -- (adder_2.north) node [midway, right] {$\tilde{e_{n - 1}}$};
		\draw [-stealth] (adder_2.west) -- (predictor.east) node [midway, above] {$\tilde{u_{n - 1}}$};
		\draw [-stealth] (predictor.west) -| (adder_1.south) node [midway, left] {$\tilde{\hat{u_n}}$};
		\draw [-stealth] (predictor.west) -- (3,-4) |- (delay_2.west);
		\draw [-stealth] (delay_2.east) -| (adder_2.south) node [midway, below right] {$\tilde{\hat{u_{n - 1}}}$};
	\end{tikzpicture}
	\end{adjustbox}
	\caption{Practical Implementation of PCM Transmitter}
	\label{fig:practical_implementation_of_PCM_transmitter}
\end{figure}

\section{Delta Modulation}

\begin{definition}[Delta modulation]
	A modulation scheme in which the sign of the difference between the data at a particular time and that of the previous time is transmitted is called delta modulation.
	Hence, a value of $+\Delta$ is transmitted if the data point is higher than the previous data point, and a value of $-\Delta$ is transmitted if it is lower.
\end{definition}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [adder] (adder) at (1,0) {};
		\node [block] (relay) at (5,0) {$\pm \Delta$ relay};
		\node [block] (integrator_amplifier) at (8,-2) {integrator and amplifier};

		\draw [-stealth] (adder.west) -- ++(-1,0) node [left] {$u(t)$};
		\draw [-stealth] (adder.east) -- (relay.west) node [midway, above] {$e(t)$};
		\draw (relay.east) to [closing switch = ZOH with $t_n {=} n T$] (11,0);
		\draw [-stealth] (11,0) |- (integrator_amplifier.east);
		\draw [-stealth] (integrator_amplifier.west) -| (adder.south);
		\draw [-stealth] (11,0) -- ++(1,0) node [right] {$\pm \Delta$};
	\end{tikzpicture}
	\caption{Delta Modulation Transmitter}
	\label{fig:delta_modulation_transmitter}
\end{figure}

As the transmitted values are $\pm \Delta$, the reconstructed signal changes by a large amount even if the real signal changes by a very small amount.
This results in large oscillations in the reconstruction when the input is relatively constant.
This phenomenon is called granular noise.\\
Also, if the real signal increases very rapidly, hence the reconstructed signal cannot keep up with the rate of increase, and results in a slower response.
This phenomenon is called slope overload noise.

\section{Complex Envelope Representation of Band Pass Signals}

\begin{definition}[Band pass signal]
	A signal whose spectrum is around some high frequency $\omega_c$, such that the bandwidth $B$ of the signal is much smaller than $\omega_c$, is called a band pass signal.
\end{definition}

\begin{definition}
	A complex function
	\begin{align*}
		g(t) &= x(t) + j x(t)
	\end{align*}
	is said to be the complex envelope of a band pass signal $v(t)$, if
	\begin{align*}
		v(t) &= \Re\left\{ g(t) e^{j \omega_c t} \right\}\\
		&= \frac{1}{2} g(t) e^{j \omega_c t} + \frac{1}{2} g^*(t) e^{-j \omega_c t}
	\end{align*}
	Hence,
	\begin{align*}
		x(t) &= \Re\left\{ g(t) \right\}
	\end{align*}
	is called the in-phase component of $v(t)$, and
	\begin{align*}
		y(t) &= \Im\left\{ g(t) \right\}
	\end{align*}
	is called the quadrature component of $v(t)$.
\end{definition}

\begin{theorem}
	A band pass signal $v(t)$ can be represented as
	\begin{align*}
		v(t) &= R(t) \cos\left( \omega_c t + \varphi(t) \right)
	\end{align*}
	where
	\begin{align*}
		R(t) &= \sqrt{x^2(t) + y^2(t)}\\
		\varphi(t) &= \atan2\left( y(t),x(t) \right)
	\end{align*}
\end{theorem}

\begin{theorem}
	For a bandpass signal $v(t)$ with complex envelope $g(t)$,
	\begin{align*}
		V(\omega) &= \frac{1}{2} G(\omega - \omega_c) + \frac{1}{2} G^*\left( -(\omega + \omega_c) \right)\\
		\tilde{R}_{v v}(t_1,t_2) &= \frac{1}{2} \Re\left\{ \tilde{R}_{g g}(t_1,t_2) e^{j \omega_c (t_1 - t_2)} + \tilde{R}_{g g^*}(t_1,t_2) e^{j \omega_c (t_1 + t_2)} \right\}
	\end{align*}
	and if $g(t)$ is WSS and circular\footnote{See \cref{def:circular_random_process}.},
	\begin{align*}
		R_{v v}(\tau) &= \frac{1}{4} \left( R_{g g}(\tau) e^{j \omega_c \tau} + R_{g g^*}(\tau) e^{-j \omega_c \tau} \right)
	\end{align*}
	and hence
	\begin{align*}
		S_{v v}(\omega) &= \frac{1}{4} \left( S_{g g}(\omega - \omega_c) + S_{g g}^*\left( -(\omega + \omega_c) \right) \right)\\
		S_{v v}(\omega) &= \frac{1}{4} \left( S_{g g}(\omega - \omega_c) + S_{g g}\left( -(\omega + \omega_c) \right) \right)
	\end{align*}
\end{theorem}

\begin{theorem}
	Let $v(t)$ be a band pass signal with complex envelope $g(t)$.
	If $v(t)$ and $g(t)$ are deterministic,
	\begin{align*}
		E_v &= \frac{1}{2} E_g
	\end{align*}
	where $E_v$ and $E_g$ are the energies of $v(t)$ and $g(t)$, respectively.\\
	If $v(t)$ and $g(t)$ are stochastic,
	\begin{align*}
		P_v &= \frac{1}{2} P_g
	\end{align*}
	where $P_v$ and $P_g$ are the powers of $v(t)$ and $g(t)$, respectively.
\end{theorem}

\begin{proof}
	If $v(t)$ and $g(t)$ are deterministic, the energy of $v(t)$ is
	\begin{align*}
		E_v &= \int\limits_{-\infty}^{\infty} v^2(t) \dif t\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} \left| V(\omega) \right|^2 \dif \omega\\
		&= \int\limits_{-\infty}^{\infty} \left| \tilde{V}(f) \right| \dif f\\
		&= 2 \int\limits_{-\infty}^{\infty} \left| \frac{1}{2} \tilde{G}(f) \right|^2 \dif f\\
		&= \frac{1}{2} \int\limits_{-\infty}^{\infty} \left| \tilde{G}(f) \right|^2 \dif f\\
		&= \frac{E_g}{2}
	\end{align*}
	If $v(t)$ and $g(t)$ are stochastic, the power of $v(t)$ is
	\begin{align*}
		P_v &= \expct\left[ v^2(t) \right]\\
		&= R_{v v}(0)\\
		&= \frac{1}{2 \pi} \int\limits_{-\infty}^{\infty} S_{v v}(\omega) \dif \omega\\
		&= \int\limits_{-\infty}^{\infty} \tilde{S}_{v v}(f) \dif f\\
		&= 2 \int\limits_{-\infty}^{\infty} \frac{1}{4} \tilde{S}_{g g}(f) \dif f\\
		&= \frac{1}{2} \int\limits_{-\infty}^{\infty} \tilde{S}_{g g}(f) \dif f\\
		&= \frac{1}{2} P_g
	\end{align*}
\end{proof}

\begin{question}
	Consider a band pass signal
	\begin{align*}
		v(t) &= A \sin(\omega_0 t) \cos\left( \omega_c t +  m(t) \right)
	\end{align*}
	where
	\begin{align*}
		m(t) &= 10 \pi t
	\end{align*}
	\begin{enumerate}
		\item
			Find the complex envelope $g(t)$ of $v(t)$.
		\item
			Find the in-phase and quadrature components $x(t)$ and $y(t)$ of $v(t)$.
		\item
			Find $G(\omega)$ and $V(\omega)$.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			\begin{align*}
				v(t) &= \Re\left\{ A \sin(\omega_0 t) e^{j \left( \omega_c t + m(t) \right)} \right\}\\
				&= \Re\left\{ \left( A \sin(\omega_0 t) e^{j m(t)} \right) e^{j \omega_c t} \right\}
			\end{align*}
			Hence,
			\begin{align*}
				g(t) &= A \sin(\omega_0 t) e^{j m(t)}\\
				&= A \sin(\omega_0 t) e^{j 10 \pi t}
			\end{align*}
		\item
			\begin{align*}
				x(t) &= \Re\left\{ g(t) \right\}\\
				&= \Re\left\{ \sin(\omega_0 t) e^{j 10 \pi t} \right\}\\
				&= A \sin(\omega_0 t) \cos(10 \pi t)\\
				y(t) &= \Im\left\{ g(t) \right\}\\
				&= \Im\left\{ \sin(\omega_0 t) e^{j 10 \pi t} \right\}\\
				&= A \sin(\omega_0 t) \sin(10 \pi t)
			\end{align*}
		\item
			\begin{align*}
				G(\omega) &= \FT\left\{ g(t) \right\}\\
				&= \FT\left\{ A \sin(\omega_0 t) e^{j 10 \pi t} \right\}\\
				&= A \FT\left\{ \sin(\omega_0 t) e^{j 10 \pi t} \right\}\\
				&= A \FT\left\{ \sin(\omega_0 t) \right\} \Big|_{\omega = 10 \pi}\\
				&= \frac{A \pi}{j} \left( \delta(\omega - 10 \pi - \omega_0) - \delta(\omega - 10 \pi + \omega_0) \right)\\
				&= j A \omega \left( \delta(\omega - 10 \pi + \omega_0) - \delta(\omega - 10 \pi - \omega_0) \right)
			\end{align*}
			Similarly,
			\begin{align*}
				V(\omega) &= \FT\left\{ v(t) \right\}\\
				&= \frac{1}{2} \left( G(\omega - \omega_c) + G^*(-\omega - \omega_c) \right)\\
				&= \frac{j A \pi}{2} \left( \quad \delta(\omega - \omega_c - 10 \pi + \omega_0) - \delta(\omega - \omega_c - 10 \pi - \omega_0) \right.\\
				&\quad\quad\quad\quad \left. + \delta(-\omega - \omega_c - 10 \pi - \omega_0) - \delta(-\omega - \omega_c - 10 \pi + \omega_0) \right)\\
				&= \frac{j A \pi}{2} \left( \quad \delta\left( \omega - (10 \pi - \omega_0 + \omega_c) \right) - \delta\left( \omega - (10 \pi + \omega_0 + \omega_c) \right) \right.\\
				&\quad\quad\quad\quad \left. - \delta\left( \omega - (-10 \pi + \omega_0 - \omega_c) \right) + \delta\left( \omega - (-10 \pi - \omega_0 - \omega_c) \right) \right)
			\end{align*}
	\end{enumerate}
\end{solution}

\begin{question}
	Let $g(t)$ be complex random process with mean $\tilde{\eta}_g(t)$, ACF $\tilde{R}_{g g}(t_1,t_2)$, and pseudo ACF $\tilde{R}_{g g^*}(t_1,t_2)$.
	\begin{enumerate}
		\item
			Compute $\tilde{\eta}_v(t)$ and $\tilde{R}_{v v^*}(t_1,t_2)$
		\item
			Given
			\begin{align*}
				\tilde{\eta_g}(t) &= 0\\
				\tilde{R}_{g g}(t_1,t_2) &= \tilde{R}_{g g}(t_1 - t_2)
			\end{align*}
			is $v(t)$ WSS?
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}
		\item
			\begin{align*}
				\tilde{\eta}_v(t) &= \expct\left[ v(t) \right]\\
				&= \expct\left[ \Re\left\{ g(t) e^{j \omega_c t} \right\} \right]\\
				&= \expct\left[ \frac{1}{2} g(t) e^{j \omega_c t} + \frac{1}{2} g^*(t) e^{-j \omega_c t} \right]\\
				&= \frac{1}{2} \tilde{\eta}_g(t) e^{j \omega_c t} + \frac{1}{2} {\tilde{\eta}_g}^*(t) e^{-j \omega_c t}\\
				&= \Re\left\{ \tilde{\eta}_g(t) e^{j \omega_c t} \right\}
			\end{align*}
			Similarly,
			\begin{align*}
				\tilde{R}_{v v^*}(t_1,t_2) &= \expct\left[ \left( \frac{1}{2} g(t_1) e^{j \omega_c t_1} + \frac{1}{2} g^*(t_1) e^{-j \omega_c t_1} \right) \right.\\
				&\quad\quad \left. \left( \frac{1}{2} g^*(t_2) e^{-j \omega_c t_2} + \frac{1}{2} g(t_2) e^{-j \omega_c t_2} \right) \right]\\
				&= \frac{1}{2} \Re\left\{ \tilde{R}_{g g}(t_1,t_2) e^{j \omega_c (t_1 + t_2)} + \tilde{R}_{g g^*}(t_1,t_2) e^{j \omega_c (t_1 + t_2)} \right\}
			\end{align*}
		\item
			\begin{align*}
				\tilde{\eta}_v(t) &= \Re\left\{ \tilde{\eta}_g(t) e^{j \omega_c t} \right\}\\
				&= 0\\
				&= \eta_v\\
				\tilde{R}_{v v}(t_1,t_2) &= \frac{1}{2} \Re\left\{ \tilde{R}_{g g}(t_1 - t_2) e^{j \omega_c (t_1 - t_2)} + \tilde{R}_{g g^*}(t_1,t_2) e^{j \omega_c (t_1 + t_2)} \right\}
			\end{align*}
			Hence, as the ACF is dependent on $t_1 + t_2$, $v(t)$ is not WSS
	\end{enumerate}
\end{solution}

\clearpage
\part{Noise}

\section{Thermal Noise}

\begin{theorem}
	Thermal noise can be modelled as white Gaussian noise $n(t)$ with spectral level $\frac{N_0}{2}$, i.e.
	\begin{align*}
		N(\omega) &= \frac{N_0}{2}
	\end{align*}
	where
	\begin{align*}
		N_0 &= K T
	\end{align*}
	where $K$ is the Boltzmann constant, and $T$ is the absolute temperature.
	Hence, at $\SI{290}{\kelvin}$,
	\begin{align*}
		N_0 &= \SI{4e-21}{\watt\per\hertz}\\
		&= \SI{4e-18}{\milli\watt\per\hertz}\\
		&= \SI{4e-12}{\milli\watt\per\mega\hertz}\\
		&= \SI{-114}{\dBm\per\mega\hertz}
	\end{align*}
	Hence, the power corresponding to thermal noise is
	\begin{align*}
		P_{\text{noise}} &= N_0 B
	\end{align*}
	Equivalently, the power is the sum of $N_0$ and the bandwidth, both expressed in decibels.
\end{theorem}

\begin{question}
	What is the power corresponding to a noise with bandwidth $\SI{2}{\mega\hertz}$ at $\SI{290}{\kelvin}$?
\end{question}

\begin{solution}
	\begin{align*}
		P &= N_0 B\\
		&= -114 + 3\\
		&= \SI{-111}{\dBm}
	\end{align*}
\end{solution}

\begin{question}
	Given a white Gaussian thermal noise at $\SI{290}{\kelvin}$ going through a BPF with a unilateral width of
	\begin{align*}
		B &= \SI{4}{\mega\hertz}
	\end{align*}
	what is the power of the output noise?
\end{question}

\begin{solution}
	\begin{align*}
		P_{\text{out}} &= N_0 B\\
		&= \left( \SI{4e-12}{\milli\watt\per\mega\hertz} \right) (\SI{4}{\mega\hertz})\\
		&= \SI{1.6e-11}{\milli\watt}
	\end{align*}
	Equivalently,
	\begin{align*}
		(P_{\text{out}})_{\decibel} &= N_0 B\\
		&= (N_0)_{\decibel} + B_{\decibel}\\
		&= \SI{-114}{\dBm\per\mega\hertz} + \SI{6}{\decibel\mega\hertz}\\
		&= \SI{-108}{\dBm}
	\end{align*}
\end{solution}

\section{Component Noise Model}

\begin{definition}[Signal to noise ratio]
	The signal to noise ratio is defined to be the ratio of the powers of the signal and the noise, i.e.
	\begin{align*}
		\SNR &= \frac{S}{N}
	\end{align*}
	where $S$ and $N$ are the powers of the signal and the noise, respectively.
\end{definition}

\begin{definition}[Noise figure]
	A component's noise figure is defined to be the SNR degradation inflicted by the component, i.e.
	\begin{align*}
		F &= \frac{\frac{S_{\text{in}}}{N_{\text{in}}}}{\frac{S_{\text{out}}}{N_{\text{out}}}}
	\end{align*}
	where the $S$s and $N$s are the powers of the corresponding signals and noises.
\end{definition}

\begin{theorem}
	A typical RF component can be modelled as in \cref{fig:model_of_RF_component_for_noise_calculations}.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\node [buffer] (amplifier) at (4,0) {};
			\node [adder] (adder) at (2,0) {};
			\draw [-stealth] (1,-3) rectangle (5,2);
			\draw [-stealth] (0,0) node [left] {$x(t)$} -- (adder.west);
			\draw [stealth-] (adder.south) -- ++(0,-1) node [below] {$n_l(t)$};
			\draw [-stealth] (adder.east) -- (amplifier.in);
			\draw [-stealth] (amplifier.out) -- (6,0) node [right] {$y(t)$};
			\node at (4,-1) {$G$};
		\end{tikzpicture}
		\caption{Model of RF component for Noise Calculations}
		\label{fig:model_of_RF_component_for_noise_calculations}
	\end{figure}
	Hence,
	\begin{align*}
		F &= 1 + \frac{N_l}{N_{\text{in}}}\\
		&= 1 + \frac{T_l}{T_{\text{in}}}
	\end{align*}
	where $T_l$ is the equivalent temperature of the internal noise, i.e. the temperature at which the thermal noise is equal to the internal noise for the component, and $T_{\text{in}}$ is the equivalent temperature of the input noise.
\end{theorem}

\begin{proof}
	\begin{align*}
		S_{\text{out}} &= G S_{\text{in}}\\
		N_{\text{out}} &= G (N_{\text{in}} + N_l)
	\end{align*}
	Therefore,
	\begin{align*}
		F &= \frac{\frac{S_{\text{in}}}{N_{\text{in}}}}{\frac{S_{\text{out}}}{N_{\text{out}}}}\\
		&= \frac{N_{\text{in}} + N_l}{N_{\text{in}}}\\
		&= 1 + \frac{N_l}{N_{\text{in}}}\\
		&= 1 + \frac{T_l}{T_{\text{in}}}
	\end{align*}
	Therefore, let $T_l$ be the equivalent temperature of the internal noise, i.e. the temperature at which the thermal noise is equal to the internal noise for the component.
	Similarly, let $T_{\text{in}}$ be the equivalent temperature of the input noise.
\end{proof}

\begin{definition}
	\begin{align*}
		F_0 &= 1 + \frac{T_l}{T_0}
	\end{align*}
	where
	\begin{align*}
		T_0 &= \SI{290}{\kelvin}
	\end{align*}
\end{definition}

\begin{theorem}
	For passive components, i.e. with $G < 1$, $T_l$ is usually assumed to be $T_0$, and hence,
	\begin{align*}
		S_{\text{out}} &= \frac{1}{A}\\
		N_{\text{out}} &= N_{\text{in}}
	\end{align*}
	where $A$ is the attenuation.
\end{theorem}

\begin{theorem}[Frii's Formula]
	For a chain of components with parameters $G_i$, $N_i$, $F_i$, the equivalent parameters are
	\begin{align*}
		\overline{G} &= \prod\limits_{i = 1}^{L} G_i\\
		\overline{F} &= \sum\limits_{i = 1}^{L} \frac{1}{\prod\limits_{j = 1}^{i - 1} G_j} (F_i - 1)
	\end{align*}
\end{theorem}

\section{Narrowband Noise}

\begin{theorem}
	Let $n(t)$ be a narrowband noise obtained by passing a white Gaussian noise through band pass filter with unilateral width $B$.
	Hence, let
	\begin{align*}
		v_c(t) &= n(t) \left( 2 \cos(\omega_0 t + \theta) \right)\\
		v_s(t) &= n(t) \left( 2 \sin(\omega_0 t + \theta) \right)
	\end{align*}
	Hence, let $x(t)$ and $y(t)$ be the signals obtained by filtering $v_c(t)$ and $v_s(t)$, respectively, by low pass filters of widths $\frac{B}{2}$.\\
	Then, $v_c(t)$ and $v_s(t)$ are Gaussian but not WSS, and $x(t)$ and $y(t)$ are Gaussian and WSS.
	Also, if
	\begin{align*}
		S_{n n} &= P(\omega - \omega_0) + \overline{P}(\omega - \omega_0)
	\end{align*}
	where
	\begin{align*}
		\overline{P}(\omega) &= P(-\omega)
	\end{align*}
	then
	\begin{align*}
		S_{x x} &= P(\omega) + \overline{P}(\omega)\\
		S_{y y} &= P(\omega) + \overline{P}(\omega)\\
		S_{x y} &= j \left( \overline{P}(\omega) - P(\omega) \right)
	\end{align*}
	Also,
	\begin{align*}
		\expct\left[ x^2(t) \right] &= \expct\left[ y^2(t) \right]\\
		&= \expct\left[ n^2(t) \right]\\
		&= N_0 \int\limits_{-\infty}^{\infty} P(\omega) \dif \omega
	\end{align*}
\end{theorem}

\clearpage
\part{Analogue Modulation Schemes}

\section{Double Side Band Modulation}

\begin{definition}[DSB modulation]
	A modulation scheme in which the information signal $m(t)$ is multiplied by $A \cos(\omega_0 t + \varphi)$ where $\omega_0$ is the carrier frequency, in order to obtain the modulated signal, is called double side band or DSB modulation.
	Hence,
	\begin{align*}
		v(t) &= A m(t) \cos(\omega_0 t + \varphi)
	\end{align*}
\end{definition}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [mixer] (mixer) at (2,0) {};
		\draw [stealth-] (mixer.west) -- ++(-1,0) node [left] {$m(t)$};
		\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$A \cos(\omega_0 t)$};
		\draw [-stealth] (mixer.east) -- ++(1,0) node [right] {$v(t)$};
	\end{tikzpicture}
	\caption{DSB Transmitter}
	\label{fig:DSB_transmitter}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [mixer] (mixer) at (2,0) {};
		\draw [stealth-] (mixer.west) -- ++(-1,0) node [left] {$v(t)$};
		\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$2 \cos(\omega_0 t + \varphi_1)$};
		\draw [-stealth] (mixer.east) -- ++(1,0) node [right] {$\tilde{m}(t)$};
	\end{tikzpicture}
	\caption{DSB Receiver}
	\label{fig:DSB_receiver}
\end{figure}

\begin{theorem}
	Let $m(t)$ be modulated with DSB modulation, such that
	\begin{align*}
		v(t) &= A m(t) \cos(\omega_0 t + \varphi)
	\end{align*}
	Then, if $m(t)$ is deterministic,
	\begin{align*}
		V(\omega) &= \frac{A}{2} \left( e^{j \varphi} M(\omega - \omega_0) + e^{-j \varphi} M(\omega + \omega_0) \right)
	\end{align*}
	and if $m(t)$ is stochastic, and $\varphi$ uniformly distributed between $0$ and $2 \pi$, and independent of $m(t)$, then
	\begin{align*}
		R_{v v}(\tau) &= \frac{A^2}{2} R_{m m}(\tau) \cos(\omega_0 \tau)\\
		S_{v v}(\omega) &= \frac{A^2}{4} \left( S_{m m}(\omega - \omega_0) + S_{m m}(\omega + \omega_0) \right)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		v(t) &= A m(t) \frac{1}{2} \left( e^{j (\omega_0 t + \varphi)} + e^{-j (\omega_0 t + \varphi)} \right)\\
		&= \frac{A}{2} \left( e^{j \varphi} m(t) e^{j \omega_0 t} + e^{-j \varphi} m(t) e^{-j \omega_0 t} \right)
	\end{align*}
	Hence, if $m(t)$ is considered to be deterministic, with Fourier transform $M(\omega)$, and assuming $\varphi$ to be deterministic,
	\begin{align*}
		V(\omega) &= \frac{A}{2} \left( e^{j \varphi} M(\omega - \omega_0) + e^{-j \varphi} M(\omega + \omega_0) \right)
	\end{align*}
	Similarly, if $m(t)$ is considered to be a WSS process, with spectrum $S_{m m}(\omega)$ and ACF $R_{m m}(\tau)$, and assuming $\varphi$ to be deterministic,
	\begin{align*}
		R_{v v}(t,\tau) &= \expct\left[ v(t + \tau) v(t) \right]\\
		&= A^2 \expct\left[ m(t + \tau) \cos\left( \omega_0 (t + \tau) + \varphi) \right) m(t) \cos(\omega_0 t + \varphi) \right]\\
		&= A^2 R_{m m}(\tau) \cos\left( \omega_0 t + \omega_0 \tau + \varphi \right) \cos(\omega_0 t + \varphi)\\
		&= \frac{A^2}{2} R_{m m}(\tau) \left( \cos(2 \omega_0 t + \omega_0 \tau + 2 \varphi) + \cos(\omega_0 \tau) \right)
	\end{align*}
	Hence, $v(t)$ is cyclostationary with period $\frac{\pi}{\omega_0}$.\\
	If $\varphi$ is assumed to be uniformly distributed between $0$ and $2 \pi$ and independent of $m(t)$, the ACF of $v(t)$ given $\varphi$ is equal to the ACF of $v(t)$ for a deterministic $\varphi$.
	Hence,
	\begin{align*}
		R_{v v}(t,\tau | \varphi) &= \frac{A^2}{2} R_{m m}(\tau) \left( \cos(2 \omega_0 t + \omega_0 \tau + 2 \varphi) + \cos(\omega_0 \tau) \right)
	\end{align*}
	Therefore,
	\begin{align*}
		R_{v v}(t,\tau) &= \expct\left[ R_{v v}(t,\tau | \varphi) \right]\\
		&= \int\limits_{-\infty}^{\infty} R_{v v}(t,\tau | \varphi) \pdf_{\varphi}(\varphi) \dif \varphi\\
		&= \frac{1}{\pi} \int\limits_{0}^{\pi} R_{v v}(t,\tau | \varphi) \dif \varphi\\
		&= \frac{1}{\pi} \int\limits_{0}^{\pi} \frac{A^2}{2} R_{m m}(\tau) \left( \cos\left( 2 \omega_0 t + \omega_0 \tau + 2 \varphi \right) + \cos(\omega_0 \tau) \right) \dif \varphi\\
		&= \frac{A^2}{2} R_{m m}(\tau) \cos(\omega_0 \tau)
	\end{align*}
	Hence,
	\begin{align*}
		S_{v v}(\omega) &= \FT\left\{ R_{v v}(\tau) \right\}\\
		&= \FT\left\{ \frac{A^2}{2} R_{m m}(\tau) \frac{1}{2} \left( e^{j \omega_0 \tau} + e^{-j \omega_0 \tau} \right) \right\}\\
		&= \frac{A^2}{4} \left( S_{m m}(\omega - \omega_0) + S_{m m}(\omega + \omega_0) \right)
	\end{align*}
\end{proof}

\begin{align*}
	u(t) &= v(t) 2 \cos(\omega_0 t + \varphi_1)\\
	&= A m(t) 2 \cos(\omega_0 t + \varphi) \cos(\omega_0 t + \varphi_1)\\
	&= A m(t) \left( \cos(2 \omega_0 t + \varphi + \varphi_1) + \cos(\varphi_1 - \varphi) \right)\\
	&= A m(t) \cos(2 \omega_0 t + \varphi + \varphi_1) + A m(t) \cos(\varphi_1 - \varphi)
\end{align*}
Hence, after passing $u(t)$ through the LPF,
\begin{align*}
	\tilde{m}(t) &= A m(t) \cos(\varphi_1 - \varphi)
\end{align*}
Hence, the receiver reconstructs the signal with an error of $\cos(\varphi_1 - \varphi)$.
This issue can be solved by sending the carrier frequency along with the modulated signal, as in amplitude modulation.

\section{Amplitude Modulation}

\begin{definition}[Amplitude modulation]
	A modulation scheme in which the information signal $m(t)$ is modulated such that
	\begin{align*}
		v(t) &= A (1 + m(t)) \cos(\omega_0 t + \varphi)
	\end{align*}
	is called amplitude modulation.
\end{definition}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [mixer] (mixer) at (2,0) {};
		\node [adder] (adder) at (4,0) {};
		\draw [stealth-] (mixer.west) -- ++(-1,0) node [left] {$m(t)$};
		\draw [stealth-] (mixer.south) |- (3,-2);
		\draw [stealth-] (adder.south) |- (3,-2);
		\draw (3,-2) -- ++(0,-1) node [below] {$A \cos(\omega_0 t + \varphi)$};
		\draw (mixer.east) -- (adder.west) node [midway, above] {$m'(t)$};
		\draw [-stealth] (adder.east) -- ++(1,0) node [right] {$v(t)$};
	\end{tikzpicture}
	\caption{AM Transmitter}
	\label{fig:AM_transmitter}
\end{figure}

Let
\begin{align*}
	m'(t) &= 1 + m(t)
\end{align*}
If $m(t)$ is deterministic,
\begin{align*}
	\FT\left\{ 1 + m(t) \right\} &= 2 \pi \delta(\omega) + M(\omega)
\end{align*}
Therefore,
\begin{align*}
	V(\omega) &= \frac{A}{2} \left( 2 \pi \varphi(\omega - \omega_0) + M(\omega - \omega_0) + 2 \pi \varphi(\omega + \omega_0) + M(\omega + \omega_0) \right)
\end{align*}
If $m(t)$ is stochastic,
\begin{align*}
	R_{m' m'}(\tau) &= \expct\left[ \left( 1 + m(t + \tau) \right) \left( 1 + m(t) \right) \right]\\
	&= 1 + 2 \eta_m + R_{m m}(\tau)
\end{align*}
Usually,
\begin{align*}
	\eta_m &= \expct\left[ m(t) \right]\\
	&= 0
\end{align*}
Therefore,
\begin{align*}
	R_{m' m'}(\tau) &= 1 + R_{m m}(\tau)
\end{align*}
Hence,
\begin{align*}
	S_{m' m'}(\omega) &= 2 \pi \delta(\omega) + S_{m m}(\omega)
\end{align*}
Therefore, assuming $\varphi$ to be uniformly distributed between $0$ and $2 \pi$,
\begin{align*}
	S_{v v}(\omega) &= \frac{A^2}{4} \left( 2 \pi \delta(\omega - \omega_0) + S_{m m}(\omega - \omega_0) + 2 \pi \delta (\omega + \omega_0) + S_{m m}(\omega + \omega_0) \right)
\end{align*}

In amplitude modulation, the carrier wave is enveloped by $\pm A m'(t)$, i.e. by $\pm A \left( 1 + m(t) \right)$.
Hence, let the modulation indices be defined as in \cref{def:modulation_indices_for_AM}.

\begin{definition}[Modulation indices for amplitude modulation]
	\begin{align*}
		\mu_+ &= \max_{t} m(t)\\
		\mu_- &= -\min_{t} m(t)\\
		\mu &= \frac{1}{2} \left( \mu_+ + \mu_- \right)\\
		&= \frac{1}{2} \left( \max_{t} m(t) - \min_{t} m(t) \right)
	\end{align*}
	\label{def:modulation_indices_for_AM}
\end{definition}

Hence,
\begin{align*}
	\mu_+ &= \frac{\max_{t} v(t) - A}{A}\\
	\mu_- &= \frac{A - \min_{t} v(t)}{A}
\end{align*}
Hence, the signal can be reconstructed using a envelope detector as in \cref{fig:envelope_detector}.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\draw (0,0) node [left] {$m(t)$} to [D] (4,0);
		\draw (4,0) to [R] (4,-2);
		\draw (6,0) to [C] (6,-2);
		\draw (4,0) to (8,0) node [right] {$\tilde{m}(t)$};
		\draw (4,-2) node [ground] {};
		\draw (6,-2) node [ground] {};
	\end{tikzpicture}
	\caption{Envelope Detector}
	\label{fig:envelope_detector}
\end{figure}

\begin{definition}[Efficiency index]
	The efficiency index of AM with respect to DSB is defined to be
	\begin{align*}
		\eta &= \frac{\expct\left[ v_{\DSB}^2(t) \right]}{\expct\left[ v_{\AM}^2(t) \right]}
	\end{align*}
\end{definition}

\section{Single Side Band Modulation}

\begin{align*}
	M_L(\omega) &=
		\begin{cases}
			M(\omega) &;\quad \omega \le 0\\
			0 &;\quad \omega > 0\\
		\end{cases}\\
	M_U(\omega) &=
		\begin{cases}
			0 &;\quad \omega \le 0\\
			M(\omega) &;\quad \omega > 0\\
		\end{cases}
\end{align*}
Hence,
\begin{align*}
	M(\omega) &= M_L(\omega) + M_U(\omega)
\end{align*}
Let $v_{\USB}(t)$ and $v_{\LSB}(t)$ be signals such that
\begin{align*}
	V_{\USB}(\omega) &= A \left( M_U(\omega - \omega_0) + M_L(\omega + \omega_0) \right)\\
	V_{\LSB}(\omega) &= A \left( M_L(\omega - \omega_0) + M_U(\omega + \omega_0) \right)\\
\end{align*}
Also, let $\hat{M}(\omega)$ be the Hilbert transform of $M(\omega)$, i.e.
\begin{align*}
	\hat{M}(\omega) &= -j \sign M(\omega)\\
	&= j M_L(\omega) - j M_U(\omega)
\end{align*}
Therefore,
\begin{align*}
	M_U(\omega) &= \frac{1}{2} \left( M(\omega) + j \hat{M}(\omega) \right)\\
	M_L(\omega) &= \frac{1}{2} \left( M(\omega) - j \hat{M}(\omega) \right)
\end{align*}
Therefore,
\begin{align*}
	V_{\USB}(\omega) &= A \left( M_U(\omega - \omega_0) + M_L(\omega + \omega_0) \right)\\
	&= \frac{1}{2} \left( M(\omega - \omega_0) + M(\omega + \omega_0) \right) + \frac{-1}{2 j} \left( \hat{M}(\omega - \omega_0) - \hat{M}(\omega + \omega_0) \right)
\end{align*}
Therefore,
\begin{align*}
	v_{\USB}(t) &= A \left( m(t) \cos(\omega_0 t) - \hat{m}(t) \sin(\omega_0 t) \right)
\end{align*}
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [mixer] (mixer_cos) at (4,2) {};
		\node [mixer] (mixer_sin) at (6,-2) {};
		\node [block] (Hilbert) at (2,-2) {$H(\omega)$};
		\node [adder] (adder) at (8,0) {};

		\draw (-1,0) node [left] {$m(t)$} -- (0,0);
		\draw [-stealth] (0,0) |- (mixer_cos.west);
		\draw [-stealth] (0,0) |- (Hilbert.west);
		\draw [-stealth] (Hilbert.east) -- (mixer_sin.west) node [midway, above] {$\hat{m}(t)$};
		\draw [-stealth] (mixer_cos.east) -| (adder.north);
		\draw [-stealth] (mixer_sin.east) -| (adder.south);
		\draw [-stealth] (adder.east) -- ++(1,0) node [right] {$v_{\USB}(t)$};

		\draw [stealth-] (mixer_cos.north) -- ++(0,1) node [above] {$A \cos(\omega_0 t)$};
		\draw [stealth-] (mixer_sin.south) -- ++(0,-1) node [below] {$-A \sin(\omega_0 t)$};
	\end{tikzpicture}
	\caption{Implementation of USB Transmitter with Hilbert Transformer}
	\label{fig:implementation_of_USB_transmitter_with_Hilbert_transformer}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [mixer] (mixer1) at (1,0) {};
		\node [block] (HPF1) at (4,0) {HPF};
		\node [mixer] (mixer2) at (7,0) {};
		\node [block] (HPF2) at (10,0) {HPF};

		\draw [-stealth] (0,0) node [left] {$m(t)$} -- (mixer1.west);
		\draw [-stealth] (mixer1.east) -- (HPF1.west);
		\draw [-stealth] (HPF1.east) -- (mixer2.west);
		\draw [-stealth] (mixer2.east) -- (HPF2.west);
		\draw [-stealth] (HPF2.east) -- ++(1,0) node [right] {$v_{\USB}(t)$};

		\draw [stealth-] (mixer1.north) -- ++(0,1) node [above] {$2 A \cos(\omega_{\IF} t)$};
		\draw [stealth-] (mixer2.north) -- ++(0,1) node [above] {$2 A \cos\left( (\omega_0 - \omega_{\IF}) t \right)$};
	\end{tikzpicture}
	\caption{Implementation of USB Transmitter with Frequency Domain Slicing}
	\label{fig:implementation_of_USB_transmitter_with_frequency_domain_slicing}
\end{figure}

\begin{align*}
	\tilde{M}(\omega) &= A \left( \cos(\varphi) M(\omega) + \sin(\varphi) \hat{M}(\omega) \right)\\
	&= A \left( \cos(\varphi) + \sin(\varphi) \left( -j \sign(\omega) \right) \right) M(\omega)\\
	&=
		\begin{cases}
			A M(\omega) e^{j \varphi} &;\quad \omega \le 0\\
			A M(\omega) e^{-j \varphi} &;\quad \omega > 0\\
		\end{cases}
\end{align*}

\clearpage
\part{Phase Modulations}

\begin{definition}[Phase modulation]
	Modulation schemes in which the phase of the carrier wave is modulated according to the data signal are called phase modulations.
	Hence, the modulated signal is
	\begin{align*}
		v(t) &= R(t) \cos\left( \omega_0 t + \theta(t) \right)
	\end{align*}
\end{definition}

\section{Phase Modulation}

\begin{definition}[Phase modulation]
	A phase modulation scheme in which the phase is modulated according to the data signal, i.e.
	\begin{align*}
		\theta(t) &= D_P m(t)
	\end{align*}
	is called phase modulation.
	Hence,
	\begin{align*}
		v(t) &= A \cos\left( \omega_0 t + D_P m(t) \right)
	\end{align*}
	where $D_p$ is a constant.
\end{definition}

\section{Frequency Modulation}

\begin{definition}[Instantaneous frequency]
	Let
	\begin{align*}
		\psi(t) &= \omega_0 t + \theta(t)
	\end{align*}
	Hence, the instantaneous frequency is defined to be
	\begin{align*}
		f_i &= \frac{1}{2 \pi} \dod{\psi(t)}{t}
	\end{align*}
\end{definition}

\begin{definition}[Frequency modulation]
	A phase modulation scheme in which the phase is modulated according to the data signal, i.e.
	\begin{align*}
		\theta(t) &= D_{\omega} \int\limits_{-\infty}^{t} m(\tau) \dif \tau
	\end{align*}
	is called frequency modulation.
	Hence,
	\begin{align*}
		\psi(t) &= \omega_0 t + \theta(t)\\
		&= \omega_0 t + D_{\omega} \int\limits_{-\infty}^{t} m(\tau) \dif \tau\\
		&= 2 \pi f_0 t + D_{\omega} \int\limits_{-\infty}^{t} m(\tau) \dif \tau
	\end{align*}
	Hence, the instantaneous frequency is
	\begin{align*}
		f_i(t) &= \frac{1}{2 \pi} \left( 2 \pi f_0 + D_{\omega} m(t) \right)\\
		&= f_0(t) + \frac{D_{\omega}}{2 \pi} m(t)
	\end{align*}
\end{definition}

\begin{theorem}
	The maximum frequency deviation for a FM signal is
	\begin{align*}
		\Delta f &= \frac{D_{\omega}}{2 \pi} \max_{t} \left| m(t) \right|
	\end{align*}
\end{theorem}

\begin{definition}
	For a data signal with frequency $f_m$,
	\begin{align*}
		\beta &= \frac{\Delta f}{f_m}
	\end{align*}
\end{definition}

\subsection{Narrow Band FM Generation}

\begin{align*}
	v(t) &= \Re\left\{ g(t) e^{j \omega_0 t} \right\}
\end{align*}
where
\begin{align*}
	g(t) &= A e^{j \theta(t)}
\end{align*}
Let
\begin{align*}
	\left| \theta(t) \right| &<< 1
\end{align*}
Therefore,
\begin{align*}
	g(t) &= A \left( 1 + j \theta(t) \right)
\end{align*}
Therefore,
\begin{align*}
	v(t) &= \Re\left\{ g(t) e^{j \omega_0 t} \right\}\\
	&= \Re\left\{ A \left( 1 + j \theta(t) \right) \left( \cos(\omega_0 t) + j \sin(\omega_0 t) \right) \right\}\\
	&= A \left( \cos(\omega_0 t) - \theta(t) \sin(\omega_0 t) \right)
\end{align*}
Hence, a narrow band FM signal can be generated as in \cref{fig:narrow_band_FM_transmitter}.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [block] (integrator) at (2,0) {$\int\limits_{-\infty}^{t}$};
		\node [mixer] (mixer_sin) at (4,0) {};
		\node [mixer] (mixer_cos) at (6,0) {};
		\draw [stealth-] (integrator.west) -- ++(-1,0) node [left] {$m(t)$};
		\draw [-stealth] (integrator.east) -- (mixer_sin.west);
		\draw [-stealth] (mixer_sin.east) -- (mixer_cos.west);
		\draw [-stealth] (mixer_cos.east) -- ++(1,0) node [right] {$v(t)$};
		\draw (mixer_sin.south) -- ++(0,-1) node [below] {$-A \sin(\omega_0 t)$};
		\draw (mixer_cos.south) -- ++(0,-1) node [below] {$A \cos(\omega_0 t)$};
	\end{tikzpicture}
	\caption{Narrow Band FM Transmitter}
	\label{fig:narrow_band_FM_transmitter}
\end{figure}
Similarly, a receiver can be designed as in \cref{fig:narrow_band_FM_receiver}.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [block] (differentiator) at (2,0) {$\dod{}{t}$};
		\node [mixer] (envelope_detector) at (4,0) {};
		\draw [stealth-] (differentiator.west) -- ++(-1,0) node [left] {$v(t)$};
		\draw [-stealth] (differentiator.east) -- (envelope_detector.west);
		\draw [-stealth] (envelope_detector.east) -- ++(1,0) node [right] {$\tilde{m}(t)$};
	\end{tikzpicture}
	\caption{Narrow Band FM Receiver}
	\label{fig:narrow_band_FM_receiver}
\end{figure}

\subsection{Non Narrow Band FM Generation}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{max width=\columnwidth}
	\begin{tikzpicture}
		\node [block] (spectrum_compressor) at (2,0) {$\frac{1}{M}$};
		\node [block] (NBFM) at (6,0) {NBFM transmitter};
		\node [block] (multiplication) at (12,0) {$M$ fold frequency multiplication};
		\draw [stealth-] (spectrum_compressor.west) -- ++(-1,0) node [left] {$m(t)$};
		\draw [-stealth] (spectrum_compressor.east) -- (NBFM.west);
		\draw [-stealth] (NBFM.east) -- (multiplication.west);
		\draw [-stealth] (multiplication.east) -- ++(1,0) node [right] {$v(t)$};
	\end{tikzpicture}
	\end{adjustbox}
	\caption{Non Narrow Band FM Transmitter}
	\label{fig:non_narrow_band_FM_transmitter}
\end{figure}

\end{document}
